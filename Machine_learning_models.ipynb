{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This document/.ipynb file was originaly created by Sacha Dubrulle in Python 2 \n",
    "# and further worked upon and converted to Python 3 by Thomas Valcke.\n",
    "# This notebook has been converted to Python 3 because the syntax changed with Python 3.\n",
    "\n",
    "# When run on the server there was 1 Cell that did not work RF_classification again a random forest model\n",
    "# ==> get_ipython().run_cell_magic('time', '', 'RF_model_all = RF_classification(model_description = \"All features RF\",\\n                    df = ML_BT,\\n                    cat_columns = text_cat_cols + non_text_cat_cols, \\n                    num_columns = text_num_cols + non_text_num_cols)\\nRF_model_text = RF_classification(model_description = \"Text features RF\",\\n                  df = ML_BT,\\n                  cat_columns = text_cat_cols, \\n                  num_columns = text_num_cols)\\n\\nRF_model_all = RF_classification(model_description = \"Non-text features RF\",\\n                    df = ML_BT,\\n                    cat_columns = non_text_cat_cols, \\n                    num_columns = non_text_num_cols)')\n",
    "# The reason for this cell not working is still unkown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = 'color:darkred'> Imports </span style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['SPARK_HOME'] = 'D:/School/STAGE_BP/spark' # your spark location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession initialized\n"
     ]
    }
   ],
   "source": [
    "################################ JUPYTER ###################################\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "################################ SPARK ###################################\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "\n",
    "try:\n",
    "    sc = pyspark.SparkContext(appName = 'App')\n",
    "    spark = SparkSession(sparkContext=sc)\n",
    "    print(\"SparkSession initialized\")\n",
    "except ValueError:\n",
    "    print(\"SparkSession already initialized\")\n",
    "    \n",
    "from pyspark.sql.functions import udf, explode, lower, regexp_extract, split, col, regexp_replace\n",
    "from pyspark.sql.types import StringType, TimestampType, IntegerType, ArrayType, DoubleType, BooleanType\n",
    "from pyspark.ml.feature import Binarizer, StringIndexer, StringIndexerModel, OneHotEncoder, MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "################################ PYTHON ###################################\n",
    "    \n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", -1)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "import numpy as np\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.dates import YearLocator, MonthLocator, DayLocator, WeekdayLocator, DateFormatter\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read prepared data from data preperation in parquet format\n",
    "ML_BT = spark.read.parquet(\"D:/School/STAGE_BP/ipo/ML_features.parquet\")\n",
    "\n",
    "#ML_BT.drop(\"cashtags\", \"hashtags\").repartition(1).write.csv(\"MLBT.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML_BT.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML_BT.select(\"contains_cashtag\").show(10, truncate =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                          |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "|KAYA PAS HOKAGE KETIGA LAWAN OROCHIMARU https://t.co/0g8lxrdkRo                                               |\n",
      "|RT https://t.co/pG2TepSKoC                                                                                    |\n",
      "|my right leg keeps popping out of place and my abs ache.....I love dance so much                              |\n",
      "|RT @_velvetbullets: Twitter needs an edit button. I stay looking uneducated af bc of typos smh                |\n",
      "|RT @wiwer77: The Humanist's Emerging Role in #BigData \n",
      "https://t.co/dIvlvGYkXv via @datanami                  |\n",
      "|@mohdhaider1984 @uniquepersonaap @NitinTyagiAAP \n",
      "\n",
      "Hum India Ko mehnat se aage lekar jaana chahte hain Aur aap?|\n",
      "|I do the same thing  https://t.co/gbB23vyvnU                                                                  |\n",
      "|????? ??????? 1 ???? ?????? ?? https://t.co/4E8FIVNTX7  2 ???? ??????? 3 ??? https://t.co/phm4oY5cnS pQd      |\n",
      "|ERPEMES: #OMF DEEPL0W emg kalo aku ngasih tau siapa aku kamu mau sama aku?                                    |\n",
      "|RT @Bre_Leee_: Actually I need to eat a bowl of kale after watching draya snaps today like how tf             |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ML_BT.sample(withReplacement=False, fraction = 0.1).select(\"text\").show(10, truncate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                                        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|@leoradiofobia como ? que c? faz pros olhos n?o fritarem depois de horas vendo uma timeline de edi??o? Puta merda, viu?!                    |\n",
      "|RT @its_branndon: Legit this is karr &amp; Kim idc https://t.co/HdqtqNhxSN                                                                  |\n",
      "|@maryterry2015 You've been quoted in my #Storify story \"NRMS SS STAAR Chat #2 - A New Nation\" https://t.co/LFbpJu2iUh                       |\n",
      "|I cant believe wtf i just heard i dare you have that nigga say somethin to me im knockin him tf out                                         |\n",
      "|i dag totadi stelpa mig a klosetinu eg er bara 9 ara hun er lika 9                                                                          |\n",
      "|RT @MaiMaitreya: #SayonaraMdT Si no se renueva, por favor, se?ores de la tele, no vuelvan a poner \"RTVE\" y \"cultura\" en la misma frase.     |\n",
      "|He wants to be hip so bad ? https://t.co/hiiDflJATr                                                                                        |\n",
      "|KAYA PAS HOKAGE KETIGA LAWAN OROCHIMARU https://t.co/0g8lxrdkRo                                                                             |\n",
      "|RT @KaradagSencer: Diyanet ??leri Ba?kanl???na ba?l? T?rkiye genelinde 100 Civar? M?ft?. Gelen emre ra?men Ezan ve sala Okutmad?. ?hbar ya??|\n",
      "|Ara is goals https://t.co/Uu8PZwxWPl                                                                                                        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ML_BT.select(\"text\").show(10, truncate =  False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = 'color:darkred'> Classical ensemble using Spark </span style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation_to_Pandas(df, cat_columns, num_columns, output_column = \"retweet_binary\"):\n",
    "    stringIndexers = []\n",
    "    indexed_cols = []\n",
    "\n",
    "    encoders = []\n",
    "    encoded_cols = []\n",
    "\n",
    "    for column in cat_columns:\n",
    "        inCol= column\n",
    "        outCol = column + \"indexed\"\n",
    "        stringIndexer = StringIndexer(inputCol = inCol, outputCol=outCol)\n",
    "\n",
    "        stringIndexers.append(stringIndexer)\n",
    "        indexed_cols.append(outCol)\n",
    "\n",
    "    for column in indexed_cols:\n",
    "        inCol= column\n",
    "        outCol = column + \"encoded\"\n",
    "        encoder = OneHotEncoder(inputCol=inCol, outputCol=outCol)\n",
    "\n",
    "        encoders.append(encoder)\n",
    "        encoded_cols.append(outCol)\n",
    "\n",
    "    binarizer = Binarizer(inputCol = \"retweet_count_double\", outputCol = output_column, threshold=0.9)\n",
    "\n",
    "    assembler = VectorAssembler(inputCols= num_columns + encoded_cols, outputCol=\"features\")\n",
    "    \n",
    "    pipeline = Pipeline(stages= stringIndexers + encoders + [assembler] + [binarizer])\n",
    "    pipelineModel = pipeline.fit(df)\n",
    "\n",
    "    transformed_df = (pipelineModel.transform(df)\n",
    "                                   .select(\"features\", \n",
    "                                            col(\"retweet_binary\").cast(\"integer\"), \n",
    "                                            \"retweet_count\")\n",
    "                     )\n",
    "    \n",
    "    return transformed_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = 'color:darkred'> Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>train AUC</th>\n",
       "      <th>test AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [algorithm, train AUC, test AUC]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results  = pd.DataFrame(columns = [\"algorithm\", \"train AUC\", \"test AUC\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_classification(model_description, df, cat_columns, num_columns, output_column = \"retweet_binary\"):\n",
    "    stringIndexers = []\n",
    "    indexed_cols = []\n",
    "    encoders = []\n",
    "    encoded_cols = []\n",
    "\n",
    "    for column in cat_columns:\n",
    "        inCol= column\n",
    "        outCol = column + \"indexed\"\n",
    "        stringIndexer = StringIndexer(inputCol = inCol, outputCol=outCol)\n",
    "        stringIndexers.append(stringIndexer)\n",
    "        indexed_cols.append(outCol)\n",
    "\n",
    "    for column in indexed_cols:\n",
    "        inCol= column\n",
    "        outCol = column + \"encoded\"\n",
    "        encoder = OneHotEncoder(inputCol=inCol, outputCol=outCol)\n",
    "        encoders.append(encoder)\n",
    "        encoded_cols.append(outCol)\n",
    "\n",
    "    binarizer = Binarizer(inputCol = \"retweet_count_double\", outputCol = output_column, threshold=0.9)\n",
    "    assembler = VectorAssembler(inputCols= num_columns + encoded_cols, outputCol=\"features\")\n",
    "    \n",
    "    pipeline = Pipeline(stages= stringIndexers + encoders + [assembler] + [binarizer])\n",
    "    pipelineModel = pipeline.fit(df)\n",
    "    \n",
    "    transformed_df = (pipelineModel.transform(df)\n",
    "                                   .select(\"features\", col(\"retweet_binary\").cast(\"integer\"), \"retweet_count\")\n",
    "                     )\n",
    "    \n",
    "    train, validation, test = transformed_df.randomSplit([0.8, 0.1, 0.1], seed = 42)\n",
    "    \n",
    "    rf = RandomForestClassifier(numTrees=500, labelCol=\"retweet_binary\")\n",
    "    rf_model = rf.fit(train)\n",
    "\n",
    "    train_predictions = rf_model.transform(train)\n",
    "    validation_predictions = rf_model.transform(validation)\n",
    "    \n",
    "    AUC_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"retweet_binary\", metricName=\"areaUnderROC\")\n",
    "\n",
    "    AUC_train = AUC_evaluator.evaluate(train_predictions)\n",
    "    AUC_validation = AUC_evaluator.evaluate(validation_predictions)\n",
    "    print(\"###################\" + model_description.upper() + \"##################\")\n",
    "    print(\"TRAIN AUC:       {}\".format(AUC_train))\n",
    "    print(\"VALIDATION AUC:        {}\".format(AUC_validation))\n",
    "    \n",
    "    global results\n",
    "    results = results.append({\"algorithm\": model_description, \"train AUC\": AUC_train, \"test AUC\": AUC_validation}, ignore_index=True)\n",
    "    \n",
    "    return rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_text_num_cols = [\"user_seniority\", \"user_favourites_count\", \"user_friends_count\", \"user_followers_count\"]\n",
    "non_text_cat_cols = [\"day_of_week\"]\n",
    "text_num_cols = [\"cashtag_count\", \"hashtag_count\"]\n",
    "text_cat_cols = [\"lang\", \"contains_cashtag\", \"contains_cashtag_with_spaces\", \"contains_ipo\", \"contains_hashtags\", \"contains_url\"]\n",
    "dependent_variable = [\"retweet_count\"]\n",
    "text = [\"clean_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o623.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 26.0 failed 1 times, most recent failure: Lost task 11.0 in stage 26.0 (TID 251, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1\r\n\tat org.apache.spark.ml.linalg.DenseVector.apply(Vectors.scala:462)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator$$anonfun$1.apply(BinaryClassificationEvaluator.scala:82)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator$$anonfun$1.apply(BinaryClassificationEvaluator.scala:81)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:306)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:168)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:148)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:155)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:223)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:86)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:97)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:87)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 1\r\n\tat org.apache.spark.ml.linalg.DenseVector.apply(Vectors.scala:462)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator$$anonfun$1.apply(BinaryClassificationEvaluator.scala:82)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator$$anonfun$1.apply(BinaryClassificationEvaluator.scala:81)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-9ca8d4ace742>\u001b[0m in \u001b[0;36mRF_classification\u001b[1;34m(model_description, df, cat_columns, num_columns, output_column)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mAUC_evaluator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawPredictionCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rawPrediction\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"retweet_binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetricName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"areaUnderROC\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mAUC_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAUC_evaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mAUC_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAUC_evaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"###################\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel_description\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"##################\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o623.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 26.0 failed 1 times, most recent failure: Lost task 11.0 in stage 26.0 (TID 251, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1\r\n\tat org.apache.spark.ml.linalg.DenseVector.apply(Vectors.scala:462)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator$$anonfun$1.apply(BinaryClassificationEvaluator.scala:82)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator$$anonfun$1.apply(BinaryClassificationEvaluator.scala:81)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:306)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:168)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:148)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:155)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:223)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:86)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:97)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:87)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 1\r\n\tat org.apache.spark.ml.linalg.DenseVector.apply(Vectors.scala:462)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator$$anonfun$1.apply(BinaryClassificationEvaluator.scala:82)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator$$anonfun$1.apply(BinaryClassificationEvaluator.scala:81)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "get_ipython().run_cell_magic('time', '', 'RF_model_all = RF_classification(model_description = \"All features RF\",\\n                    df = ML_BT,\\n                    cat_columns = text_cat_cols + non_text_cat_cols, \\n                    num_columns = text_num_cols + non_text_num_cols)\\nRF_model_text = RF_classification(model_description = \"Text features RF\",\\n                  df = ML_BT,\\n                  cat_columns = text_cat_cols, \\n                  num_columns = text_num_cols)\\n\\nRF_model_all = RF_classification(model_description = \"Non-text features RF\",\\n                    df = ML_BT,\\n                    cat_columns = non_text_cat_cols, \\n                    num_columns = non_text_num_cols)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = 'color:darkred'> Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_classification(model_description, df, cat_columns, num_columns, output_column = \"retweet_binary\"):\n",
    "    stringIndexers = []\n",
    "    indexed_cols = []\n",
    "\n",
    "    encoders = []\n",
    "    encoded_cols = []\n",
    "\n",
    "    for column in cat_columns:\n",
    "        inCol= column\n",
    "        outCol = column + \"indexed\"\n",
    "        stringIndexer = StringIndexer(inputCol = inCol, outputCol=outCol)\n",
    "\n",
    "        stringIndexers.append(stringIndexer)\n",
    "        indexed_cols.append(outCol)\n",
    "\n",
    "    for column in indexed_cols:\n",
    "        inCol= column\n",
    "        outCol = column + \"encoded\"\n",
    "        encoder = OneHotEncoder(inputCol=inCol, outputCol=outCol)\n",
    "\n",
    "        encoders.append(encoder)\n",
    "        encoded_cols.append(outCol)\n",
    "\n",
    "    binarizer = Binarizer(inputCol = \"retweet_count_double\", outputCol = output_column, threshold=0.9)\n",
    "\n",
    "    assembler = VectorAssembler(inputCols= num_columns + encoded_cols, outputCol=\"features\")\n",
    "    \n",
    "    pipeline = Pipeline(stages= stringIndexers + encoders + [assembler] + [binarizer])\n",
    "    pipelineModel = pipeline.fit(df)\n",
    "\n",
    "    transformed_df = (pipelineModel.transform(df)\n",
    "                                   .select(\"features\", \n",
    "                                            col(\"retweet_binary\").cast(\"integer\"), \n",
    "                                            \"retweet_count\")\n",
    "                     )\n",
    "    \n",
    "    transformed_df = transformed_df\n",
    "    \n",
    "    train, validation, test = transformed_df.randomSplit([0.8, 0.1, 0.1], seed = 42)\n",
    "    \n",
    "    lr = LogisticRegression(labelCol=\"retweet_binary\")\n",
    "    lr_model = lr.fit(train)\n",
    "\n",
    "    train_predictions = lr_model.transform(train)\n",
    "    validation_predictions = lr_model.transform(validation)\n",
    "    \n",
    "    AUC_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"retweet_binary\", metricName=\"areaUnderROC\")\n",
    "\n",
    "    AUC_train = AUC_evaluator.evaluate(train_predictions)\n",
    "    AUC_validation = AUC_evaluator.evaluate(validation_predictions)\n",
    "    print(\"###################\" + model_description.upper() + \"##################\")\n",
    "    print(\"TRAIN AUC:       {}\".format(AUC_train))\n",
    "    print(\"TEST AUC:        {}\".format(AUC_validation))\n",
    "    \n",
    "    global results\n",
    "    results = results.append({\"algorithm\": model_description, \"train AUC\": AUC_train, \"test AUC\": AUC_validation}, ignore_index=True)\n",
    "    \n",
    "    return lr_model\n",
    "    #return encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################ALL FEATURES LR##################\n",
      "TRAIN AUC:       0.0\n",
      "TEST AUC:        0.0\n",
      "###################TEXT FEATURES LR##################\n",
      "TRAIN AUC:       0.0\n",
      "TEST AUC:        0.0\n",
      "###################NON-TEXT FEATURES LR##################\n",
      "TRAIN AUC:       0.0\n",
      "TEST AUC:        0.0\n",
      "Wall time: 7.47 s\n"
     ]
    }
   ],
   "source": [
    "# Because the parquet in data preperation is not created correctly with enugh data the Machine Learning can not create a good prediction\n",
    "\n",
    "get_ipython().run_cell_magic('time', '', 'LR_model_all = LR_classification(model_description = \"All features LR\",\\n                    df = ML_BT,\\n                    cat_columns = text_cat_cols + non_text_cat_cols, \\n                    num_columns = text_num_cols + non_text_num_cols)\\n\\nLR_model_text = LR_classification(model_description = \"Text features LR\",\\n                  df = ML_BT,\\n                  cat_columns = text_cat_cols, \\n                  num_columns = text_num_cols)\\n\\nLR_model_non_text = LR_classification(model_description = \"Non-text features LR\",\\n                  df = ML_BT,\\n                  cat_columns = non_text_cat_cols,\\n                  num_columns = non_text_num_cols)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>train AUC</th>\n",
       "      <th>test AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All features LR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Text features LR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Non-text features LR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              algorithm  train AUC  test AUC\n",
       "0  All features LR       0.0        0.0     \n",
       "1  Text features LR      0.0        0.0     \n",
       "2  Non-text features LR  0.0        0.0     "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because the parquet in data preperation is not created correctly with enugh data the Machine Learning can not create a good prediction\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"D:/School/STAGE_BP/ipo/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_classification(model_description, df, cat_columns, num_columns, output_column = \"retweet_binary\"):\n",
    "    stringIndexers = []; indexed_cols = []; encoders = []; encoded_cols = []\n",
    "\n",
    "    for column in cat_columns:\n",
    "        inCol= column\n",
    "        outCol = column + \"indexed\"\n",
    "        stringIndexer = StringIndexer(inputCol = inCol, outputCol=outCol)\n",
    "        stringIndexers.append(stringIndexer)\n",
    "        indexed_cols.append(outCol)\n",
    "\n",
    "    for column in indexed_cols:\n",
    "        inCol= column\n",
    "        outCol = column + \"encoded\"\n",
    "        encoder = OneHotEncoder(inputCol=inCol, outputCol=outCol)\n",
    "        encoders.append(encoder)\n",
    "        encoded_cols.append(outCol)\n",
    "\n",
    "    binarizer = Binarizer(inputCol = \"retweet_count_double\", outputCol = output_column, threshold=0.9)\n",
    "    assembler = VectorAssembler(inputCols= num_columns + encoded_cols, outputCol=\"features\")\n",
    "    \n",
    "    pipeline = Pipeline(stages= stringIndexers + encoders + [assembler] + [binarizer])\n",
    "    pipelineModel = pipeline.fit(df)\n",
    "    \n",
    "    transformed_df = (pipelineModel.transform(df)\n",
    "                                   .select(\"features\", col(\"retweet_binary\").cast(\"integer\"), \"retweet_count\")\n",
    "                     )\n",
    "    \n",
    "    train, validation, test = transformed_df.randomSplit([0.8, 0.1, 0.1], seed = 42)\n",
    "    \n",
    "    rf = RandomForestClassifier(numTrees=500, labelCol=\"retweet_binary\")\n",
    "    rf_model = rf.fit(train)\n",
    "\n",
    "    train_predictions = rf_model.transform(train)\n",
    "    validation_predictions = rf_model.transform(validation)\n",
    "    \n",
    "    AUC_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"retweet_binary\", metricName=\"areaUnderROC\")\n",
    "\n",
    "    AUC_train = AUC_evaluator.evaluate(train_predictions)\n",
    "    AUC_validation = AUC_evaluator.evaluate(validation_predictions)\n",
    "    print(\"###################\" + model_description.upper() + \"##################\")\n",
    "    print(\"TRAIN AUC:       {}\".format(AUC_train))\n",
    "    print(\"VALIDATION AUC:        {}\".format(AUC_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
