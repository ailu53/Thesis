{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxHx4f3tqCc9"
   },
   "source": [
    "# <span style = 'color:darkred'> Set notebook to full width </span style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OF1WOJEqCdB"
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import JavaGateway, GatewayParameters # Bridge between Python and Java\n",
    "gateway = JavaGateway(gateway_parameters=GatewayParameters(port=25333))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-47-0a93766e12be>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-0a93766e12be>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    spark-shell --driver-memory 2g\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rEoF-HLMqCdS",
    "outputId": "5b462516-17c0-48ec-a92a-796312ea23e8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CuS2z9nUqCd1"
   },
   "source": [
    "# <span style = 'color:darkred'> Notebook structure </span style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewMqgneMqCd2"
   },
   "source": [
    "- ### <a href=#imports> Imports  </a>\n",
    "- ### <a href=#read_in> Reading in (sub)set </a>\n",
    "- ### <a href=#tickers_ipo_extraction> Extracting ticker formatted strings and tweets containing IPO </a>\n",
    "- ### <a href=#redun_columns> Drop standard redundant columns after checking</a>\n",
    "- ### <a href=#expand_subcols> Expand the subcolumns that do not contain WrapperArrays </a>\n",
    "- ### <a href=#check_and_drop_redundant_cols> Check for non-standard redundant columns and drop them </a>\n",
    "- ### <a href=#none> ... </a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVHofAEfqCd4"
   },
   "source": [
    "# <span style = 'color:darkred'> Imports </span style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GEJxZDzqCd6",
    "outputId": "815abb3c-a8a8-4fb5-dda0-cfc619a63c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession already initialized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "#os.environ['SPARK_HOME'] = 'D:/School/STAGE_BP/spark'\n",
    "#import findspark\n",
    "#findspark.init(spark_home = \"/home/sachadubrulle/spark-2.1.0\")\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "\n",
    "try:\n",
    "    sc = pyspark.SparkContext(appName = 'App')\n",
    "    spark = SparkSession(sparkContext=sc)\n",
    "    print(\"SparkSession initialized\")\n",
    "except ValueError:\n",
    "    print(\"SparkSession already initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=pyspark.sql.SparkSession.builder.appName('Bottom-Up-Analysis').config('spark.driver.memory','8G').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nuOMmPbEqCeD"
   },
   "source": [
    "# <span style = 'color:darkred'> To do </span style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47smrucZqCeF"
   },
   "source": [
    "- Add remaining 7000 tweets that are not yet included for 2016\n",
    "- Based on tweets with a known ticker -> find tweets based on language analysis that talk about IPO without necessarilty containing a tickerm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MD_MemfPqCeG"
   },
   "source": [
    "# <span style = 'color:darkred'> Imports </span style> <a name='imports' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TpPQkalLqCeJ"
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Spark imports\n",
    "from pyspark.sql.functions import udf, year, month, dayofmonth, explode, lower, regexp_extract, split, upper, col, regexp_replace\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "import imp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UhPJUBREqCeO"
   },
   "source": [
    "# <span style = 'color:darkred'> ETL </span style> <a name='read_in' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9N4HZ4ocqCeQ"
   },
   "source": [
    "## <span style = 'color:green'> Reading in tweets </span style> <a name='read_in' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mma/Downloads/IPO_project/commented_files\n"
     ]
    }
   ],
   "source": [
    "cd /home/mma/Downloads/IPO_project/commented_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in functions from external file written in python and should be present in the same directory,\n",
    "# Where this file is running from\n",
    "import functions as f_ipo\n",
    "\n",
    "f_ipo = imp.reload(f_ipo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform paterns. We are defining functions, with the functions we just imported.\n",
    "\n",
    "#Extracting all cashtags from the tweets\n",
    "udf_cashtag_re_extract = udf(f_ipo.cashtag_re_extract, ArrayType(elementType = StringType()))\n",
    "#Gives the count of the mentions of the company's name\n",
    "udf_contains_company_name = udf(f_ipo.contains_company_name, IntegerType())\n",
    "non_empty_column_udf = udf(f_ipo.non_empty_column)\n",
    "udf_contains_pattern = udf(f_ipo.contains_pattern)\n",
    "\n",
    "# Define UDF to transform timestamp from StringType to TimestampType\n",
    "udf_datatime = udf(lambda string: datetime.strptime(string, \"%a %b %d %H:%M:%S +0000 %Y\"), TimestampType())\n",
    "\n",
    "udf_day_of_week = udf(lambda x: str(x.strftime('%A')), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cashtag_expression is a regex that filters out everything that starts with $ and has 1-6 letters\n",
    "cashtag_expression = \"(\\$[a-z]{1,6})+\"\n",
    "cashtag_expression_including_spaces = '\\s\\$[a-z]{1,6}\\s' #add spaces to the cashtag\n",
    "ipo_expression = ' #ipo | ipo ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjIYW63AqCeS",
    "outputId": "727414f1-7e8c-480f-af0c-9a54b30dc042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original columns were: ['contributors', 'coordinates', 'display_text_range', 'entities', 'extended_entities', 'extended_tweet', 'favorite_count', 'favorited', 'filter_level', 'geo', 'id', 'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'possibly_sensitive', 'quote_count', 'quoted_status', 'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink', 'reply_count', 'retweet_count', 'retweeted', 'retweeted_status', 'source', 'text', 'timestamp_ms', 'truncated', 'user', 'withheld_in_countries', 'timestamp', 'text_lower', 'cashtags', 'contains_ipo', 'contains_cashtag', 'contains_cashtag_with_spaces', 'contains_company_name', 'retweet_pattern', 'contains_RT_pattern']\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display  \n",
    "from ipywidgets import FloatProgress  \n",
    "import time\n",
    "\n",
    "\n",
    "# saving the file path\n",
    "path = \"/home/mma/Downloads/IPO_Mining/tweets/*.json\"\n",
    "\n",
    "df = (spark.read.json('/home/mma/Downloads/IPO_Mining/tweets/*.json')\n",
    "\n",
    "      \n",
    "        # Add lower text column and timestamp\n",
    "        .select(\"*\", \n",
    "               udf_datatime(col(\"created_at\")).alias(\"timestamp\"),\n",
    "               lower(col(\"text\")).alias(\"text_lower\")\n",
    "               )\n",
    "        # Drop created_at string column\n",
    "        .drop(\"created_at\")\n",
    "        # Add columns related to ipo, cashtags & retweet patterns\n",
    "        .select(\"*\", \n",
    "               udf_cashtag_re_extract('text_lower').alias(\"cashtags\"),\n",
    "               non_empty_column_udf(regexp_extract('text_lower', ipo_expression, 0)).alias(\"contains_ipo\"),\n",
    "               non_empty_column_udf(regexp_extract('text_lower', cashtag_expression, 0)).alias(\"contains_cashtag\"),\n",
    "               non_empty_column_udf(regexp_extract('text_lower', cashtag_expression_including_spaces, 0)).alias(\"contains_cashtag_with_spaces\"),\n",
    "               udf_contains_company_name(\"text_lower\").alias(\"contains_company_name\"),\n",
    "               regexp_extract(str = \"text\", pattern = \"^RT @[A-Za-z0-9]+: \", idx = 0).alias(\"retweet_pattern\")\n",
    "               )\n",
    "        .select(\"*\", \n",
    "               udf_contains_pattern(\"retweet_pattern\").alias(\"contains_RT_pattern\")\n",
    "              )\n",
    "     )\n",
    "\n",
    "\n",
    "#saving the column names\n",
    "original_columns = df.columns\n",
    "\n",
    "#selecting required columns\n",
    "df = (df.withColumnRenamed(\"id_str\", \"tweet_id_str\") #rename to avoid confusion with user_id, retweet_id, ...\n",
    "        .withColumn(\"user_id_str\", df.user.id_str) \n",
    "        .select(\"*\")\n",
    "     )\n",
    "\n",
    "print(\"The original columns were: {}\".format(original_columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"*\").filter('lang == \"en\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contributors',\n",
       " 'coordinates',\n",
       " 'display_text_range',\n",
       " 'entities',\n",
       " 'extended_entities',\n",
       " 'extended_tweet',\n",
       " 'favorite_count',\n",
       " 'favorited',\n",
       " 'filter_level',\n",
       " 'geo',\n",
       " 'id',\n",
       " 'tweet_id_str',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_status_id_str',\n",
       " 'in_reply_to_user_id',\n",
       " 'in_reply_to_user_id_str',\n",
       " 'is_quote_status',\n",
       " 'lang',\n",
       " 'place',\n",
       " 'possibly_sensitive',\n",
       " 'quote_count',\n",
       " 'quoted_status',\n",
       " 'quoted_status_id',\n",
       " 'quoted_status_id_str',\n",
       " 'quoted_status_permalink',\n",
       " 'reply_count',\n",
       " 'retweet_count',\n",
       " 'retweeted',\n",
       " 'retweeted_status',\n",
       " 'source',\n",
       " 'text',\n",
       " 'timestamp_ms',\n",
       " 'truncated',\n",
       " 'user',\n",
       " 'withheld_in_countries',\n",
       " 'timestamp',\n",
       " 'text_lower',\n",
       " 'cashtags',\n",
       " 'contains_ipo',\n",
       " 'contains_cashtag',\n",
       " 'contains_cashtag_with_spaces',\n",
       " 'contains_company_name',\n",
       " 'retweet_pattern',\n",
       " 'contains_RT_pattern',\n",
       " 'user_id_str']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tickers=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_expression_no_numbers = '(\\s\\$[A-Za-z]{1,6})+'\n",
    "ticker_expression_including_numbers = '\\$[A-Za-z0-9]{1,6}'\n",
    "ipo_expression = '#ipo|ipo|#IPO|IPO'\n",
    "\n",
    "df_tickers = (df.withColumn(\"tickers\", regexp_extract('text', ticker_expression_no_numbers, 0))\n",
    "                .withColumn(\"contains_ipo\", regexp_extract('text', ipo_expression, 0))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 53613 tweets in the total set that containt the word ipo or a pattern that matches a ticker, including the dollar sign\n"
     ]
    }
   ],
   "source": [
    "# Filter full dataset for tweets that contain either a ticker-pattern or the IPO pattern\n",
    "df_tickers_and_ipo = df_tickers.filter((df_tickers[\"tickers\"] != \"\") | (df_tickers[\"contains_ipo\"] != \"\"))\n",
    "\n",
    "print(\"There are {} tweets in the total set that containt the word ipo or a pattern that matches a ticker, including the dollar sign\".format(df_tickers.count()))\n",
    "\n",
    "#df_tickers_and_ipo.select(\"tweet_id_str\", \"contains_ipo\", \"tickers\").show(5)\n",
    "#df.createOrReplaceTempView(\"dfSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the tweets that contain one or multiple tickers\n",
    "df_tickers = (df_tickers_and_ipo.filter(df_tickers_and_ipo[\"tickers\"] != \"\"))\n",
    "\n",
    "\n",
    "df_tickers = (df_tickers.select('contributors','coordinates','display_text_range','entities','extended_entities','extended_tweet','favorite_count','favorited','filter_level','geo','id','tweet_id_str','in_reply_to_screen_name','in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'possibly_sensitive', 'quote_count', 'quoted_status', 'quoted_status_id', 'quoted_status_id_str', 'reply_count', 'retweet_count', 'retweeted', 'retweeted_status', 'source', 'text', 'timestamp_ms', 'truncated', 'user', 'withheld_in_countries', 'timestamp', 'text_lower','cashtags','contains_ipo','contains_cashtag','contains_cashtag_with_spaces','contains_company_name','retweet_pattern','contains_RT_pattern','user_id_str', split(df_tickers.tickers, '\\s\\$').alias('tickers')))\n",
    "\n",
    "\n",
    "df_tickers = df_tickers.withColumn(\"ticker\", explode(df_tickers.tickers))\n",
    "df_tickers = df_tickers.filter(df_tickers.ticker != \"\")\n",
    "\n",
    "aggregated_tickers = (df_tickers.withColumn(\"ticker\", upper(df_tickers.ticker))\n",
    "                                .groupBy(\"ticker\").count()\n",
    "                                .withColumnRenamed('count', \"ticker_occurence\")\n",
    "                     )\n",
    "aggregated_tickers = aggregated_tickers.sort(aggregated_tickers.ticker_occurence.desc())\n",
    "\n",
    "#aggregated_tickers.coalesce(1).write.csv(\"ticker_occurence_2016\", mode = \"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/home/mma/Downloads/IPO_Mining/stock_mining/profit.csv' does not exist: b'/home/mma/Downloads/IPO_Mining/stock_mining/profit.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-65791c1e0ff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_profit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/mma/Downloads/IPO_Mining/stock_mining/profit.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/home/mma/Downloads/IPO_Mining/stock_mining/profit.csv' does not exist: b'/home/mma/Downloads/IPO_Mining/stock_mining/profit.csv'"
     ]
    }
   ],
   "source": [
    "df_profit = pd.read_csv(\"/home/mma/Downloads/IPO_Mining/stock_mining/profit.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TpjnbT4YqCej",
    "outputId": "beebb588-10ef-4eb0-f02c-410743bb1f6e"
   },
   "outputs": [],
   "source": [
    "#df.select(\"entities\").show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "olffHbljqCer"
   },
   "outputs": [],
   "source": [
    "#df.printSchema()\n",
    "#df.select(\"retweeted_status\").rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_CMQpGRBqCex"
   },
   "source": [
    "## <span style = 'color:darkgreen'> Extracting tickers & presence of IPO out of text </span style> <a name='tickers_ipo_extraction' />\n",
    "\n",
    " - containing $-sign in the text or containing ipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlmyLu62qCez"
   },
   "outputs": [],
   "source": [
    "'''# REGEX to extract tickers or presence of IPO\n",
    "ticker_expression_no_numbers = '(\\s\\$[A-Za-z]{1,6})+'\n",
    "ticker_expression_including_numbers = '\\$[A-Za-z0-9]{1,6}'\n",
    "ipo_expression = '#ipo|ipo|#IPO|IPO'\n",
    "\n",
    "df_tickers = (df.withColumn(\"tickers\", regexp_extract('text', ticker_expression_no_numbers, 0))\n",
    "                .withColumn(\"contains_ipo\", regexp_extract('text', ipo_expression, 0))\n",
    "            )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''df_tickers.columns'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3TQSFaFqCe5",
    "outputId": "e21dc80c-5e88-4cb0-af4d-c0bad4820182"
   },
   "outputs": [],
   "source": [
    "'''# Filter full dataset for tweets that contain either a ticker-pattern or the IPO pattern\n",
    "df_tickers_and_ipo = df_tickers.filter((df_tickers[\"tickers\"] != \"\") | (df_tickers[\"contains_ipo\"] != \"\"))\n",
    "\n",
    "print(\"There are {} tweets in the total set that containt the word ipo or a pattern that matches a ticker, including the dollar sign\".format(df_tickers.count()))\n",
    "\n",
    "#df_tickers_and_ipo.select(\"tweet_id_str\", \"contains_ipo\", \"tickers\").show(5)\n",
    "#df.createOrReplaceTempView(\"dfSQL\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEBvKpOQqCfD"
   },
   "outputs": [],
   "source": [
    "'''# Filter out the tweets that contain one or multiple tickers\n",
    "df_tickers = (df_tickers_and_ipo.filter(df_tickers_and_ipo[\"tickers\"] != \"\"))\n",
    "\n",
    "\n",
    "df_tickers = (df_tickers.select('contributors','coordinates','display_text_range','entities','extended_entities','extended_tweet','favorite_count','favorited','filter_level','geo','id','tweet_id_str','in_reply_to_screen_name','in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'possibly_sensitive', 'quote_count', 'quoted_status', 'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink', 'reply_count', 'retweet_count', 'retweeted', 'retweeted_status', 'source', 'text', 'timestamp_ms', 'truncated', 'user', 'withheld_in_countries', 'timestamp', 'text_lower','cashtags','contains_ipo','contains_cashtag','contains_cashtag_with_spaces','contains_company_name','retweet_pattern','contains_RT_pattern','user_id_str', split(df_tickers.tickers, '\\s\\$').alias('tickers')))\n",
    "\n",
    "df_tickers = df_tickers.withColumn(\"ticker\", explode(df_tickers.tickers))\n",
    "df_tickers = df_tickers.filter(df_tickers.ticker != \"\")\n",
    "\n",
    "aggregated_tickers = (df_tickers.withColumn(\"ticker\", upper(df_tickers.ticker))\n",
    "                                .groupBy(\"ticker\").count()\n",
    "                                .withColumnRenamed('count', \"ticker_occurence\")\n",
    "                     )\n",
    "aggregated_tickers = aggregated_tickers.sort(aggregated_tickers.ticker_occurence.desc())\n",
    "\n",
    "#aggregated_tickers.coalesce(1).write.csv(\"ticker_occurence_2016\", mode = \"overwrite\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojnzV8w4qCfJ",
    "outputId": "101a73ea-ba42-49f7-e66a-a767a3a7b196"
   },
   "outputs": [],
   "source": [
    "#counting the Tickers occurences\n",
    "'''aggregated_tickers.rdd.take(10)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_tickers.columns'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mma/Downloads/IPO_project/IPO_Tickers_List\n"
     ]
    }
   ],
   "source": [
    "cd /home/mma/Downloads/IPO_project/IPO_Tickers_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mma/\u001b/lib/python3.7/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "#Reading in the tickers files and appending into one dataframe\n",
    "tickers=pd.read_excel('IPO_data_final_2.xlsx')\n",
    "tickers=tickers.append(pd.read_excel('IPO_data_final.xlsx'))\n",
    "tickers=tickers.append(pd.read_excel('IPO_data_final_3.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_list=tickers.iloc[:,-2].to_list()# Subsetting tickers variable and converting into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_list = set(tickers_list) # removing duplicates from the tickers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Subsetting the df_tickers which contains only required tickers from tickers_list\n",
    "one_ticker_df = df_tickers.filter(df_tickers['ticker'].isin(tickers_list))\n",
    "# As this is in spark dataframe, Here converting into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_tickers'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''one_ticker_df'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ltD9LdOzpmI"
   },
   "source": [
    "# In the Below \"for\" loop you can put as many thickers you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing VADER sentiment package\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer() #Initiating the sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to get sentiment score from the text (tweets)\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    x = analyser.polarity_scores(sentence)\n",
    "    #print(\"{:-<40} {}\".format(sentence, str(score)))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions to clean the text that contains punctuations, \n",
    "def clean_text(text):\n",
    "    # Remove pattern that signifies retweet\n",
    "    clean_text = re.sub(pattern = \"RT\\s@[a-zA-Z0-9]+:\\s\", repl = \"\", string = text)\n",
    "    clean_text = re.sub(r\"\\'s\", \" \", clean_text)\n",
    "    clean_text = re.sub(r\"\\'ve\", \" have \", clean_text)\n",
    "    clean_text = re.sub(r\"can't\", \"cannot \", clean_text)\n",
    "    clean_text = re.sub(r\"n't\", \" not \", clean_text)\n",
    "    clean_text = re.sub(r\"i'm\", \"i am \", clean_text)\n",
    "    clean_text = re.sub(r\"\\'re\", \" are \", clean_text)\n",
    "    clean_text = re.sub(r\"\\'d\", \" would \", clean_text)\n",
    "    clean_text = re.sub(r\"\\'ll\", \" will \", clean_text)\n",
    "    clean_text = re.sub(pattern = \"\\$[A-Za-z]{1,5}\", repl = \" ticker \", string = clean_text)\n",
    "    clean_text = re.sub(pattern =  \"w/\", repl = \"with\", string = clean_text)\n",
    "    clean_text = re.sub(pattern = \"http(s)?:\\/\\/[^\\s]+\", repl = \"website\", string = clean_text)\n",
    "    clean_text = re.sub(pattern = \"[\\|?\\.\\[\\]\\\"'(\\n):\\$\\&\\#\\-\\%\\+]+\", repl = \" \", string = clean_text)\n",
    "    clean_text = re.sub(pattern = ' +', repl = ' ', string = clean_text)\n",
    "    clean_text = re.sub(pattern = \"@[A-Za-z0-9]+\", repl = \"user\", string= clean_text)\n",
    "    clean_text = clean_text.lower()\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets=pd.read_json(\"/home/mma/Downloads/IPO_project/commented_files/NIO.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "array = []\n",
    "tweets=pd.read_json(\"/home/mma/Downloads/IPO_project/commented_files/NIO.json\") \n",
    "for tweet in tweets.text:\n",
    "    score = sentiment_analyzer_scores(clean_text(tweet))\n",
    "    array.append(score['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6341, 0.0, 0.0, 0.0, 0.0, 0.0772, 0.0, -0.2023, -0.5837, 0.0, 0.0, 0.0772, 0.0, 0.0, 0.0, 0.0, 0.2023, 0.0, 0.0, -0.5837, 0.3612, -0.5837, -0.5837, 0.7418, 0.0, 0.7906, -0.5837, 0.3612, 0.0, -0.5267, 0.0, 0.0, 0.0, -0.3595, 0.0, 0.7418, 0.7713, 0.7418, 0.743, 0.7418, -0.0772, 0.0, -0.2023, 0.4767, 0.4767, 0.3818, 0.2023, -0.5837, 0.0, 0.0, 0.7906, 0.0, 0.0, 0.3818, 0.34, -0.2023, 0.0, 0.0, 0.0, 0.0, 0.7096, 0.4404, 0.0, 0.0, 0.0, -0.5267, 0.0, 0.0, 0.0, 0.7418, -0.4559, 0.0, 0.0, 0.0, 0.8547, 0.0, -0.5837, 0.0, 0.1531]\n"
     ]
    }
   ],
   "source": [
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json_dir_name = '/home/mma/Downloads/IPO_project/commented_files/' #file path\n",
    "json_pattern = os.path.join(json_dir_name,'*.json') #searching for the JSON files \n",
    "file_list = glob.glob(json_pattern) # GLob for Unix path finding\n",
    "#file_list= file_list[:100]\n",
    "\n",
    "np_array_values=[] # Inititating the array\n",
    "for files in file_list: #Loop to read all the files in the particular directory\n",
    "    #print(files)\n",
    " #As we have different versions of JSON files from Twitter. Here We found some corrupted files, \n",
    "#then we moved those files into another folder .Therefore we used \"try\",\"except\" \n",
    "    try:\n",
    "        tweets=pd.read_json(files) \n",
    "        for tweet in tweets.text:\n",
    "            score = sentiment_analyzer_scores(clean_text(tweet))\n",
    "            array.append(score['compound'])\n",
    "            #print(i)\n",
    "            #print(clean_text(one_ticker_df.text[i]))\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'DataFrame' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8d1781de1b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_ticker_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mone_ticker_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_ticker_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_analyzer_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_ticker_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'DataFrame' has no len()"
     ]
    }
   ],
   "source": [
    "print(len(one_ticker_df))\n",
    "array = []\n",
    "one_ticker_df = []\n",
    "for i in range(len(one_ticker_df)):\n",
    "    score = sentiment_analyzer_scores(clean_text(one_ticker_df.text[i]))\n",
    "    array.append(score['compound'])\n",
    "    #print(i)\n",
    "    #print(clean_text(one_ticker_df.text[i]))\n",
    "                \n",
    "print(array)\n",
    "print(len(array))\n",
    "print(one_ticker_df.text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4835, 47)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tickers.toPandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tms_AikDqCfq",
    "outputId": "71dee36e-48eb-4f49-8436-353518a539b6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker: nan\n",
      "0\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "|contributors|coordinates|display_text_range|entities|extended_entities|extended_tweet|favorite_count|favorited|filter_level|geo| id|tweet_id_str|in_reply_to_screen_name|in_reply_to_status_id|in_reply_to_status_id_str|in_reply_to_user_id|in_reply_to_user_id_str|is_quote_status|lang|place|possibly_sensitive|quote_count|quoted_status|quoted_status_id|quoted_status_id_str|reply_count|retweet_count|retweeted|retweeted_status|source|text|timestamp_ms|truncated|user|withheld_in_countries|timestamp|text_lower|cashtags|contains_ipo|contains_cashtag|contains_cashtag_with_spaces|contains_company_name|retweet_pattern|contains_RT_pattern|user_id_str|tickers|ticker|\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "\n",
      "Ticker: TDACU\n",
      "0\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "|contributors|coordinates|display_text_range|entities|extended_entities|extended_tweet|favorite_count|favorited|filter_level|geo| id|tweet_id_str|in_reply_to_screen_name|in_reply_to_status_id|in_reply_to_status_id_str|in_reply_to_user_id|in_reply_to_user_id_str|is_quote_status|lang|place|possibly_sensitive|quote_count|quoted_status|quoted_status_id|quoted_status_id_str|reply_count|retweet_count|retweeted|retweeted_status|source|text|timestamp_ms|truncated|user|withheld_in_countries|timestamp|text_lower|cashtags|contains_ipo|contains_cashtag|contains_cashtag_with_spaces|contains_company_name|retweet_pattern|contains_RT_pattern|user_id_str|tickers|ticker|\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "\n",
      "Ticker: SAFE\n",
      "0\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "|contributors|coordinates|display_text_range|entities|extended_entities|extended_tweet|favorite_count|favorited|filter_level|geo| id|tweet_id_str|in_reply_to_screen_name|in_reply_to_status_id|in_reply_to_status_id_str|in_reply_to_user_id|in_reply_to_user_id_str|is_quote_status|lang|place|possibly_sensitive|quote_count|quoted_status|quoted_status_id|quoted_status_id_str|reply_count|retweet_count|retweeted|retweeted_status|source|text|timestamp_ms|truncated|user|withheld_in_countries|timestamp|text_lower|cashtags|contains_ipo|contains_cashtag|contains_cashtag_with_spaces|contains_company_name|retweet_pattern|contains_RT_pattern|user_id_str|tickers|ticker|\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "\n",
      "Ticker: NEBUU\n",
      "0\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "|contributors|coordinates|display_text_range|entities|extended_entities|extended_tweet|favorite_count|favorited|filter_level|geo| id|tweet_id_str|in_reply_to_screen_name|in_reply_to_status_id|in_reply_to_status_id_str|in_reply_to_user_id|in_reply_to_user_id_str|is_quote_status|lang|place|possibly_sensitive|quote_count|quoted_status|quoted_status_id|quoted_status_id_str|reply_count|retweet_count|retweeted|retweeted_status|source|text|timestamp_ms|truncated|user|withheld_in_countries|timestamp|text_lower|cashtags|contains_ipo|contains_cashtag|contains_cashtag_with_spaces|contains_company_name|retweet_pattern|contains_RT_pattern|user_id_str|tickers|ticker|\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "\n",
      "Ticker: Yunji\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "|contributors|coordinates|display_text_range|entities|extended_entities|extended_tweet|favorite_count|favorited|filter_level|geo| id|tweet_id_str|in_reply_to_screen_name|in_reply_to_status_id|in_reply_to_status_id_str|in_reply_to_user_id|in_reply_to_user_id_str|is_quote_status|lang|place|possibly_sensitive|quote_count|quoted_status|quoted_status_id|quoted_status_id_str|reply_count|retweet_count|retweeted|retweeted_status|source|text|timestamp_ms|truncated|user|withheld_in_countries|timestamp|text_lower|cashtags|contains_ipo|contains_cashtag|contains_cashtag_with_spaces|contains_company_name|retweet_pattern|contains_RT_pattern|user_id_str|tickers|ticker|\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "\n",
      "Ticker: LBM\n",
      "0\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "|contributors|coordinates|display_text_range|entities|extended_entities|extended_tweet|favorite_count|favorited|filter_level|geo| id|tweet_id_str|in_reply_to_screen_name|in_reply_to_status_id|in_reply_to_status_id_str|in_reply_to_user_id|in_reply_to_user_id_str|is_quote_status|lang|place|possibly_sensitive|quote_count|quoted_status|quoted_status_id|quoted_status_id_str|reply_count|retweet_count|retweeted|retweeted_status|source|text|timestamp_ms|truncated|user|withheld_in_countries|timestamp|text_lower|cashtags|contains_ipo|contains_cashtag|contains_cashtag_with_spaces|contains_company_name|retweet_pattern|contains_RT_pattern|user_id_str|tickers|ticker|\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "\n",
      "Ticker: PRT\n",
      "0\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "|contributors|coordinates|display_text_range|entities|extended_entities|extended_tweet|favorite_count|favorited|filter_level|geo| id|tweet_id_str|in_reply_to_screen_name|in_reply_to_status_id|in_reply_to_status_id_str|in_reply_to_user_id|in_reply_to_user_id_str|is_quote_status|lang|place|possibly_sensitive|quote_count|quoted_status|quoted_status_id|quoted_status_id_str|reply_count|retweet_count|retweeted|retweeted_status|source|text|timestamp_ms|truncated|user|withheld_in_countries|timestamp|text_lower|cashtags|contains_ipo|contains_cashtag|contains_cashtag_with_spaces|contains_company_name|retweet_pattern|contains_RT_pattern|user_id_str|tickers|ticker|\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "+------------+-----------+------------------+--------+-----------------+--------------+--------------+---------+------------+---+---+------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------+-------------+---------+----------------+------+----+------------+---------+----+---------------------+---------+----------+--------+------------+----------------+----------------------------+---------------------+---------------+-------------------+-----------+-------+------+\n",
      "\n",
      "Ticker: GPMT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5c70161a9cc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mone_ticker_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tickers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tickers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ticker: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_ticker_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mone_ticker_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \"\"\"\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Loop to extract tweets with respect to tickers\n",
    "\n",
    "for t in tickers_list:\n",
    "    one_ticker_df = df_tickers.filter(df_tickers.ticker == t)\n",
    "    print(\"Ticker: {}\".format(t))\n",
    "    print(one_ticker_df.count())\n",
    "    one_ticker_df.show(10)\n",
    "    \n",
    "    #one_ticker_df.write.json(\"{}_tweets.json\".format(t), mode = \"overwrite\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDKlPYMfqCfz",
    "outputId": "9a1a1a25-64bf-4f3e-a7d8-aaddb0f442b9"
   },
   "outputs": [],
   "source": [
    "#df.filter(\"Viking Therapeutics\" in df.text)\n",
    "#df.filter(df.text.contains(\"Viking Therapeutics\")).collect() # DEZE WERK LOKAAL WEL MAAR NIET OP SERVER ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcT1SQUnqCgb"
   },
   "outputs": [],
   "source": [
    "#Creating user defined function te replace \";\" with \",\"\n",
    "replaceSemicolon = udf(lambda x: x.replace(\";\", \",\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w6VD_gnwqCgm"
   },
   "outputs": [],
   "source": [
    "#Identified \";\" in the Text variable, so apply replace Semicolon function.\n",
    "edit_df = df.withColumn(\"text\", replaceSemicolon(df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjhyCIXvqCgq"
   },
   "outputs": [],
   "source": [
    "#edit_df = edit_df.withColumn(\"text\", replaceSemicolon(edit_df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5VXuEOPfqCgv",
    "outputId": "401b67e1-dcb8-4cc7-f599-935ad576abb7"
   },
   "outputs": [],
   "source": [
    "#edit_df.rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Jt7VJShqCg4",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'CSV data source does not support struct<coordinates:array<double>,type:string> data type.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o872.csv.\n: org.apache.spark.sql.AnalysisException: CSV data source does not support struct<coordinates:array<double>,type:string> data type.;\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$$anonfun$verifySchema$1.apply(DataSourceUtils.scala:69)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$$anonfun$verifySchema$1.apply(DataSourceUtils.scala:67)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.verifySchema(DataSourceUtils.scala:67)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.verifyWriteSchema(DataSourceUtils.scala:34)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:100)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-3f227d3e2246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#edit_df.write.csv(\"editas_tweets.csv\", mode = \"overwrite\", sep = \";\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0medit_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/mnt/bdata1/twitter/editas_tweets3.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue)\u001b[0m\n\u001b[1;32m    925\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                        encoding=encoding, emptyValue=emptyValue)\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'CSV data source does not support struct<coordinates:array<double>,type:string> data type.;'"
     ]
    }
   ],
   "source": [
    "#edit_df.write.csv(\"editas_tweets.csv\", mode = \"overwrite\", sep = \";\")\n",
    "edit_df.write.csv(\"/mnt/bdata1/twitter/editas_tweets3.csv\").mode('overwrite').delimiter(\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wYwfGJb6qChW",
    "outputId": "7ed737a9-ce1f-4879-ead8-32ad3f959f89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4835"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXic-ojDqCh8"
   },
   "outputs": [],
   "source": [
    "users_df = (df_tickers.select(\"user_id_str\")\n",
    "                   .groupBy(\"user_id_str\")\n",
    "                   .count()\n",
    "                   .withColumnRenamed(\"count\", \"nbr_tweets_with_ticker\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IaeJv9yxqCiA"
   },
   "outputs": [],
   "source": [
    "\n",
    "stockbard_id = \"738550990698336261\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C4NIy14UqCiK",
    "outputId": "2a8fdb45-6476-4e5d-8101-090df5bde86b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+\n",
      "|        user_id_str|nbr_tweets_with_ticker|\n",
      "+-------------------+----------------------+\n",
      "|         2422703600|                   153|\n",
      "|           49637104|                   146|\n",
      "|1085491709188980736|                    62|\n",
      "|1087232853094494208|                    60|\n",
      "|         2964287876|                    60|\n",
      "|           23059499|                    53|\n",
      "|           72928001|                    50|\n",
      "|           42912883|                    48|\n",
      "|1015088125603926018|                    44|\n",
      "|         1664138053|                    40|\n",
      "+-------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#counting number of tweets per userID\n",
    "users_df = (users_df.sort(users_df.nbr_tweets_with_ticker.desc())\n",
    "            )\n",
    "\n",
    "users_df.filter(users_df.user_id_str != stockbard_id)\n",
    "\n",
    "users_df.count()\n",
    "\n",
    "users_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "FFruXN4MqCia"
   },
   "source": [
    "# <span style = 'color:darkred'> Language </span style>\n",
    "# check df_tickers from where it comes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWVfLVndqCid",
    "outputId": "39899727-a1bc-41bb-86b2-b438f0f3ceea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|lang|languageCount|\n",
      "+----+-------------+\n",
      "|  en|         4835|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#counting the Tweets by language\n",
    "df_lang = (df_tickers.select(\"lang\") \n",
    "                .groupBy(\"lang\")\n",
    "                .count()\n",
    "                .withColumnRenamed(\"count\", \"languageCount\")\n",
    "                .orderBy(\"count\", ascending = False)\n",
    "    )\n",
    "\n",
    "\n",
    "df_lang.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "yUnXd6jiqCim"
   },
   "source": [
    "# <span style = 'color:darkred'> Distribution of tweets over time </span style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRc958kbqCjF"
   },
   "outputs": [],
   "source": [
    "# UDF to convert created_at (String) to created_at_datetime (TimestampType)\n",
    "\n",
    "def created_at_string_to_timestamp(x):\n",
    "    return datetime.strptime(x,'%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "to_datetimestamp =  udf (created_at_string_to_timestamp, TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jyA4_fO8qCj-",
    "outputId": "28504c27-bf9c-45f5-f599-617d6fa5baa0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contributors',\n",
       " 'coordinates',\n",
       " 'display_text_range',\n",
       " 'entities',\n",
       " 'extended_entities',\n",
       " 'extended_tweet',\n",
       " 'favorite_count',\n",
       " 'favorited',\n",
       " 'filter_level',\n",
       " 'geo',\n",
       " 'id',\n",
       " 'tweet_id_str',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_status_id_str',\n",
       " 'in_reply_to_user_id',\n",
       " 'in_reply_to_user_id_str',\n",
       " 'is_quote_status',\n",
       " 'lang',\n",
       " 'place',\n",
       " 'possibly_sensitive',\n",
       " 'quote_count',\n",
       " 'quoted_status',\n",
       " 'quoted_status_id',\n",
       " 'quoted_status_id_str',\n",
       " 'reply_count',\n",
       " 'retweet_count',\n",
       " 'retweeted',\n",
       " 'retweeted_status',\n",
       " 'source',\n",
       " 'text',\n",
       " 'timestamp_ms',\n",
       " 'truncated',\n",
       " 'user',\n",
       " 'withheld_in_countries',\n",
       " 'timestamp',\n",
       " 'text_lower',\n",
       " 'cashtags',\n",
       " 'contains_ipo',\n",
       " 'contains_cashtag',\n",
       " 'contains_cashtag_with_spaces',\n",
       " 'contains_company_name',\n",
       " 'retweet_pattern',\n",
       " 'contains_RT_pattern',\n",
       " 'user_id_str',\n",
       " 'tickers',\n",
       " 'ticker']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assigning  df_tickers= df\n",
    "df = df_tickers\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o988.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 60.0 failed 1 times, most recent failure: Lost task 3.0 in stage 60.0 (TID 1506, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 83, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-42-8797a5a80ae5>\", line 4, in created_at_string_to_timestamp\n  File \"/home/mma/\u001b/lib/python3.7/_strptime.py\", line 577, in _strptime_datetime\n    tt, fraction, gmtoff_fraction = _strptime(data_string, format)\n  File \"/home/mma/\u001b/lib/python3.7/_strptime.py\", line 359, in _strptime\n    (data_string, format))\nValueError: time data '1556333351377' does not match format '%a %b %d %H:%M:%S +0000 %Y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3257)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 83, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-42-8797a5a80ae5>\", line 4, in created_at_string_to_timestamp\n  File \"/home/mma/\u001b/lib/python3.7/_strptime.py\", line 577, in _strptime_datetime\n    tt, fraction, gmtoff_fraction = _strptime(data_string, format)\n  File \"/home/mma/\u001b/lib/python3.7/_strptime.py\", line 359, in _strptime\n    (data_string, format))\nValueError: time data '1556333351377' does not match format '%a %b %d %H:%M:%S +0000 %Y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-f33ec2dd4369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_time_pandas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_time_pandas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_time_pandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/%s/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdayofmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%d/%m/%Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2142\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \"\"\"\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/\u001b/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o988.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 60.0 failed 1 times, most recent failure: Lost task 3.0 in stage 60.0 (TID 1506, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 83, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-42-8797a5a80ae5>\", line 4, in created_at_string_to_timestamp\n  File \"/home/mma/\u001b/lib/python3.7/_strptime.py\", line 577, in _strptime_datetime\n    tt, fraction, gmtoff_fraction = _strptime(data_string, format)\n  File \"/home/mma/\u001b/lib/python3.7/_strptime.py\", line 359, in _strptime\n    (data_string, format))\nValueError: time data '1556333351377' does not match format '%a %b %d %H:%M:%S +0000 %Y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3257)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 83, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/home/mma/\u001b/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-42-8797a5a80ae5>\", line 4, in created_at_string_to_timestamp\n  File \"/home/mma/\u001b/lib/python3.7/_strptime.py\", line 577, in _strptime_datetime\n    tt, fraction, gmtoff_fraction = _strptime(data_string, format)\n  File \"/home/mma/\u001b/lib/python3.7/_strptime.py\", line 359, in _strptime\n    (data_string, format))\nValueError: time data '1556333351377' does not match format '%a %b %d %H:%M:%S +0000 %Y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df_time_pandas = df_time.toPandas()\n",
    "df_time_pandas['date'] = df_time_pandas.apply(lambda x: datetime.strptime('%s/%s/%s' % (x.dayofmonth, x.month, x.year), '%d/%m/%Y'),axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHD54wLRqCkT",
    "outputId": "8e24343d-2e9c-4ce2-bc1d-5b98bbd643e8"
   },
   "outputs": [],
   "source": [
    "#get_ipython().run_cell_magic('time', '', '\\ndf = df.withColumn(\\'created_at_timestamp\\', to_datetimestamp(df.created_at))\\n\\ndf_time = (df.select(year(\"created_at_timestamp\").alias(\"year\"),\\n                 month(\"created_at_timestamp\").alias(\"month\"),\\n                 dayofmonth(\"created_at_timestamp\").alias(\"dayofmonth\")\\n                 )\\n            .groupBy(\"dayofmonth\", \"month\", \"year\")\\n            .count()\\n            .withColumnRenamed(\"count\", \"tweets_per_day\")\\n            .orderBy(\"year\", \"month\", \"dayofmonth\", ascending = True)\\n)\\n\\ndf_time_pandas = df_time.toPandas()\\ndf_time_pandas[\\'date\\'] = df_time_pandas.apply(lambda x: datetime.strptime(\\'%s/%s/%s\\' % (x.dayofmonth, x.month, x.year), \\'%d/%m/%Y\\'),axis=1)\\n\\n#df_time_pandas')\n",
    "df = df.withColumn('created_at_timestamp', to_datetimestamp(df.timestamp_ms))\n",
    "\n",
    "df_time = (df.select(year(\"created_at_timestamp\").alias(\"year\"),\n",
    "month(\"created_at_timestamp\").alias(\"month\"),\n",
    "dayofmonth(\"created_at_timestamp\").alias(\"dayofmonth\")\n",
    ")\n",
    ".groupBy(\"dayofmonth\", \"month\", \"year\")\n",
    ".count()\n",
    ".withColumnRenamed(\"count\", \"tweets_per_day\")\n",
    ".orderBy(\"year\", \"month\", \"dayofmonth\", ascending = True)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contributors',\n",
       " 'coordinates',\n",
       " 'display_text_range',\n",
       " 'entities',\n",
       " 'extended_entities',\n",
       " 'extended_tweet',\n",
       " 'favorite_count',\n",
       " 'favorited',\n",
       " 'filter_level',\n",
       " 'geo',\n",
       " 'id',\n",
       " 'tweet_id_str',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_status_id_str',\n",
       " 'in_reply_to_user_id',\n",
       " 'in_reply_to_user_id_str',\n",
       " 'is_quote_status',\n",
       " 'lang',\n",
       " 'place',\n",
       " 'possibly_sensitive',\n",
       " 'quote_count',\n",
       " 'quoted_status',\n",
       " 'quoted_status_id',\n",
       " 'quoted_status_id_str',\n",
       " 'reply_count',\n",
       " 'retweet_count',\n",
       " 'retweeted',\n",
       " 'retweeted_status',\n",
       " 'source',\n",
       " 'text',\n",
       " 'timestamp_ms',\n",
       " 'truncated',\n",
       " 'user',\n",
       " 'withheld_in_countries',\n",
       " 'timestamp',\n",
       " 'text_lower',\n",
       " 'cashtags',\n",
       " 'contains_ipo',\n",
       " 'contains_cashtag',\n",
       " 'contains_cashtag_with_spaces',\n",
       " 'contains_company_name',\n",
       " 'retweet_pattern',\n",
       " 'contains_RT_pattern',\n",
       " 'user_id_str',\n",
       " 'tickers',\n",
       " 'ticker',\n",
       " 'created_at_timestamp']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xg38oD7LqCka",
    "outputId": "da63fd25-6936-43f7-a6b2-e85eaed410be"
   },
   "outputs": [],
   "source": [
    "#creating new variable for tweets in thousands\n",
    "df_time_pandas[\"tweets_per_day_thousands\"] = df_time_pandas[\"tweets_per_day\"].apply(lambda x: x/float(1000))\n",
    "\n",
    "#function to chexk if the tweet is from first day of the month\n",
    "def labels(x):\n",
    "    if x.day == 1:\n",
    "        return str(x)[0:10]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#Calling the 'label' function \n",
    "df_time_pandas[\"label_x_axis\"] = df_time_pandas[\"date\"].apply(lambda x: labels(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGYGeLaeqCkh",
    "outputId": "27d0e54b-97dd-4234-cb68-18a67b1a342e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_time_pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-0dd100d4da60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plotting number of tweets as per days\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_time_pandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweets_per_day\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_time_pandas' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting number of tweets as per days\n",
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(df_time_pandas.tweets_per_day)\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of tweets\")\n",
    "\n",
    "plt.ylim(ymin = 0, ymax = 4000)\n",
    "\n",
    "plt.title(\"Tweets per day with a ticker\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "HgTIdB3MqCkp"
   },
   "source": [
    "# <span style = 'color:darkred'> Hashtags </span style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13ixqDhMqClB"
   },
   "outputs": [],
   "source": [
    "#df_hashtags = (df.select('tweet_id_str', \"entities\")\n",
    "#                 .withColumnRenamed(\"text\", \"hashtag_array\")\n",
    "#              )\n",
    "\n",
    "#df_hashtags = df_hashtags.select(\"tweet_id_str\", explode(df_hashtags.hashtag_array).alias(\"hashtags\"))\n",
    "\n",
    "#df_hashtags = df_hashtags.select(\"tweet_id_str\", lower(df_hashtags['hashtags']).alias(\"hashtags\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovtTsTX7qClh"
   },
   "outputs": [],
   "source": [
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1T-34FD5qClo"
   },
   "outputs": [],
   "source": [
    "\n",
    "#df_hashtags.createOrReplaceTempView(\"hashtagsSQL\")\n",
    " \n",
    "#df_top = spark.sql(\"SELECT hashtags, count(hashtags) as count FROM hashtagsSQL GROUP BY hashtags ORDER BY 2 DESC \").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4D7j1Y_3qClt",
    "outputId": "afb1401d-502d-4406-84bd-60f7eb7532ae"
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VeR6ZnC4qCmF",
    "outputId": "b96fe699-fdaf-421f-e4d5-a454c10cd951"
   },
   "outputs": [],
   "source": [
    "df_time_pandas.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u4ZnnS-sqCqP"
   },
   "outputs": [],
   "source": [
    "def sentiment:\n",
    "    \n",
    "\n",
    "\n",
    "f_sent = udf(sentiment, string())\n",
    "result_sentiment = df.withColumn('text', sentiment(result.x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0CML76StqCqV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bottom-Up-Analysis.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
