{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This document/.ipynb file was originaly created by Sacha Dubrulle in Python 2 \n",
    "# and further worked upon and converted to Python 3 by Thomas Valcke.\n",
    "# This notebook has been converted to Python 3 because the syntax changed with Python 3.\n",
    "\n",
    "# This notebook handels the data preperation and is the largest of the files.\n",
    "# 5 Problems remain:\n",
    "\n",
    "# 1. # Mine stock prices -> this gets its data from Yahoo.finance, This has been discontinued so can not work.\n",
    "\n",
    "# 2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = 'color:darkred'> Imports </span style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession initialized\n"
     ]
    }
   ],
   "source": [
    "################################ NEW IMPORT ###############################\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "#os.environ['SPARK_HOME'] = 'D:/School/STAGE_BP/spark'\n",
    "################################ JUPYTER ###################################\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image\n",
    "import imp\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "################################ SPARK ###################################\n",
    "\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "\n",
    "try:\n",
    "    sc = pyspark.SparkContext(appName = 'App')\n",
    "    spark = SparkSession(sparkContext=sc)\n",
    "    print(\"SparkSession initialized\")\n",
    "except ValueError:\n",
    "    print(\"SparkSession already initialized\")\n",
    "    \n",
    "    \n",
    "from pyspark.sql.functions import udf, explode, lower, regexp_extract, split, col, regexp_replace,upper\n",
    "from pyspark.sql.types import StringType, TimestampType, IntegerType, ArrayType, DoubleType, BooleanType\n",
    "from pyspark.ml.feature import Binarizer, StringIndexer, StringIndexerModel, OneHotEncoder, MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "################################ PYTHON ###################################\n",
    "    \n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", -1)\n",
    "\n",
    "import numpy as np\n",
    "import urllib\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.dates import YearLocator, MonthLocator, DayLocator, WeekdayLocator, DateFormatter\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "################################ Alpha Vantage API ###################################\n",
    "import requests\n",
    "import alpha_vantage\n",
    "import csv\n",
    "#This is our current api key, you might have to request a new one\n",
    "AV_API_key=\"OES7ORJTQ21CYZM9\"\n",
    "AV_API_URL = \"https://www.alphavantage.co/query\" \n",
    "from datetime import datetime\n",
    "date_start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=pyspark.sql.SparkSession.builder.appName('Bottom-Up-Analysis').config('spark.driver.memory','12G').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We initialize some colors for the graphs\n",
    "colors = {\"orange\": np.array([255,128,14], dtype = np.float16),\n",
    "          \"blue\": np.array([0,107,164], dtype = np.float16),\n",
    "          \"green\": np.array([44, 160, 44], dtype = np.float16)\n",
    "         }\n",
    "\n",
    "for k in list(colors.keys()):\n",
    "    colors[k] = np.divide(colors[k], 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on data that was mined from twitter. Primarily, twitter was mined using a list of keywords, all related to Initial Public Offerings, herinafter IPO's. The list of keywords contained the names and abbreviations or tickers for a number of companies that were know to perform their IPO soon, but also using some more generic words related to IPO's. The data ranges from the end of March 2016 until the end of march 2017 and hence comprises an entire year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <span style = 'color:darkred'> Import functions & make udfs </span style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mma/Downloads/IPO_project/commented_files\n"
     ]
    }
   ],
   "source": [
    "cd /home/mma/Downloads/IPO_project/commented_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in functions from external file written in python and should be present in the same directory,\n",
    "# Where this file is running from\n",
    "import functions as f_ipo\n",
    "\n",
    "f_ipo = imp.reload(f_ipo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment Below Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform paterns. We are defining functions, with the functions we just imported.\n",
    "\n",
    "#Extracting all cashtags from the tweets\n",
    "udf_cashtag_re_extract = udf(f_ipo.cashtag_re_extract, ArrayType(elementType = StringType()))\n",
    "#Gives the count of the mentions of the company's name\n",
    "udf_contains_company_name = udf(f_ipo.contains_company_name, IntegerType())\n",
    "non_empty_column_udf = udf(f_ipo.non_empty_column)\n",
    "udf_contains_pattern = udf(f_ipo.contains_pattern)\n",
    "\n",
    "# Define UDF to transform timestamp from StringType to TimestampType\n",
    "udf_datatime = udf(lambda string: datetime.strptime(string, \"%a %b %d %H:%M:%S +0000 %Y\"), TimestampType())\n",
    "\n",
    "udf_day_of_week = udf(lambda x: str(x.strftime('%A')), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil # Package to move files by python commands\n",
    "json_dir_name = '/home/mma/Downloads/IPO_project/tweets/' #file path\n",
    "json_pattern = os.path.join(json_dir_name,'*.json') #searching for the JSON files \n",
    "file_list = glob.glob(json_pattern) # GLob for Unix path finding\n",
    "#file_list= file_list[:100]\n",
    "\n",
    "np_array_values=[] # Inititating the array\n",
    "for files in file_list: #Loop to read all the files in the particular directory\n",
    "    #print(files)\n",
    " #As we have different versions of JSON files from Twitter. Here We found some corrupted files, \n",
    "#then we moved those files into another folder .Therefore we used \"try\",\"except\" \n",
    "    try:\n",
    "        tweets=spark.read.json(files)\n",
    "        if len(tweets.columns) > 27: # while merging all the files must contain equal number variables\n",
    "            np_array_values.append(tweets)\n",
    "        else:\n",
    "            print(\"not enough columns for file {}\".format(files))\n",
    "            #Here we are moving all the files those contain less than 28 Variables\n",
    "            shutil.move(files, \"/home/mma/Downloads/IPO_project/tweets/corrupt_files/{}\".format(files[-19:]))\n",
    "    except:  # Skips the step in any case if the file contains error in the folder\n",
    "        print(\"error with {}\".format(files))\n",
    "        shutil.move(files, \"/home/mma/Downloads/IPO_project/tweets/corrupt_files/{}\".format(files[-19:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = 'color:darkred'> Read in and basic transformations </span style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered dataset contains 1610431 tweets\n",
      "\n",
      "The columns in this dataset are the following:\n",
      "['_corrupt_record', 'contributors', 'coordinates', 'display_text_range', 'entities', 'extended_entities', 'extended_tweet', 'favorite_count', 'favorited', 'filter_level', 'geo', 'id', 'tweet_id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'possibly_sensitive', 'quote_count', 'quoted_status', 'quoted_status_id', 'quoted_status_id_str', 'reply_count', 'retweet_count', 'retweeted', 'retweeted_status', 'source', 'text', 'timestamp_ms', 'truncated', 'user', 'withheld_copyright', 'withheld_in_countries', 'withheld_scope', 'timestamp', 'text_lower', 'cashtags', 'contains_ipo', 'contains_cashtag', 'contains_cashtag_with_spaces', 'contains_company_name', 'retweet_pattern', 'contains_RT_pattern', 'user_id_str']\n"
     ]
    }
   ],
   "source": [
    "#cashtag_expression is a regex that filters out everything that starts with $ and has 1-6 letters\n",
    "cashtag_expression = \"(\\$[a-z]{1,6})+\"\n",
    "cashtag_expression_including_spaces = '\\s\\$[a-z]{1,6}\\s' #add spaces to the cashtag\n",
    "ipo_expression = ' #ipo | ipo ' \n",
    "\n",
    "#reads all companies in the list we've provided\n",
    "with open(\"/home/mma/Downloads/IPO_project/company_names.txt\", \"r\") as f:\n",
    "    companies_list = f.readlines() \n",
    "    companies_list = [x.replace(\"\\n\", \"\").lower() for x in companies_list]\n",
    "\n",
    "    \n",
    "# Here I used the provided IPO tweets, \n",
    "# i am not 100% this is the data set used by the creator of this project but it runs and it is the closest i got.\n",
    "# However if this data is not the same as used by the creator this would explain a lot of the random errors...\n",
    "# original directory on the creators Laptop: /home/sachadubrulle/Data/tweets_IPO/done_batches/M*B3/*.json\n",
    "# directory on the retailp server : /mnt/bdata1/twitter/archief/ipo/tweets/*.json\n",
    "# other copy of IPO files :         /mnt/bdata4/ipo/tweets/*.json\n",
    "df = (spark.read.json('/home/mma/Downloads/IPO_project/tweets/*.json')\n",
    "\n",
    "      \n",
    "        # Add lower text column and timestamp\n",
    "        .select(\"*\", \n",
    "               udf_datatime(col(\"created_at\")).alias(\"timestamp\"),\n",
    "               lower(col(\"text\")).alias(\"text_lower\")\n",
    "               )\n",
    "        # Drop created_at string column\n",
    "        .drop(\"created_at\")\n",
    "        # Add columns related to ipo, cashtags & retweet patterns\n",
    "        .select(\"*\", \n",
    "               udf_cashtag_re_extract('text_lower').alias(\"cashtags\"),\n",
    "               non_empty_column_udf(regexp_extract('text_lower', ipo_expression, 0)).alias(\"contains_ipo\"),\n",
    "               non_empty_column_udf(regexp_extract('text_lower', cashtag_expression, 0)).alias(\"contains_cashtag\"),\n",
    "               non_empty_column_udf(regexp_extract('text_lower', cashtag_expression_including_spaces, 0)).alias(\"contains_cashtag_with_spaces\"),\n",
    "               udf_contains_company_name(\"text_lower\").alias(\"contains_company_name\"),\n",
    "               regexp_extract(str = \"text\", pattern = \"^RT @[A-Za-z0-9]+: \", idx = 0).alias(\"retweet_pattern\")\n",
    "               )\n",
    "        .select(\"*\", \n",
    "               udf_contains_pattern(\"retweet_pattern\").alias(\"contains_RT_pattern\")\n",
    "              )\n",
    "     )\n",
    "df = (df.withColumnRenamed(\"id_str\", \"tweet_id_str\") #rename to avoid confusion with user_id, retweet_id, ...\n",
    "        .withColumn(\"user_id_str\", df.user.id_str) \n",
    "        .select(\"*\")\n",
    "     )\n",
    "\n",
    "#df.cache()\n",
    "\n",
    "#display(df['_corrupt_record'])\n",
    "nbr_total = df.count()\n",
    "print(\"The filtered dataset contains {} tweets\".format(nbr_total))\n",
    "print(\"\\nThe columns in this dataset are the following:\\n{}\".format(str(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check about below columns?????\n",
    "#df.select(\"contains_ipo\",\"contains_cashtag\",\"contains_cashtag_with_spaces\").filter(\"contains_cashtag > 0\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mma/\u001b/lib/python3.7/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "tickers=pd.read_excel('/home/mma/Downloads/IPO_project/IPO_Tickers_List/IPO_data_final_2.xlsx')\n",
    "tickers=tickers.append(pd.read_excel('/home/mma/Downloads/IPO_project/IPO_Tickers_List/IPO_data_final.xlsx'))\n",
    "tickers=tickers.append(pd.read_excel('/home/mma/Downloads/IPO_project/IPO_Tickers_List/IPO_data_final_3.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exchange</th>\n",
       "      <th>IPODate</th>\n",
       "      <th>IPOPrice</th>\n",
       "      <th>LockupExpiration</th>\n",
       "      <th>Name</th>\n",
       "      <th>Name</th>\n",
       "      <th>QuietPeriodExpiration</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (5/6/2016)</td>\n",
       "      <td>$12.00</td>\n",
       "      <td>11/2/2016</td>\n",
       "      <td>SPRING BANK PHARMACEUTICALS, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/15/2016</td>\n",
       "      <td>SBPH</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/spring-bank-pharmaceuticals-inc-898133-80118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (4/7/2016)</td>\n",
       "      <td>$10.00</td>\n",
       "      <td>10/4/2016</td>\n",
       "      <td>AEGLEA BIOTHERAPEUTICS, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/17/2016</td>\n",
       "      <td>AGLE</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/aeglea-biotherapeutics-inc-961076-78705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NYSE</td>\n",
       "      <td>Priced (4/21/2016)</td>\n",
       "      <td>$22.00</td>\n",
       "      <td>10/18/2016</td>\n",
       "      <td>AMERICAN RENAL ASSOCIATES HOLDINGS, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/31/2016</td>\n",
       "      <td>ARA</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/american-renal-associates-holdings-inc-835298-79295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NYSE</td>\n",
       "      <td>Priced (10/6/2016)</td>\n",
       "      <td>$18.00</td>\n",
       "      <td>4/4/2017</td>\n",
       "      <td>AQUAVENTURE HOLDINGS LTD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/15/2016</td>\n",
       "      <td>WAAS</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/aquaventure-holdings-ltd-766355-79466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NYSE</td>\n",
       "      <td>Priced (3/17/2017)</td>\n",
       "      <td>$14.00</td>\n",
       "      <td>9/13/2017</td>\n",
       "      <td>PROPETRO HOLDING CORP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4/26/2017</td>\n",
       "      <td>PUMP</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/propetro-holding-corp-999784-82763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (7/20/2016)</td>\n",
       "      <td>$15.00</td>\n",
       "      <td>1/16/2017</td>\n",
       "      <td>AUDENTES THERAPEUTICS, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/29/2016</td>\n",
       "      <td>BOLD</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/audentes-therapeutics-inc-953394-80103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NYSE</td>\n",
       "      <td>Priced (10/12/2016)</td>\n",
       "      <td>$18.00</td>\n",
       "      <td>4/10/2017</td>\n",
       "      <td>AZURE POWER GLOBAL LTD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/21/2016</td>\n",
       "      <td>AZRE</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/azure-power-global-ltd-981770-80005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (3/23/2016)</td>\n",
       "      <td>$15.00</td>\n",
       "      <td>9/19/2016</td>\n",
       "      <td>CORVUS PHARMACEUTICALS, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/2/2016</td>\n",
       "      <td>CRVS</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/corvus-pharmaceuticals-inc-951574-80105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (2/3/2016)</td>\n",
       "      <td>$16.00</td>\n",
       "      <td>8/1/2016</td>\n",
       "      <td>EDITAS MEDICINE, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3/14/2016</td>\n",
       "      <td>EDIT</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/editas-medicine-inc-973356-80107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NYSE</td>\n",
       "      <td>Filed (9/24/2014)</td>\n",
       "      <td>&lt;span class=\"nodata\"&gt; -- &lt;/span&gt;</td>\n",
       "      <td>&lt;span class=\"nodata\"&gt; -- &lt;/span&gt;</td>\n",
       "      <td>EXMAR ENERGY PARTNERS LP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;span class=\"nodata\"&gt; -- &lt;/span&gt;</td>\n",
       "      <td>XMLP</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/exmar-energy-partners-lp-945504-76551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NYSE</td>\n",
       "      <td>Priced (4/27/2017)</td>\n",
       "      <td>$21.00</td>\n",
       "      <td>10/24/2017</td>\n",
       "      <td>FLOOR &amp;amp;amp; DECOR HOLDINGS, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/6/2017</td>\n",
       "      <td>FND</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/floor-decor-holdings-inc-842788-82785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NYSE</td>\n",
       "      <td>Priced (5/19/2016)</td>\n",
       "      <td>$11.00</td>\n",
       "      <td>11/15/2016</td>\n",
       "      <td>GRUPO SUPERVIELLE S.A.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/28/2016</td>\n",
       "      <td>SUPV</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/grupo-supervielle-sa-983312-80119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (7/7/2015)</td>\n",
       "      <td>$7.00</td>\n",
       "      <td>1/4/2016</td>\n",
       "      <td>HAILIANG EDUCATION GROUP INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/17/2015</td>\n",
       "      <td>HLG</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/hailiang-education-group-inc-953462-77324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NASDAQ Global Select</td>\n",
       "      <td>Priced (2/1/2017)</td>\n",
       "      <td>$14.00</td>\n",
       "      <td>7/31/2017</td>\n",
       "      <td>LAUREATE EDUCATION, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3/13/2017</td>\n",
       "      <td>LAUR</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/laureate-education-inc-3789-79506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (7/23/2015)</td>\n",
       "      <td>$17.00</td>\n",
       "      <td>1/19/2016</td>\n",
       "      <td>LIVE OAK BANCSHARES, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/1/2015</td>\n",
       "      <td>LOB</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/live-oak-bancshares-inc-803316-78750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NASDAQ Global Select</td>\n",
       "      <td>Priced (10/14/2016)</td>\n",
       "      <td>$15.00</td>\n",
       "      <td>4/12/2017</td>\n",
       "      <td>MAMMOTH ENERGY SERVICES, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/23/2016</td>\n",
       "      <td>TUSK</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/mammoth-energy-services-inc-1002759-81637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (2/22/2002)</td>\n",
       "      <td>$19.00</td>\n",
       "      <td>8/21/2002</td>\n",
       "      <td>PETCO ANIMAL SUPPLIES INC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3/19/2002</td>\n",
       "      <td>PETC</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/petco-animal-supplies-inc-9730-16041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NYSE</td>\n",
       "      <td>Filed (4/9/2014)</td>\n",
       "      <td>&lt;span class=\"nodata\"&gt; -- &lt;/span&gt;</td>\n",
       "      <td>&lt;span class=\"nodata\"&gt; -- &lt;/span&gt;</td>\n",
       "      <td>PRINCIPAL MARITIME TANKERS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;span class=\"nodata\"&gt; -- &lt;/span&gt;</td>\n",
       "      <td>PMAR</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/principal-maritime-tankers-corp-931869-75119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (4/21/2004)</td>\n",
       "      <td>$10.50</td>\n",
       "      <td>10/18/2004</td>\n",
       "      <td>PROCENTURY CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/17/2004</td>\n",
       "      <td>PROS</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/procentury-corp-610917-36920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NYSE</td>\n",
       "      <td>Withdrawn (8/8/2018)</td>\n",
       "      <td>12.00-13.00</td>\n",
       "      <td>&lt;span class=\"nodata\"&gt; -- &lt;/span&gt;</td>\n",
       "      <td>PSAV, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;span class=\"nodata\"&gt; -- &lt;/span&gt;</td>\n",
       "      <td>PSAV</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/psav-inc-974802-79354?tab=news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (5/18/2016)</td>\n",
       "      <td>$4.00</td>\n",
       "      <td>11/14/2016</td>\n",
       "      <td>PULSE BIOSCIENCES, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/27/2016</td>\n",
       "      <td>PLSE</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/pulse-biosciences-inc-949419-80045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Filed (11/17/2015)</td>\n",
       "      <td>$10.00</td>\n",
       "      <td>&lt;span class=\"nodata\"&gt; -- &lt;/span&gt;</td>\n",
       "      <td>PULTE ACQUISITION CORP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;span class=\"nodata\"&gt; -- &lt;/span&gt;</td>\n",
       "      <td>PLTEU</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/pulte-acquisition-corp-979789-79840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (5/26/2016)</td>\n",
       "      <td>$11.00</td>\n",
       "      <td>11/22/2016</td>\n",
       "      <td>REATA PHARMACEUTICALS INC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7/5/2016</td>\n",
       "      <td>RETA</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/reata-pharmaceuticals-inc-704859-80104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (4/27/2016)</td>\n",
       "      <td>$19.50</td>\n",
       "      <td>10/24/2016</td>\n",
       "      <td>RED ROCK RESORTS, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/6/2016</td>\n",
       "      <td>RRR</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/red-rock-resorts-inc-977107-79572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NASDAQ Global</td>\n",
       "      <td>Priced (10/5/2017)</td>\n",
       "      <td>$17.00</td>\n",
       "      <td>4/3/2018</td>\n",
       "      <td>RHYTHM PHARMACEUTICALS, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/14/2017</td>\n",
       "      <td>RYTM</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/rhythm-pharmaceuticals-inc-972920-84659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (5/6/2016)</td>\n",
       "      <td>$12.00</td>\n",
       "      <td>11/2/2016</td>\n",
       "      <td>SPRING BANK PHARMACEUTICALS, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/15/2016</td>\n",
       "      <td>SBPH</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/spring-bank-pharmaceuticals-inc-898133-80118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (3/3/2016)</td>\n",
       "      <td>$12.00</td>\n",
       "      <td>8/30/2016</td>\n",
       "      <td>SYNDAX PHARMACEUTICALS INC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4/12/2016</td>\n",
       "      <td>SNDX</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/syndax-pharmaceuticals-inc-740721-80110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NASDAQ Global</td>\n",
       "      <td>Priced (9/29/2016)</td>\n",
       "      <td>$12.00</td>\n",
       "      <td>3/28/2017</td>\n",
       "      <td>TABULA RASA HEALTHCARE, INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/8/2016</td>\n",
       "      <td>TRHC</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/tabula-rasa-healthcare-inc-983094-80108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (7/28/2016)</td>\n",
       "      <td>$10.00</td>\n",
       "      <td>1/24/2017</td>\n",
       "      <td>TACTILE SYSTEMS TECHNOLOGY INC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/6/2016</td>\n",
       "      <td>TCMD</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/tactile-systems-technology-inc-111659-80227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>Priced (2/4/2000)</td>\n",
       "      <td>$20.00</td>\n",
       "      <td>8/2/2000</td>\n",
       "      <td>THERMA WAVE INC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/29/2000</td>\n",
       "      <td>TWAV</td>\n",
       "      <td>www.nasdaq.com/markets/ipos/company/therma-wave-inc-41664-2724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FPXI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GIGU</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>github</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IAMU</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ILTP</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LGCU</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LHCU</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MPMH</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PMHC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PQH</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRCI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PSND</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PSWW</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QSP</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SDTI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SFY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPCI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPTF</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SRTS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SRTS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>StationCasinos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SwiftEnergy</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TF</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TNKS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRAM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vice</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VITA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VMET</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>637 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Exchange               IPODate  \\\n",
       "0    NASDAQ                Priced (5/6/2016)      \n",
       "1    NASDAQ                Priced (4/7/2016)      \n",
       "2    NYSE                  Priced (4/21/2016)     \n",
       "3    NYSE                  Priced (10/6/2016)     \n",
       "4    NYSE                  Priced (3/17/2017)     \n",
       "5    NASDAQ                Priced (7/20/2016)     \n",
       "6    NYSE                  Priced (10/12/2016)    \n",
       "7    NASDAQ                Priced (3/23/2016)     \n",
       "8    NASDAQ                Priced (2/3/2016)      \n",
       "9    NYSE                  Filed (9/24/2014)      \n",
       "10   NYSE                  Priced (4/27/2017)     \n",
       "11   NYSE                  Priced (5/19/2016)     \n",
       "12   NASDAQ                Priced (7/7/2015)      \n",
       "13   NASDAQ Global Select  Priced (2/1/2017)      \n",
       "14   NASDAQ                Priced (7/23/2015)     \n",
       "15   NASDAQ Global Select  Priced (10/14/2016)    \n",
       "16   NASDAQ                Priced (2/22/2002)     \n",
       "17   NYSE                  Filed (4/9/2014)       \n",
       "18   NASDAQ                Priced (4/21/2004)     \n",
       "19   NYSE                  Withdrawn (8/8/2018)   \n",
       "20   NASDAQ                Priced (5/18/2016)     \n",
       "21   NASDAQ                Filed (11/17/2015)     \n",
       "22   NASDAQ                Priced (5/26/2016)     \n",
       "23   NASDAQ                Priced (4/27/2016)     \n",
       "24   NASDAQ Global         Priced (10/5/2017)     \n",
       "25   NASDAQ                Priced (5/6/2016)      \n",
       "26   NASDAQ                Priced (3/3/2016)      \n",
       "27   NASDAQ Global         Priced (9/29/2016)     \n",
       "28   NASDAQ                Priced (7/28/2016)     \n",
       "29   NASDAQ                Priced (2/4/2000)      \n",
       "..      ...                              ...      \n",
       "86   NaN                   NaN                    \n",
       "87   NaN                   NaN                    \n",
       "88   NaN                   NaN                    \n",
       "89   NaN                   NaN                    \n",
       "90   NaN                   NaN                    \n",
       "91   NaN                   NaN                    \n",
       "92   NaN                   NaN                    \n",
       "93   NaN                   NaN                    \n",
       "94   NaN                   NaN                    \n",
       "95   NaN                   NaN                    \n",
       "96   NaN                   NaN                    \n",
       "97   NaN                   NaN                    \n",
       "98   NaN                   NaN                    \n",
       "99   NaN                   NaN                    \n",
       "100  NaN                   NaN                    \n",
       "101  NaN                   NaN                    \n",
       "102  NaN                   NaN                    \n",
       "103  NaN                   NaN                    \n",
       "104  NaN                   NaN                    \n",
       "105  NaN                   NaN                    \n",
       "106  NaN                   NaN                    \n",
       "107  NaN                   NaN                    \n",
       "108  NaN                   NaN                    \n",
       "109  NaN                   NaN                    \n",
       "110  NaN                   NaN                    \n",
       "111  NaN                   NaN                    \n",
       "112  NaN                   NaN                    \n",
       "113  NaN                   NaN                    \n",
       "114  NaN                   NaN                    \n",
       "115  NaN                   NaN                    \n",
       "\n",
       "                             IPOPrice                  LockupExpiration  \\\n",
       "0    $12.00                            11/2/2016                          \n",
       "1    $10.00                            10/4/2016                          \n",
       "2    $22.00                            10/18/2016                         \n",
       "3    $18.00                            4/4/2017                           \n",
       "4    $14.00                            9/13/2017                          \n",
       "5    $15.00                            1/16/2017                          \n",
       "6    $18.00                            4/10/2017                          \n",
       "7    $15.00                            9/19/2016                          \n",
       "8    $16.00                            8/1/2016                           \n",
       "9    <span class=\"nodata\"> -- </span>  <span class=\"nodata\"> -- </span>   \n",
       "10   $21.00                            10/24/2017                         \n",
       "11   $11.00                            11/15/2016                         \n",
       "12   $7.00                             1/4/2016                           \n",
       "13   $14.00                            7/31/2017                          \n",
       "14   $17.00                            1/19/2016                          \n",
       "15   $15.00                            4/12/2017                          \n",
       "16   $19.00                            8/21/2002                          \n",
       "17   <span class=\"nodata\"> -- </span>  <span class=\"nodata\"> -- </span>   \n",
       "18   $10.50                            10/18/2004                         \n",
       "19   12.00-13.00                       <span class=\"nodata\"> -- </span>   \n",
       "20   $4.00                             11/14/2016                         \n",
       "21   $10.00                            <span class=\"nodata\"> -- </span>   \n",
       "22   $11.00                            11/22/2016                         \n",
       "23   $19.50                            10/24/2016                         \n",
       "24   $17.00                            4/3/2018                           \n",
       "25   $12.00                            11/2/2016                          \n",
       "26   $12.00                            8/30/2016                          \n",
       "27   $12.00                            3/28/2017                          \n",
       "28   $10.00                            1/24/2017                          \n",
       "29   $20.00                            8/2/2000                           \n",
       "..      ...                                 ...                           \n",
       "86   NaN                               NaN                                \n",
       "87   NaN                               NaN                                \n",
       "88   NaN                               NaN                                \n",
       "89   NaN                               NaN                                \n",
       "90   NaN                               NaN                                \n",
       "91   NaN                               NaN                                \n",
       "92   NaN                               NaN                                \n",
       "93   NaN                               NaN                                \n",
       "94   NaN                               NaN                                \n",
       "95   NaN                               NaN                                \n",
       "96   NaN                               NaN                                \n",
       "97   NaN                               NaN                                \n",
       "98   NaN                               NaN                                \n",
       "99   NaN                               NaN                                \n",
       "100  NaN                               NaN                                \n",
       "101  NaN                               NaN                                \n",
       "102  NaN                               NaN                                \n",
       "103  NaN                               NaN                                \n",
       "104  NaN                               NaN                                \n",
       "105  NaN                               NaN                                \n",
       "106  NaN                               NaN                                \n",
       "107  NaN                               NaN                                \n",
       "108  NaN                               NaN                                \n",
       "109  NaN                               NaN                                \n",
       "110  NaN                               NaN                                \n",
       "111  NaN                               NaN                                \n",
       "112  NaN                               NaN                                \n",
       "113  NaN                               NaN                                \n",
       "114  NaN                               NaN                                \n",
       "115  NaN                               NaN                                \n",
       "\n",
       "                                         Name Name   \\\n",
       "0    SPRING BANK PHARMACEUTICALS, INC.         NaN    \n",
       "1    AEGLEA BIOTHERAPEUTICS, INC.              NaN    \n",
       "2    AMERICAN RENAL ASSOCIATES HOLDINGS, INC.  NaN    \n",
       "3    AQUAVENTURE HOLDINGS LTD                  NaN    \n",
       "4    PROPETRO HOLDING CORP.                    NaN    \n",
       "5    AUDENTES THERAPEUTICS, INC.               NaN    \n",
       "6    AZURE POWER GLOBAL LTD                    NaN    \n",
       "7    CORVUS PHARMACEUTICALS, INC.              NaN    \n",
       "8    EDITAS MEDICINE, INC.                     NaN    \n",
       "9    EXMAR ENERGY PARTNERS LP                  NaN    \n",
       "10   FLOOR &amp;amp; DECOR HOLDINGS, INC.      NaN    \n",
       "11   GRUPO SUPERVIELLE S.A.                    NaN    \n",
       "12   HAILIANG EDUCATION GROUP INC.             NaN    \n",
       "13   LAUREATE EDUCATION, INC.                  NaN    \n",
       "14   LIVE OAK BANCSHARES, INC.                 NaN    \n",
       "15   MAMMOTH ENERGY SERVICES, INC.             NaN    \n",
       "16   PETCO ANIMAL SUPPLIES INC                 NaN    \n",
       "17   PRINCIPAL MARITIME TANKERS CORP           NaN    \n",
       "18   PROCENTURY CORP                           NaN    \n",
       "19   PSAV, INC.                                NaN    \n",
       "20   PULSE BIOSCIENCES, INC.                   NaN    \n",
       "21   PULTE ACQUISITION CORP.                   NaN    \n",
       "22   REATA PHARMACEUTICALS INC                 NaN    \n",
       "23   RED ROCK RESORTS, INC.                    NaN    \n",
       "24   RHYTHM PHARMACEUTICALS, INC.              NaN    \n",
       "25   SPRING BANK PHARMACEUTICALS, INC.         NaN    \n",
       "26   SYNDAX PHARMACEUTICALS INC                NaN    \n",
       "27   TABULA RASA HEALTHCARE, INC.              NaN    \n",
       "28   TACTILE SYSTEMS TECHNOLOGY INC            NaN    \n",
       "29   THERMA WAVE INC                           NaN    \n",
       "..               ...                           ...    \n",
       "86   NaN                                       NaN    \n",
       "87   NaN                                       NaN    \n",
       "88   NaN                                       NaN    \n",
       "89   NaN                                       NaN    \n",
       "90   NaN                                       NaN    \n",
       "91   NaN                                       NaN    \n",
       "92   NaN                                       NaN    \n",
       "93   NaN                                       NaN    \n",
       "94   NaN                                       NaN    \n",
       "95   NaN                                       NaN    \n",
       "96   NaN                                       NaN    \n",
       "97   NaN                                       NaN    \n",
       "98   NaN                                       NaN    \n",
       "99   NaN                                       NaN    \n",
       "100  NaN                                       NaN    \n",
       "101  NaN                                       NaN    \n",
       "102  NaN                                       NaN    \n",
       "103  NaN                                       NaN    \n",
       "104  NaN                                       NaN    \n",
       "105  NaN                                       NaN    \n",
       "106  NaN                                       NaN    \n",
       "107  NaN                                       NaN    \n",
       "108  NaN                                       NaN    \n",
       "109  NaN                                       NaN    \n",
       "110  NaN                                       NaN    \n",
       "111  NaN                                       NaN    \n",
       "112  NaN                                       NaN    \n",
       "113  NaN                                       NaN    \n",
       "114  NaN                                       NaN    \n",
       "115  NaN                                       NaN    \n",
       "\n",
       "                QuietPeriodExpiration          Ticker  \\\n",
       "0    6/15/2016                         SBPH             \n",
       "1    5/17/2016                         AGLE             \n",
       "2    5/31/2016                         ARA              \n",
       "3    11/15/2016                        WAAS             \n",
       "4    4/26/2017                         PUMP             \n",
       "5    8/29/2016                         BOLD             \n",
       "6    11/21/2016                        AZRE             \n",
       "7    5/2/2016                          CRVS             \n",
       "8    3/14/2016                         EDIT             \n",
       "9    <span class=\"nodata\"> -- </span>  XMLP             \n",
       "10   6/6/2017                          FND              \n",
       "11   6/28/2016                         SUPV             \n",
       "12   8/17/2015                         HLG              \n",
       "13   3/13/2017                         LAUR             \n",
       "14   9/1/2015                          LOB              \n",
       "15   11/23/2016                        TUSK             \n",
       "16   3/19/2002                         PETC             \n",
       "17   <span class=\"nodata\"> -- </span>  PMAR             \n",
       "18   5/17/2004                         PROS             \n",
       "19   <span class=\"nodata\"> -- </span>  PSAV             \n",
       "20   6/27/2016                         PLSE             \n",
       "21   <span class=\"nodata\"> -- </span>  PLTEU            \n",
       "22   7/5/2016                          RETA             \n",
       "23   6/6/2016                          RRR              \n",
       "24   11/14/2017                        RYTM             \n",
       "25   6/15/2016                         SBPH             \n",
       "26   4/12/2016                         SNDX             \n",
       "27   11/8/2016                         TRHC             \n",
       "28   9/6/2016                          TCMD             \n",
       "29   2/29/2000                         TWAV             \n",
       "..         ...                          ...             \n",
       "86   NaN                               FPXI             \n",
       "87   NaN                               GIGU             \n",
       "88   NaN                               github           \n",
       "89   NaN                               IAMU             \n",
       "90   NaN                               ILTP             \n",
       "91   NaN                               LGCU             \n",
       "92   NaN                               LHCU             \n",
       "93   NaN                               MPMH             \n",
       "94   NaN                               PGN              \n",
       "95   NaN                               PMHC             \n",
       "96   NaN                               PQH              \n",
       "97   NaN                               PRCI             \n",
       "98   NaN                               PSND             \n",
       "99   NaN                               PSWW             \n",
       "100  NaN                               QSP              \n",
       "101  NaN                               SDTI             \n",
       "102  NaN                               SFY              \n",
       "103  NaN                               SPCI             \n",
       "104  NaN                               SPTF             \n",
       "105  NaN                               SRTS             \n",
       "106  NaN                               SRTS             \n",
       "107  NaN                               StationCasinos   \n",
       "108  NaN                               SwiftEnergy      \n",
       "109  NaN                               TE               \n",
       "110  NaN                               TF               \n",
       "111  NaN                               TNKS             \n",
       "112  NaN                               TRAM             \n",
       "113  NaN                               vice             \n",
       "114  NaN                               VITA             \n",
       "115  NaN                               VMET             \n",
       "\n",
       "                                                                                         URL  \n",
       "0    www.nasdaq.com/markets/ipos/company/spring-bank-pharmaceuticals-inc-898133-80118         \n",
       "1    www.nasdaq.com/markets/ipos/company/aeglea-biotherapeutics-inc-961076-78705              \n",
       "2    www.nasdaq.com/markets/ipos/company/american-renal-associates-holdings-inc-835298-79295  \n",
       "3    www.nasdaq.com/markets/ipos/company/aquaventure-holdings-ltd-766355-79466                \n",
       "4    www.nasdaq.com/markets/ipos/company/propetro-holding-corp-999784-82763                   \n",
       "5    www.nasdaq.com/markets/ipos/company/audentes-therapeutics-inc-953394-80103               \n",
       "6    www.nasdaq.com/markets/ipos/company/azure-power-global-ltd-981770-80005                  \n",
       "7    www.nasdaq.com/markets/ipos/company/corvus-pharmaceuticals-inc-951574-80105              \n",
       "8    www.nasdaq.com/markets/ipos/company/editas-medicine-inc-973356-80107                     \n",
       "9    www.nasdaq.com/markets/ipos/company/exmar-energy-partners-lp-945504-76551                \n",
       "10   www.nasdaq.com/markets/ipos/company/floor-decor-holdings-inc-842788-82785                \n",
       "11   www.nasdaq.com/markets/ipos/company/grupo-supervielle-sa-983312-80119                    \n",
       "12   www.nasdaq.com/markets/ipos/company/hailiang-education-group-inc-953462-77324            \n",
       "13   www.nasdaq.com/markets/ipos/company/laureate-education-inc-3789-79506                    \n",
       "14   www.nasdaq.com/markets/ipos/company/live-oak-bancshares-inc-803316-78750                 \n",
       "15   www.nasdaq.com/markets/ipos/company/mammoth-energy-services-inc-1002759-81637            \n",
       "16   www.nasdaq.com/markets/ipos/company/petco-animal-supplies-inc-9730-16041                 \n",
       "17   www.nasdaq.com/markets/ipos/company/principal-maritime-tankers-corp-931869-75119         \n",
       "18   www.nasdaq.com/markets/ipos/company/procentury-corp-610917-36920                         \n",
       "19   www.nasdaq.com/markets/ipos/company/psav-inc-974802-79354?tab=news                       \n",
       "20   www.nasdaq.com/markets/ipos/company/pulse-biosciences-inc-949419-80045                   \n",
       "21   www.nasdaq.com/markets/ipos/company/pulte-acquisition-corp-979789-79840                  \n",
       "22   www.nasdaq.com/markets/ipos/company/reata-pharmaceuticals-inc-704859-80104               \n",
       "23   www.nasdaq.com/markets/ipos/company/red-rock-resorts-inc-977107-79572                    \n",
       "24   www.nasdaq.com/markets/ipos/company/rhythm-pharmaceuticals-inc-972920-84659              \n",
       "25   www.nasdaq.com/markets/ipos/company/spring-bank-pharmaceuticals-inc-898133-80118         \n",
       "26   www.nasdaq.com/markets/ipos/company/syndax-pharmaceuticals-inc-740721-80110              \n",
       "27   www.nasdaq.com/markets/ipos/company/tabula-rasa-healthcare-inc-983094-80108              \n",
       "28   www.nasdaq.com/markets/ipos/company/tactile-systems-technology-inc-111659-80227          \n",
       "29   www.nasdaq.com/markets/ipos/company/therma-wave-inc-41664-2724                           \n",
       "..                                                              ...                           \n",
       "86   NaN                                                                                      \n",
       "87   NaN                                                                                      \n",
       "88   NaN                                                                                      \n",
       "89   NaN                                                                                      \n",
       "90   NaN                                                                                      \n",
       "91   NaN                                                                                      \n",
       "92   NaN                                                                                      \n",
       "93   NaN                                                                                      \n",
       "94   NaN                                                                                      \n",
       "95   NaN                                                                                      \n",
       "96   NaN                                                                                      \n",
       "97   NaN                                                                                      \n",
       "98   NaN                                                                                      \n",
       "99   NaN                                                                                      \n",
       "100  NaN                                                                                      \n",
       "101  NaN                                                                                      \n",
       "102  NaN                                                                                      \n",
       "103  NaN                                                                                      \n",
       "104  NaN                                                                                      \n",
       "105  NaN                                                                                      \n",
       "106  NaN                                                                                      \n",
       "107  NaN                                                                                      \n",
       "108  NaN                                                                                      \n",
       "109  NaN                                                                                      \n",
       "110  NaN                                                                                      \n",
       "111  NaN                                                                                      \n",
       "112  NaN                                                                                      \n",
       "113  NaN                                                                                      \n",
       "114  NaN                                                                                      \n",
       "115  NaN                                                                                      \n",
       "\n",
       "[637 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = tickers[tickers['LockupExpiration'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list=tickers[\"Ticker\"].to_list()# Subsetting tickers variable and converting into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGLE\n"
     ]
    }
   ],
   "source": [
    "print(ticker_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = set(ticker_list) # removing duplicates from the tickers list£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list=list(filter(lambda x : str(x) != 'nan' , list(ticker_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGEX to extract tickers or presence of IPO\n",
    "ticker_expression_no_numbers = '(\\s\\$[A-Za-z]{1,6})+'\n",
    "ticker_expression_including_numbers = '\\$[A-Za-z0-9]{1,6}'\n",
    "ipo_expression = '#ipo|ipo|#IPO|IPO'\n",
    "\n",
    "df_tickers = (df.withColumn(\"tickers\", regexp_extract('text', ticker_expression_no_numbers, 0))\n",
    "                .withColumn(\"contains_ipo\", regexp_extract('text', ipo_expression, 0))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1610431 tweets in the total set that containt the word ipo or a pattern that matches a ticker, including the dollar sign\n"
     ]
    }
   ],
   "source": [
    "# Filter full dataset for tweets that contain either a ticker-pattern or the IPO pattern\n",
    "df_tickers_and_ipo = df_tickers.filter((df_tickers[\"tickers\"] != \"\") | (df_tickers[\"contains_ipo\"] != \"\"))\n",
    "\n",
    "print(\"There are {} tweets in the total set that containt the word ipo or a pattern that matches a ticker, including the dollar sign\".format(df_tickers.count()))\n",
    "\n",
    "#df_tickers_and_ipo.select(\"tweet_id_str\", \"contains_ipo\", \"tickers\").show(5)\n",
    "#df.createOrReplaceTempView(\"dfSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the tweets that contain one or multiple tickers\n",
    "df_tickers = (df_tickers_and_ipo.filter(df_tickers_and_ipo[\"tickers\"] != \"\"))\n",
    "\n",
    "\n",
    "df_tickers = (df_tickers.select('contributors','coordinates','display_text_range','entities','extended_entities','extended_tweet','favorite_count','favorited','filter_level','geo','id','tweet_id_str','in_reply_to_screen_name','in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'possibly_sensitive', 'quote_count', 'quoted_status', 'quoted_status_id', 'quoted_status_id_str', 'reply_count', 'retweet_count', 'retweeted', 'retweeted_status', 'source', 'text', 'timestamp_ms', 'truncated', 'user', 'withheld_in_countries', 'timestamp', 'text_lower','cashtags','contains_ipo','contains_cashtag','contains_cashtag_with_spaces','contains_company_name','retweet_pattern','contains_RT_pattern','user_id_str', split(df_tickers.tickers, '\\s\\$').alias('tickers')))\n",
    "\n",
    "\n",
    "df_tickers = df_tickers.withColumn(\"ticker\", explode(df_tickers.tickers))\n",
    "df_tickers = df_tickers.filter(df_tickers.ticker != \"\")\n",
    "\n",
    "aggregated_tickers = (df_tickers.withColumn(\"ticker\", upper(df_tickers.ticker))\n",
    "                                .groupBy(\"ticker\").count()\n",
    "                                .withColumnRenamed('count', \"ticker_occurence\")\n",
    "                     )\n",
    "aggregated_tickers = aggregated_tickers.sort(aggregated_tickers.ticker_occurence.desc())\n",
    "\n",
    "#aggregated_tickers.coalesce(1).write.csv(\"ticker_occurence_2016\", mode = \"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ticker='BTC', ticker_occurence=2380),\n",
       " Row(ticker='SPY', ticker_occurence=1757),\n",
       " Row(ticker='ETH', ticker_occurence=1342),\n",
       " Row(ticker='AAPL', ticker_occurence=1293),\n",
       " Row(ticker='TSLA', ticker_occurence=1015),\n",
       " Row(ticker='AMZN', ticker_occurence=809),\n",
       " Row(ticker='FB', ticker_occurence=732),\n",
       " Row(ticker='AAGC', ticker_occurence=689),\n",
       " Row(ticker='PVTL', ticker_occurence=638),\n",
       " Row(ticker='NFLX', ticker_occurence=603)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_tickers.rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tickers=df_tickers.select('*').filter('lang == \"en\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df_tickers.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mma\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "TRTX\n",
      "5\n",
      "TRMT\n",
      "0\n",
      "TBRGU\n",
      "0\n",
      "TC\n",
      "0\n",
      "RTLR\n",
      "2\n",
      "CHRA\n",
      "7\n",
      "RYB\n",
      "0\n",
      "VAPO\n",
      "13\n",
      "LX\n",
      "7\n",
      "OVID\n",
      "12\n",
      "SPRO\n",
      "1\n",
      "THOR\n",
      "0\n",
      "ACAMU\n",
      "8\n",
      "URGN\n",
      "4\n",
      "XERS\n",
      "5\n",
      "CNST\n",
      "0\n",
      "ETTX\n",
      "47\n",
      "SE\n",
      "0\n",
      "PSTX\n",
      "4\n",
      "NCSM\n",
      "12\n",
      "CASA\n",
      "0\n",
      "MGP\n",
      "13\n",
      "BCML\n",
      "1\n",
      "JG\n",
      "9\n",
      "GDI\n",
      "1\n",
      "DAVA\n",
      "0\n",
      "HCCHU\n",
      "7\n",
      "OPTN\n",
      "12\n",
      "SECO\n",
      "0\n",
      "LEVB\n",
      "10\n",
      "QES\n",
      "2\n",
      "FNKO\n",
      "29\n",
      "ARMO\n",
      "8\n",
      "IFRX\n",
      "0\n",
      "PLAN\n",
      "4\n",
      "BJS\n",
      "16\n",
      "SEND\n",
      "3\n",
      "CBTX\n",
      "1\n",
      "MESA\n",
      "0\n",
      "SXTC\n",
      "3\n",
      "DOGZ\n",
      "47\n",
      "SMAR\n",
      "0\n",
      "ZOOX\n",
      "4\n",
      "CPUL\n",
      "0\n",
      "PCIM\n",
      "17\n",
      "EVLO\n",
      "9\n",
      "ZLAB\n",
      "30\n",
      "WOW\n",
      "10\n",
      "KALA\n",
      "6\n",
      "DESP\n",
      "23\n",
      "SFIX\n",
      "0\n",
      "ENVN\n",
      "69\n",
      "VKTX\n",
      "6\n",
      "AQUA\n",
      "5\n",
      "CRNX\n",
      "10\n",
      "EOLS\n",
      "0\n",
      "MSC\n",
      "1\n",
      "PLSE\n",
      "0\n",
      "EQ\n",
      "78\n",
      "FND\n",
      "3\n",
      "VRCA\n",
      "5\n",
      "RRR\n",
      "6\n",
      "VCTR\n",
      "0\n",
      "VPRL\n",
      "16\n",
      "EDIT\n",
      "8\n",
      "BPMP\n",
      "14\n",
      "STXB\n",
      "1\n",
      "GMR\n",
      "15\n",
      "ADIL\n",
      "6\n",
      "MGTA\n",
      "0\n",
      "TWAV\n",
      "0\n",
      "FTCH\n",
      "37\n",
      "ZS\n",
      "9\n",
      "APTX\n",
      "0\n",
      "HZ\n",
      "1\n",
      "FOCS\n",
      "0\n",
      "CTK\n",
      "3\n",
      "ESQ\n",
      "5\n",
      "AMGP\n",
      "0\n",
      "YZCM\n",
      "0\n",
      "MQAI\n",
      "1\n",
      "BRPAU\n",
      "25\n",
      "LEVL\n",
      "0\n",
      "XMLP\n",
      "2\n",
      "KRYS\n",
      "0\n",
      "WALL\n",
      "1\n",
      "AQST\n",
      "2\n",
      "CNACU\n",
      "0\n",
      "YMAB\n",
      "0\n",
      "NIU\n",
      "2\n",
      "SOI\n",
      "8\n",
      "YOGA\n",
      "1\n",
      "RUBY\n",
      "0\n",
      "VIOT\n",
      "66\n",
      "DBX\n",
      "2\n",
      "OSPRU\n",
      "0\n",
      "TPGHU\n",
      "33\n",
      "SOGO\n",
      "10\n",
      "RYTM\n",
      "4\n",
      "EAGLU\n",
      "0\n",
      "AFGL\n",
      "3\n",
      "CSSE\n",
      "0\n",
      "ESTA\n",
      "0\n",
      "ESTC\n",
      "0\n",
      "TAI\n",
      "2\n",
      "POLY\n",
      "17\n",
      "ATUS\n",
      "10\n",
      "MGTX\n",
      "4\n",
      "ARA\n",
      "16\n",
      "TNTR\n",
      "8\n",
      "GTHX\n",
      "6\n",
      "HAIR\n",
      "0\n",
      "PUYI\n",
      "1\n",
      "OPRA\n",
      "0\n",
      "TKKSU\n",
      "6\n",
      "BDFC\n",
      "2\n",
      "GWRS\n",
      "16\n",
      "SCWX\n",
      "6\n",
      "AVRO\n",
      "4\n",
      "CADE\n",
      "0\n",
      "USOD\n",
      "0\n",
      "FPACU\n",
      "0\n",
      "PSIT\n",
      "6\n",
      "TOCA\n",
      "10\n",
      "BXG\n",
      "38\n",
      "UBX\n",
      "6\n",
      "ALRN\n",
      "0\n",
      "SIBN\n",
      "0\n",
      "HOTH\n",
      "0\n",
      "CTRA\n",
      "6\n",
      "SBT\n",
      "10\n",
      "EVOP\n",
      "0\n",
      "BWAY\n",
      "29\n",
      "BHVN\n",
      "0\n",
      "LTN'U\n",
      "2\n",
      "CAAP\n",
      "12\n",
      "BOLD\n",
      "3\n",
      "UXIN\n",
      "0\n",
      "MDJH\n",
      "1\n",
      "PSAV\n",
      "50\n",
      "DOCU\n",
      "3\n",
      "SNNA\n",
      "0\n",
      "CCHU\n",
      "1\n",
      "AUTL\n",
      "0\n",
      "BRY\n",
      "0\n",
      "DDOC\n",
      "16\n",
      "SI\n",
      "2\n",
      "CMSSU\n",
      "0\n",
      "CHACU\n",
      "0\n",
      "LTHM\n",
      "6\n",
      "SAFE\n",
      "30\n",
      "VRIO\n",
      "0\n",
      "PROS\n",
      "0\n",
      "EB\n",
      "0\n",
      "YI\n",
      "0\n",
      "ASV\n",
      "2\n",
      "CCB\n",
      "0\n",
      "RMG'U\n",
      "41\n",
      "AGMH\n",
      "0\n",
      "IPOAU\n",
      "3\n",
      "BIOX\n",
      "0\n",
      "WAAS\n",
      "13\n",
      "ONE\n",
      "2\n",
      "PDD\n",
      "0\n",
      "UTC\n",
      "12\n",
      "TMCXU\n",
      "13\n",
      "BWB\n",
      "11\n",
      "RDFN\n",
      "0\n",
      "CBUS\n",
      "0\n",
      "TDACU\n",
      "16\n",
      "AMRH\n",
      "14\n",
      "ILPT\n",
      "1\n",
      "PRVB\n",
      "8\n",
      "FPH\n",
      "42\n",
      "APRN\n",
      "0\n",
      "KLDO\n",
      "7\n",
      "TBIO\n",
      "53\n",
      "CDAY\n",
      "0\n",
      "MFACU\n",
      "0\n",
      "XYF\n",
      "11\n",
      "BY\n",
      "9\n",
      "HHR\n",
      "11\n",
      "HYRE\n",
      "23\n",
      "NINE\n",
      "0\n",
      "ARYAU\n",
      "15\n",
      "LOVE\n",
      "0\n",
      "OSMT\n",
      "0\n",
      "ATXI\n",
      "10\n",
      "AMBO\n",
      "6\n",
      "ENTX\n",
      "46\n",
      "CODX\n",
      "10\n",
      "CURO\n",
      "11\n",
      "HMI\n",
      "0\n",
      "SAMAU\n",
      "4\n",
      "BFRA\n",
      "0\n",
      "LEGH\n",
      "9\n",
      "PLYM\n",
      "2\n",
      "TCL\n",
      "16\n",
      "INSP\n",
      "0\n",
      "FTACU\n",
      "12\n",
      "QTRX\n",
      "0\n",
      "TBLT\n",
      "0\n",
      "ELAN\n",
      "2\n",
      "RNGR\n",
      "39\n",
      "TLRY\n",
      "0\n",
      "PT\n",
      "42\n",
      "HUYA\n",
      "0\n",
      "HHHHU\n",
      "5\n",
      "NCNA\n",
      "3\n",
      "MSBI\n",
      "107\n",
      "SURF\n",
      "12\n",
      "FSCT\n",
      "4\n",
      "BJ\n",
      "8\n",
      "ROAD\n",
      "0\n",
      "MNCLU\n",
      "0\n",
      "AZRE\n",
      "0\n",
      "VTIQU \n",
      "11\n",
      "FIXX\n",
      "2\n",
      "BE\n",
      "26\n",
      "MULE\n",
      "0\n",
      "SMXT\n",
      "16\n",
      "ASNS\n",
      "3\n",
      "WGRD\n",
      "21\n",
      "ASLN\n",
      "4\n",
      "VRI\n",
      "7\n",
      "RBB\n",
      "0\n",
      "SVMK\n",
      "0\n",
      "DTSS\n",
      "0\n",
      "ARDT\n",
      "10\n",
      "MDB\n",
      "0\n",
      "TME\n",
      "48\n",
      "PAGS\n",
      "16\n",
      "MNLO\n",
      "6\n",
      "MBIN\n",
      "7\n",
      "EQH\n",
      "7\n",
      "BEDU\n",
      "0\n",
      "AAAU\n",
      "9\n",
      "HUD\n",
      "10\n",
      "WHD\n",
      "11\n",
      "APPN\n",
      "0\n",
      "GLDM\n",
      "27\n",
      "CVNA\n",
      "0\n",
      "NESRU\n",
      "11\n",
      "NEXA\n",
      "5\n",
      "PQG\n",
      "9\n",
      "EIDX\n",
      "15\n",
      "AQ\n",
      "0\n",
      "REPX\n",
      "3\n",
      "XSPL\n",
      "0\n",
      "ARDS\n",
      "29\n",
      "ADOM\n",
      "0\n",
      "ALEC\n",
      "0\n",
      "PBTS\n",
      "1\n",
      "CANG\n",
      "3\n",
      "IIIV\n",
      "0\n",
      "MIIIU\n",
      "5\n",
      "KREF\n",
      "10\n",
      "BV\n",
      "0\n",
      "FSACU\n",
      "0\n",
      "CSCA\n",
      "0\n",
      "QFIN\n",
      "0\n",
      "ATIF\n",
      "0\n",
      "GMHIU\n",
      "5\n",
      "TRHC\n",
      "0\n",
      "SOLY\n",
      "0\n",
      "EDTXU\n",
      "0\n",
      "BRACU\n",
      "0\n",
      "VTEC\n",
      "5\n",
      "NTLA\n",
      "0\n",
      "AMCIU\n",
      "0\n",
      "GBSG\n",
      "7\n",
      "TORC\n",
      "9\n",
      "KIDS\n",
      "4\n",
      "NREF\n",
      "111\n",
      "EAF\n",
      "0\n",
      "TPGEU\n",
      "5\n",
      "VNCR\n",
      "0\n",
      "KOD\n",
      "0\n",
      "NDRAU\n",
      "4\n",
      "CELC\n",
      "12\n",
      "SAIL\n",
      "0\n",
      "STRO\n",
      "13\n",
      "PRT\n",
      "0\n",
      "BYND\n",
      "2\n",
      "DOTAU\n",
      "4\n",
      "ADV\n",
      "0\n",
      "MRMR\n",
      "3\n",
      "NTGN\n",
      "6\n",
      "ERYP\n",
      "0\n",
      "GH\n",
      "522\n",
      "PVTL\n",
      "18\n",
      "DNLI\n",
      "46\n",
      "LASR\n",
      "0\n",
      "MDRR\n",
      "12\n",
      "SLGL\n",
      "11\n",
      "FTSI\n",
      "8\n",
      "ATNX\n",
      "2\n",
      "STNLU\n",
      "1\n",
      "PCB\n",
      "6\n",
      "MTECU\n",
      "10\n",
      "EYE\n",
      "0\n",
      "BNGOU\n",
      "5\n",
      "SQFT\n",
      "0\n",
      "LOACU\n",
      "3\n",
      "LBM\n",
      "0\n",
      "GRTS\n",
      "0\n",
      "UMRX \n",
      "0\n",
      "LQDA\n",
      "0\n",
      "RVLV\n",
      "0\n",
      "HKIT\n",
      "0\n",
      "ARVN\n",
      "11\n",
      "FAT\n",
      "15\n",
      "COLD\n",
      "0\n",
      "RWGEU\n",
      "0\n",
      "anaplan\n",
      "6\n",
      "CMTA\n",
      "7\n",
      "BAND\n",
      "1\n",
      "RLTY\n",
      "31\n",
      "LBC\n",
      "3\n",
      "CCR\n",
      "0\n",
      "PHAS\n",
      "5\n",
      "TCMD\n",
      "5\n",
      "KNSA\n",
      "0\n",
      "GCAN\n",
      "0\n",
      "LOGC\n",
      "0\n",
      "Vizio\n",
      "7\n",
      "GPMT\n",
      "12\n",
      "DOVA\n",
      "0\n",
      "WISA\n",
      "4\n",
      "ALNA\n",
      "0\n",
      "APM\n",
      "5\n",
      "SSTI\n",
      "1\n",
      "TENB\n",
      "0\n",
      "HLG\n",
      "5\n",
      "NITE\n",
      "0\n",
      "BSVN\n",
      "12\n",
      "MOTS\n",
      "11\n",
      "SENS\n",
      "0\n",
      "GPAQU\n",
      "0\n",
      "SPAQU\n",
      "0\n",
      "ZLND\n",
      "82\n",
      "SPOT\n",
      "6\n",
      "HJLI\n",
      "3\n",
      "SBPH\n",
      "5\n",
      "FAMI\n",
      "0\n",
      "CWK\n",
      "4\n",
      "SONO\n",
      "11\n",
      "ACMR\n",
      "3\n",
      "SUPV\n",
      "16\n",
      "OBNK\n",
      "11\n",
      "PZRX\n",
      "2\n",
      "REPL\n",
      "1\n",
      "ETON\n",
      "3\n",
      "CDLX\n",
      "13\n",
      "NMRK\n",
      "0\n",
      "MUDSU\n",
      "34\n",
      "GSHD\n",
      "13\n",
      "RETA\n",
      "15\n",
      "ALTR\n",
      "0\n",
      "TOTAU\n",
      "13\n",
      "BTAI\n",
      "0\n",
      "NEWA\n",
      "3\n",
      "STIM\n",
      "0\n",
      "JFIN\n",
      "0\n",
      "ORTX\n",
      "0\n",
      "JMPW\n",
      "9\n",
      "PETQ\n",
      "0\n",
      "MOGU\n",
      "8\n",
      "LOMA\n",
      "9\n",
      "ICLK\n",
      "0\n",
      "MMDMU\n",
      "1\n",
      "WTTR\n",
      "101\n",
      "ZUO\n",
      "7\n",
      "AGLE\n",
      "0\n",
      "VZIO\n",
      "21\n",
      "QD\n",
      "3\n",
      "ZKIN\n",
      "11\n",
      "RCUS\n",
      "0\n",
      "WAFU\n",
      "5\n",
      "ODT\n",
      "11\n",
      "LAUR\n",
      "1\n",
      "VCNX\n",
      "0\n",
      "IMAC\n",
      "3\n",
      "PETZ\n",
      "3\n",
      "LUNG\n",
      "6\n",
      "CEPU\n",
      "0\n",
      "TUSK\n",
      "22\n",
      "CLDR\n",
      "2\n",
      "SSLJ\n",
      "0\n",
      "WEI\n",
      "6\n",
      "GTES\n",
      "0\n",
      "UROV\n",
      "2\n",
      "IMRN\n",
      "6\n",
      "AVI\n",
      "24\n",
      "AKCA\n",
      "0\n",
      "GOSS\n",
      "9\n",
      "MRSN\n",
      "0\n",
      "ALGRU\n",
      "0\n",
      "SGBX\n",
      "0\n",
      "CNTX\n",
      "0\n",
      "CTACU\n",
      "2\n",
      "VNTR\n",
      "12\n",
      "SNDX\n",
      "0\n",
      "PMAR\n",
      "3\n",
      "IPSC\n",
      "0\n",
      "USOU\n",
      "9\n",
      "VERI\n",
      "0\n",
      "INDUU\n",
      "5\n",
      "GNTY\n",
      "0\n",
      "ANCN\n",
      "8\n",
      "CUE\n",
      "9\n",
      "LBRT\n",
      "1\n",
      "CRVS\n",
      "60\n",
      "IQ\n",
      "131\n",
      "SNAP\n",
      "2\n",
      "ARLO\n",
      "4\n",
      "CURV\n",
      "100\n",
      "USX\n",
      "14\n",
      "AGS\n",
      "6\n",
      "XRF\n",
      "2\n",
      "BCACU\n",
      "83\n",
      "GNPX\n",
      "30\n",
      "CLPS\n",
      "12\n",
      "REDU\n",
      "2\n",
      "EEX\n",
      "3\n",
      "RETO\n",
      "0\n",
      "RRI\n",
      "0\n",
      "PHCF\n",
      "4\n",
      "NIO\n",
      "0\n",
      "PETC\n",
      "2\n",
      "OMP\n",
      "0\n",
      "FMCIU\n",
      "22\n",
      "ARGX\n",
      "0\n",
      "TLSA\n",
      "0\n",
      "MRNA\n",
      "21\n",
      "ABLX\n",
      "0\n",
      "SRTSU\n",
      "7\n",
      "CLXT\n",
      "0\n",
      "CSTX\n",
      "0\n",
      "PACQU \n",
      "2\n",
      "EPRT\n",
      "12\n",
      "BSTI\n",
      "0\n",
      "PLTEU\n",
      "0\n",
      "CLGN\n",
      "0\n",
      "TWST\n",
      "0\n",
      "RYTM \n",
      "0\n",
      "LFACU\n",
      "0\n",
      "SFTY\n",
      "3\n",
      "ZYME\n",
      "4\n",
      "SRRK\n",
      "6\n",
      "FTSV\n",
      "0\n",
      "XTF\n",
      "0\n",
      "IPIC\n",
      "0\n",
      "GRSHU\n",
      "0\n",
      "ALACU\n",
      "6\n",
      "EVER\n",
      "0\n",
      "TWLVU\n",
      "9\n",
      "CARG\n",
      "0\n",
      "VTUS\n",
      "0\n",
      "GRAFU\n",
      "0\n",
      "HVBH\n",
      "3\n",
      "LOB\n",
      "0\n",
      "NFE\n",
      "1\n",
      "GSAHU\n",
      "10\n",
      "PPDF\n",
      "0\n",
      "ALLO\n",
      "6\n",
      "KZR\n",
      "0\n",
      "BWMCU\n",
      "16\n",
      "OKTA\n",
      "18\n",
      "SWCH\n",
      "0\n",
      "LACQU\n",
      "0\n",
      "RMED\n",
      "14\n",
      "APLS\n",
      "7\n",
      "SCPH\n",
      "24\n",
      "TLC\n",
      "8\n",
      "GSKY\n",
      "9\n",
      "PUMP\n",
      "0\n",
      "DMAC\n",
      "2\n",
      "AVLR\n",
      "4\n",
      "BOMN\n",
      "41\n",
      "BILI\n",
      "83\n",
      "MOR\n",
      "14\n",
      "DCPH\n",
      "4\n",
      "ADIN\n",
      "6\n",
      "ECOR\n",
      "0\n",
      "PARK\n",
      "0\n",
      "UPWK\n",
      "0\n",
      "SLGG\n",
      "36\n",
      "CBLK\n",
      "26\n",
      "PS\n",
      "5\n",
      "NEW\n",
      "4\n",
      "OMADU\n",
      "0\n",
      "AXNX\n",
      "1\n",
      "ALLK\n",
      "5\n",
      "ITRM\n",
      "2\n",
      "TCDA\n",
      "0\n",
      "GTWY\n",
      "3\n",
      "HMP\n",
      "1\n",
      "STNE\n",
      "0\n",
      "PLIN\n",
      "4\n",
      "MCB\n",
      "1\n",
      "MYFW\n",
      "0\n",
      "ANDAU\n",
      "6\n",
      "DOMO\n",
      "0\n",
      "PLSE \n",
      "21\n",
      "ADT\n",
      "23\n",
      "SLDB\n",
      "13\n",
      "GEMP\n",
      "0\n",
      "DDMXU\n",
      "7\n",
      "VRNA\n",
      "4\n",
      "SWI\n",
      "4\n",
      "OSS\n",
      "2\n",
      "NEBUU\n",
      "0\n",
      "CLBR\n",
      "6\n",
      "EYEN\n",
      "2\n",
      "CIFS\n",
      "0\n",
      "TZACU\n",
      "0\n",
      "WRLSU\n",
      "0\n",
      "ARCE\n",
      "0\n",
      "QTT\n",
      "50\n",
      "ROKU\n"
     ]
    }
   ],
   "source": [
    "# loop to generate Individual JSON files per Ticker\n",
    "import json\n",
    "df_tickers.createOrReplaceTempView(\"tic\")\n",
    "for i in ticker_list:\n",
    "    #x = df_tickers.filter('ticker == {}'.format(i))\n",
    "    x = spark.sql(\"SELECT * from tic where ticker= \\\"{}\\\"\".format(i)) # Here it selects only required Ticker file as per requirement\n",
    "    print(x.count())\n",
    "    print(i)\n",
    "    try:\n",
    "        x = x.toPandas()\n",
    "        x.to_json(\"{}.json\".format(i))\n",
    "        #x.coalesce(1).write.format('json').save('tweets_{}.json'.format(i))\n",
    "        '''with open('tweets_{}.json'.format(i), 'w') as json_file:  \n",
    "            json.dump(x, json_file)'''\n",
    "    except:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m''$'\\033'\u001b[0m/                           FSCT.json          RBB.json\r\n",
      " AAAU.json                           FTACU.json         RCUS.json\r\n",
      " ABLX.json                           FTCH.json          RDFN.json\r\n",
      " ACAMU.json                          FTSI.json          REDU.json\r\n",
      " ACMR.json                           FTSV.json          REPL.json\r\n",
      " ADIL.json                           GBSG.json          REPX.json\r\n",
      " ADIN.json                           GCAN.json          RETA.json\r\n",
      " ADOM.json                           GDI.json           RETO.json\r\n",
      " ADT.json                            GEMP.json          RLTY.json\r\n",
      " ADV.json                            GH.json            RMED.json\r\n",
      " AFGL.json                           GLDM.json         \"RMG'U.json\"\r\n",
      " AGLE.json                           GMHIU.json         RNGR.json\r\n",
      " AGMH.json                           GMR.json           ROAD.json\r\n",
      " AGS.json                            GNPX.json          ROKU.json\r\n",
      " AKCA.json                           GNTY.json          RRI.json\r\n",
      " ALACU.json                          GOSS.json          RRR.json\r\n",
      " ALEC.json                           GPAQU.json         RTLR.json\r\n",
      " ALGRU.json                          GPMT.json          RUBY.json\r\n",
      " ALLK.json                           GRAFU.json         RVLV.json\r\n",
      " ALLO.json                           GRSHU.json         RWGEU.json\r\n",
      " ALNA.json                           GRTS.json          RYB.json\r\n",
      " ALRN.json                           GSAHU.json        'RYTM .json'\r\n",
      " ALTR.json                           GSHD.json          RYTM.json\r\n",
      " AMBO.json                           GSKY.json          SAFE.json\r\n",
      " AMCIU.json                          GTES.json          SAIL.json\r\n",
      " AMGP.json                           GTHX.json          SAMAU.json\r\n",
      " AMRH.json                           GTWY.json          SBPH.json\r\n",
      " Anaconda3-2019.03-Linux-x86_64.sh   GWRS.json          SBT.json\r\n",
      " anaplan.json                        HAIR.json          SCPH.json\r\n",
      " ANCN.json                           HCCHU.json         SCWX.json\r\n",
      " ANDAU.json                          HHHHU.json         SECO.json\r\n",
      " APLS.json                           HHR.json           SE.json\r\n",
      " APM.json                            HJLI.json          SEND.json\r\n",
      " APPN.json                           HKIT.json          SENS.json\r\n",
      " APRN.json                           HLG.json           sentiment.xlsx\r\n",
      " APTX.json                           HMI.json           SFIX.json\r\n",
      " AQ.json                             HMP.json           SFTY.json\r\n",
      " AQST.json                           HOTH.json          SGBX.json\r\n",
      " AQUA.json                           HUD.json           SIBN.json\r\n",
      " ARA.json                            HUYA.json          SI.json\r\n",
      " ARCE.json                           HVBH.json          SLDB.json\r\n",
      " ARDS.json                           HYRE.json          SLGG.json\r\n",
      " ARDT.json                           HZ.json            SLGL.json\r\n",
      " ARGX.json                           ICLK.json          SMAR.json\r\n",
      " ARLO.json                           IFRX.json          SMXT.json\r\n",
      " ARMO.json                           IIIV.json          \u001b[01;34msnap\u001b[0m/\r\n",
      " ARVN.json                           ILPT.json          SNAP.json\r\n",
      " ARYAU.json                          IMAC.json          SNDX.json\r\n",
      " ASLN.json                           IMRN.json          SNNA.json\r\n",
      " ASNS.json                           INDUU.json         SOGO.json\r\n",
      " ASV.json                            INSP.json          SOI.json\r\n",
      " ATIF.json                           IPIC.json          SOLY.json\r\n",
      " ATNX.json                           IPOAU.json         SONO.json\r\n",
      " ATUS.json                           \u001b[01;34mIPO_project\u001b[0m/       SPAQU.json\r\n",
      " ATXI.json                           IPSC.json          \u001b[01;34mspark-warehouse\u001b[0m/\r\n",
      " AUTL.json                           IQ.json            SPOT.json\r\n",
      " AVI.json                            ITRM.json          SPRO.json\r\n",
      " AVLR.json                           JFIN.json          SQFT.json\r\n",
      " AVRO.json                           JG.json            SRRK.json\r\n",
      " AXNX.json                           JMPW.json          SRTSU.json\r\n",
      " AZRE.json                           KALA.json          SSLJ.json\r\n",
      " BAND.json                           KIDS.json          SSTI.json\r\n",
      " BCACU.json                          KLDO.json          STIM.json\r\n",
      " BCML.json                           KNSA.json          STNE.json\r\n",
      " BDFC.json                           KOD.json           STNLU.json\r\n",
      " BEDU.json                           KREF.json          STRO.json\r\n",
      " BE.json                             KRYS.json          STXB.json\r\n",
      " BFRA.json                           KZR.json           SUPV.json\r\n",
      " BHVN.json                           LACQU.json         SURF.json\r\n",
      " BILI.json                           LASR.json          SVMK.json\r\n",
      " BIOX.json                           LAUR.json          SWCH.json\r\n",
      " BJ.json                             LBC.json           SWI.json\r\n",
      " BJS.json                            LBM.json           SXTC.json\r\n",
      " BNGOU.json                          LBRT.json          TAI.json\r\n",
      " BOLD.json                           LEGH.json          TBIO.json\r\n",
      " BOMN.json                           LEVB.json          TBLT.json\r\n",
      " BPMP.json                           LEVL.json          TBRGU.json\r\n",
      " BRACU.json                          LFACU.json         TCDA.json\r\n",
      " BRPAU.json                          LOACU.json         TC.json\r\n",
      " BRY.json                            LOB.json           TCL.json\r\n",
      " BSTI.json                           LOGC.json          TCMD.json\r\n",
      " BSVN.json                           LOMA.json          TDACU.json\r\n",
      " BTAI.json                           LOVE.json          \u001b[01;34mTemplates\u001b[0m/\r\n",
      " BV.json                             LQDA.json          TENB.json\r\n",
      " BWAY.json                           LTHM.json          THOR.json\r\n",
      " BWB.json                           \"LTN'U.json\"        TKKSU.json\r\n",
      " BWMCU.json                          LUNG.json          TLC.json\r\n",
      " BXG.json                            LX.json            TLRY.json\r\n",
      " BY.json                             MBIN.json          TLSA.json\r\n",
      " BYND.json                           MCB.json           TMCXU.json\r\n",
      " CAAP.json                           MDB.json           TME.json\r\n",
      " CADE.json                           MDJH.json          TNTR.json\r\n",
      " CANG.json                           MDRR.json          TOCA.json\r\n",
      " CARG.json                           MESA.json          TORC.json\r\n",
      " CASA.json                           MFACU.json         TOTAU.json\r\n",
      " CBLK.json                           MGP.json           TPGEU.json\r\n",
      " CBTX.json                           MGTA.json          TPGHU.json\r\n",
      " CBUS.json                           MGTX.json          TRHC.json\r\n",
      " CCB.json                            MIIIU.json         TRMT.json\r\n",
      " CCHU.json                           MMDMU.json         TRTX.json\r\n",
      " CCR.json                            MNCLU.json         TUSK.json\r\n",
      " CDAY.json                           MNLO.json          TWAV.json\r\n",
      " CDLX.json                           MOGU.json          tweets_AAPL.json\r\n",
      " CELC.json                           MOR.json           tweets_ANDAU.json\r\n",
      " CEPU.json                           MOTS.json          tweets_BV.json\r\n",
      " CHACU.json                          MQAI.json          tweets_CMSSU.json\r\n",
      " CHRA.json                           MRMR.json          tweets_CSSE.json\r\n",
      " CIFS.json                           MRNA.json          tweets_FAT.json\r\n",
      " CLBR.json                           MRSN.json          tweets_GTWY.json\r\n",
      " CLDR.json                           MSBI.json          tweets_KREF.json\r\n",
      " CLGN.json                           MSC.json           tweets_MCB.json\r\n",
      " CLPS.json                           MTECU.json         tweets_MGP.json\r\n",
      " CLXT.json                           MUDSU.json         tweets_MUDSU.json\r\n",
      " CMSSU.json                          MULE.json          tweets_ODT.json\r\n",
      " CMTA.json                           \u001b[01;34mMusic\u001b[0m/             tweets_PBTS.json\r\n",
      " CNACU.json                          MYFW.json          tweets_PZRX.json\r\n",
      " CNST.json                           NCNA.json          tweets_RUBY.json\r\n",
      " CNTX.json                           NCSM.json         'Tweets sentiment.ipynb'\r\n",
      " CODX.json                           NDRAU.json         tweets_SPRO.json\r\n",
      " COLD.json                           NEBUU.json         tweets_SRRK.json\r\n",
      " CPUL.json                           NESRU.json         tweets_TDACU.json\r\n",
      " CRNX.json                           NEWA.json          tweets_t.json\r\n",
      " CRVS.json                           NEW.json           tweets_TLRY.json\r\n",
      " CSCA.json                           NEXA.json          tweets_TOTAU.json\r\n",
      " CSSE.json                           NFE.json           tweets_USOU.json\r\n",
      " CSTX.json                           NINE.json          tweets_VAPO.json\r\n",
      " CTACU.json                          NIO.json           tweets_VCNX.json\r\n",
      " CTK.json                            NITE.json          TWLVU.json\r\n",
      " CTRA.json                           NIU.json           TWST.json\r\n",
      " CUE.json                            \u001b[01;34mnltk_data\u001b[0m/         TZACU.json\r\n",
      " CURO.json                           NMRK.json          UBX.json\r\n",
      " CURV.json                           NREF.json         'UMRX .json'\r\n",
      " CVNA.json                           NTGN.json          Untitled.ipynb\r\n",
      " CWK.json                            NTLA.json          UPWK.json\r\n",
      " DAVA.json                           OBNK.json          URGN.json\r\n",
      " DBX.json                            ODT.json           UROV.json\r\n",
      " DCPH.json                           OKTA.json          USOD.json\r\n",
      " DDMXU.json                          OMADU.json         USOU.json\r\n",
      " DDOC.json                           OMP.json           USX.json\r\n",
      " \u001b[01;34mDesktop\u001b[0m/                            ONE.json           UTC.json\r\n",
      " DESP.json                           OPRA.json          UXIN.json\r\n",
      " DMAC.json                           OPTN.json          VAPO.json\r\n",
      " DNLI.json                           ORTX.json          VCNX.json\r\n",
      " DOCU.json                           OSMT.json          VCTR.json\r\n",
      " \u001b[01;34mDocuments\u001b[0m/                          OSPRU.json         VERI.json\r\n",
      " DOGZ.json                           OSS.json           \u001b[01;34mVideos\u001b[0m/\r\n",
      " DOMO.json                           OVID.json          VIOT.json\r\n",
      " DOTAU.json                         'PACQU .json'       Vizio.json\r\n",
      " DOVA.json                           PAGS.json          VKTX.json\r\n",
      " \u001b[01;34mDownloads\u001b[0m/                          PARK.json          VNCR.json\r\n",
      " DTSS.json                           PBTS.json          VNTR.json\r\n",
      " EAF.json                            PCB.json           VPRL.json\r\n",
      " EAGLU.json                          PCIM.json          VRCA.json\r\n",
      " EB.json                             PDD.json           VRI.json\r\n",
      " ECOR.json                           PETC.json          VRIO.json\r\n",
      " EDIT.json                           PETQ.json          VRNA.json\r\n",
      " EDTXU.json                          PETZ.json          VTEC.json\r\n",
      " EEX.json                            PHAS.json         'VTIQU .json'\r\n",
      " EIDX.json                           PHCF.json          VTUS.json\r\n",
      " ELAN.json                           \u001b[01;34mPictures\u001b[0m/          VZIO.json\r\n",
      " ENTX.json                           PLAN.json          WAAS.json\r\n",
      " ENVN.json                           PLIN.json          WAFU.json\r\n",
      " EOLS.json                          'PLSE .json'        WALL.json\r\n",
      " EPRT.json                           PLSE.json          WEI.json\r\n",
      " EQH.json                            PLTEU.json         WGRD.json\r\n",
      " EQ.json                             PLYM.json          WHD.json\r\n",
      " ERYP.json                           PMAR.json          WISA.json\r\n",
      " ESQ.json                            POLY.json          WOW.json\r\n",
      " ESTA.json                           PPDF.json          WRLSU.json\r\n",
      " ESTC.json                           PQG.json           WTTR.json\r\n",
      " ETON.json                           PROS.json          XERS.json\r\n",
      " ETTX.json                           PRT.json           XMLP.json\r\n",
      " EVER.json                           PRVB.json          XRF.json\r\n",
      " EVLO.json                           PSAV.json          XSPL.json\r\n",
      " EVOP.json                           PSIT.json          XTF.json\r\n",
      " examples.desktop                    PS.json            XYF.json\r\n",
      " EYE.json                            PSTX.json          YI.json\r\n",
      " EYEN.json                           PT.json            YMAB.json\r\n",
      " FAMI.json                           \u001b[01;34mPublic\u001b[0m/            YOGA.json\r\n",
      " FAT.json                            PUMP.json          YZCM.json\r\n",
      " \u001b[01;34mfile_name.json\u001b[0m/                     PUYI.json          ZKIN.json\r\n",
      " FIXX.json                           PVTL.json          ZLAB.json\r\n",
      " FMCIU.json                          \u001b[01;34mPycharmProjects\u001b[0m/   ZLND.json\r\n",
      " FND.json                            PZRX.json          ZOOX.json\r\n",
      " FNKO.json                           QD.json            ZS.json\r\n",
      " FOCS.json                           QES.json           ZUO.json\r\n",
      " FPACU.json                          QFIN.json          ZYME.json\r\n",
      " FPH.json                            QTRX.json\r\n",
      " FSACU.json                          QTT.json\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = 'color:darkgreen'> Filter out all available tickers and mine their stock prices  </span style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     #earnings scheduled after the market closes today:  $CRM $BZUN $HPQ $GME $HPE $GES $CPRT $CAL $SE $QADA… https://t.co/GkeI1ezmOj                \n",
       "1     RT @TimDohrmann: #Sensera $SE1 CEO Ralph Schmitt presenting at Thursday’s #ASX #smallcap conference in #Dubai: 👀 the story with Ralph’s sli…    \n",
       "10    Huge coup for #IoT #sensor developer #Sensera $SE1 securing Ralph Schmitt as CEO, ex-@ToshibaUSA &amp; @CypressSemi… https://t.co/KJy0y1JztP    \n",
       "11    📈 in #driverless trucks = 📈 in demand for location tracking/collision avoidance #sensors. Positive for #Sensera $SE1 https://t.co/iNCwc3dqW8    \n",
       "12    #IoT #microsensor biz #Sensera $SE1 achieves Sept qtr cash receipts +151%, points to great growth prospects:… https://t.co/B7ZCRtXeCW           \n",
       "13    🎧 Listen in: #Sensera's Matt Morgan explains revenue impact for $SE1 of exclusive new chip supply deal: audio 📼 her… https://t.co/Zr2u3t9iQj    \n",
       "14    30 mins til Sensera $SE1's Matt Morgan launches investor conf call re sales traction with @smartbow_at: dial in her… https://t.co/fXs0WIjjDU    \n",
       "15    Designer &amp; maker of high-value #microsensors Sensera $SE1 signs multiyear exclusive supply deal with @smartbow_at… https://t.co/LEgvGoftmj  \n",
       "16    #Sensera is taking its #IoT sensors to the Livestock Health &amp; Mine Safety mkts — brief note on how $SE1 adds value:… https://t.co/KMLOEtzmwU\n",
       "17    Shares +5% in Sensera $SE1, #microsensor designer/maker for #IoT clients, on goals w/ newly-acquired Nanotron:… https://t.co/sugZaCkyEF         \n",
       "18    Why Is Elon Musk On Welfare? https://t.co/bIzz9wXOcL $ACW $AMC $CKEC $CPHD $CTAS $DHR $ENB $FDML $GK $GS $IEP $LOCK $MENT $NCR $SE $SIEGY       \n",
       "19    #IPO today:\\n\\n$SE\\n\\n#stocks #investing #equities #trading                                                                                     \n",
       "2     RT @FNArena: TMT Analytics believes Sensera can drive gross margins above 60% as facility utilisation  increases $SE1 https://t.co/jgSqZl2W…    \n",
       "20    #IPO today:\\n\\n$SE\\n\\n#stocks #investing #equities #trading                                                                                     \n",
       "21    Sea Limited (IPO) $SE is now on RetailRoadshow. https://t.co/qVeU6FCNoj                                                                         \n",
       "22    Sea Limited (IPO) $SE is now on RetailRoadshow. https://t.co/qVeU6FCNoj                                                                         \n",
       "23    Sea Limited (IPO) $SE is now on RetailRoadshow. https://t.co/qVeU6FCNoj                                                                         \n",
       "24    RT @highpeakscap: Meeting management today for an update. $SE1 continues to operationally tick boxes, wont be long until the market recogni…    \n",
       "25    RT @TimDohrmann: #Sensera $SE1 CEO Ralph Schmitt presenting at Thursday’s #ASX #smallcap conference in #Dubai: 👀 the story with Ralph’s sli…    \n",
       "26    Stocks in play: $REDU $SE $QD #stocks #IPO #DayTrading                                                                                          \n",
       "27    Stocks in play: $BID $GOLF $SE                                                                                                                  \n",
       "28    Stocks in play: $REDU $SE $QD #stocks #IPO #DayTrading                                                                                          \n",
       "29    Stocks in play: $BID $GOLF $SE                                                                                                                  \n",
       "3     RT @StockheadAU: Internet of Things chipmaker Sensera has finalised a deal to put motion-tracking sensors in cows. $SE1 #ASX #ausbiz https:…    \n",
       "30    RT @Marlon_Dee: $SE might be worth keeping an eye on above 15.77, 16.10, 16.70; could get moving again. China co. operating in 1) Digital e…    \n",
       "31    Investing in Dividend Growth Stocks by Shane Forbes: https://t.co/5XFTIeu4Po $COP $F $SE $LYB $CNP $SO $M                                       \n",
       "32    Investing in Dividend Growth Stocks by Shane Forbes: https://t.co/5XFTIeu4Po $COP $F $SE $LYB $CNP $SO $M                                       \n",
       "33    Investing in Dividend Growth Stocks by Shane Forbes: https://t.co/5XFTIeu4Po $COP $F $SE $LYB $CNP $SO $M                                       \n",
       "34    #IOT and #sensor based product provider Sensera $SE1 reported record #revenue for the March quarter. Cash receipts… https://t.co/Hucy1ErmbM     \n",
       "35    From animal health to mining - Sensera $SE1 location chip solution will now be used by the mining industry for unde… https://t.co/38VcFWWUtm    \n",
       "36    Sensera $SE1 signs exclusive agreement with Smartbow- provider of animal health solutions. Sensera’s sensors to col… https://t.co/g9UPjYA5I2    \n",
       "37    Nano sensor developer Sensera $SE1 posts USD3.4m loss for HY18. Acquisition of Nanotron during the half year comple… https://t.co/lKaXVwpVHo    \n",
       "38    Investing in Dividend Growth Stocks by Shane Forbes: https://t.co/5XFTIeu4Po $COP $F $SE $LYB $CNP $SO $M                                       \n",
       "39    Investing in Dividend Growth Stocks by Shane Forbes: https://t.co/5XFTIeu4Po $COP $F $SE $LYB $CNP $SO $M                                       \n",
       "4     #Sensera announces deal with @abiomedimpella - via @Finnewsnetwork https://t.co/ffem0Al80H $SE1 #Abiomed… https://t.co/lPi24YIIYg               \n",
       "40    Investing in Dividend Growth Stocks by Shane Forbes: https://t.co/5XFTIeu4Po $COP $F $SE $LYB $CNP $SO $M                                       \n",
       "41    Currently 1% cash so purchases will be limited but here's my watchlist.\\n$AMTD $SCHW $GRUB $OMCL $MZOR $CARG $PRAH $SE                          \n",
       "42    Gappers of Interest:  Long - $BOOT $NTRP $RIO $TGT $MU $OKTA $SE    Short -  Not an entry here watch the open and s… https://t.co/QHmbayxm2A    \n",
       "43    RT @MikeEdward_TTG: VIDEO: Check out my +25% SWING TRADE RECAP in $SE. Great swing trade lessons. https://t.co/HuZMMVjoIH                       \n",
       "44    +24% as Previous Video Lesson Paid Big &amp; 2 Swing Trade Updates $SE $SESN $ADOM https://t.co/2MunFzDP2V                                      \n",
       "45    RT @MikeEdward_TTG: 3 NEW SWING TRADES opened on May 16th: $CHEK $EBIO $SE See my Video Analysis here: https://t.co/c8SqmVCMBV https://t.co…    \n",
       "46    Sea Could Sink When #IPO Lockup Expires $SE https://t.co/nwLhwpMiYO                                                                             \n",
       "5     Good news for #IoT #sensor developer #Sensera $SE1 gaining clearance to reveal anchor client with which it just ink… https://t.co/eL7FCzZfpz    \n",
       "6     Solid update from #Sensera $SE1 paves the path to more than $9 million in FY18 revenue from #microsensor design and… https://t.co/WmThk8i30V    \n",
       "7     New 3-year multimillion dollar supply agreement for #IoT #sensor developer #Sensera $SE1 with anchor client, an $8… https://t.co/qGcfUvxSxJ     \n",
       "8     Missed the 📞 with incoming #Sensera $SE1 CEO Ralph Schmitt? Listen back here to hear about Ralph's #semiconductor e… https://t.co/TLGLSVsoEv    \n",
       "9     RT @Jessica_Danelle: . #Sensera $SE1 hires #Nasdaq experienced CEO Ralph Schmitt, who has worked with #Toshiba #Ausbiz https://t.co/JJhYkbN…    \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(\"SE.json\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# change treshold depending on the cashtag count. I used a small data set to test the code so i lowerd it to 2\n",
    "# but once u start using bigger amounts of data u can up this to 500 or more depending on the results.\n",
    "treshold = 30 # collecting IPO's that contains threshold amount of tweets.\n",
    "\n",
    "#cashtags will now contain all the cashtags in the tweets, this includes duplicates.\n",
    "cashtags = df.select(explode(\"cashtags\").alias(\"cashtag\"))\n",
    "#cashtag_count will now contain all the cashtags with a count of how many times they occur.\n",
    "cashtag_count = cashtags.groupBy(\"cashtag\").count().sort(\"count\", ascending = False)\n",
    "#cashtag_count_above_treshold will only contain the cashtags above the treshold.\n",
    "cashtag_count_above_treshold = cashtag_count.filter(\"count >= {}\".format(treshold))\n",
    "\n",
    "print((\"Considering all tweets, there is a total amount of {} cashtags.\\n\" +  \n",
    "       \"In total, there are {} distinct cashtags.\\n\" +\n",
    "       \"{} of these cashtags occur more than {} times.\").format(cashtags.count(), \n",
    "                                                                cashtag_count.distinct().count(), \n",
    "                                                                cashtag_count_above_treshold.count(),\n",
    "                                                                treshold))\n",
    "\n",
    "cashtag_count.show(5) # printing cashtags\n",
    "\n",
    "#ticker_rdd will now only contain the cashtags above the treshold, these will be put into\n",
    "#the ticker_list\n",
    "ticker_rdd = cashtag_count_above_treshold.select(\"cashtag\").collect()\n",
    "\n",
    "ticker_list = []\n",
    "for i in tickers:\n",
    "    ticker_list.append(i.cashtag)\n",
    "print(ticker_list[0:10])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the current proffit file, because we are appending, not writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm /home/mma/Downloads/IPO_Mining/stock_mining/profit.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ODT\n",
      "[[0.0, 'ODT']]\n",
      "ESQ\n",
      "[[-0.0409, 'ESQ']]\n",
      "IQ\n",
      "[[-0.1456, 'IQ']]\n",
      "MOGU\n",
      "[[0.1667, 'MOGU']]\n",
      "UXIN\n",
      "[[-0.0702, 'UXIN']]\n",
      "CLBR\n",
      "ALGRU\n",
      "[[-0.001, 'ALGRU']]\n",
      "SLDB\n",
      "[[-0.1129, 'SLDB']]\n",
      "MRNA\n",
      "[[-0.1545, 'MRNA']]\n",
      "HLG\n",
      "[[0.2, 'HLG']]\n",
      "SNDX\n",
      "[[-0.0083, 'SNDX']]\n",
      "VTEC\n",
      "PARK\n",
      "AGMH\n",
      "[[0.1455, 'AGMH']]\n",
      "BXG\n",
      "[[0.0359, 'BXG']]\n",
      "JG\n",
      "[[-0.0222, 'JG']]\n",
      "AAAU\n",
      "[[-0.0084, 'AAAU']]\n",
      "VPRL\n",
      "BWB\n",
      "[[-0.0032, 'BWB']]\n",
      "UPWK\n",
      "[[-0.0791, 'UPWK']]\n",
      "ACAMU\n",
      "[[-0.002, 'ACAMU']]\n",
      "BILI\n",
      "[[0.1469, 'BILI']]\n",
      "VRCA\n",
      "[[-0.125, 'VRCA']]\n",
      "JFIN\n",
      "[[0.4682, 'JFIN']]\n",
      "ESTC\n",
      "[[0.0, 'ESTC']]\n",
      "ARLO\n",
      "[[0.1946, 'ARLO']]\n",
      "HZ\n",
      "[[0.0, 'HZ']]\n",
      "BAND\n",
      "[[0.0062, 'BAND']]\n",
      "CSSE\n",
      "[[-0.2885, 'CSSE']]\n",
      "CANG\n",
      "[[0.0016, 'CANG']]\n",
      "HHR\n",
      "[[0.0744, 'HHR']]\n",
      "DOMO\n",
      "[[0.1471, 'DOMO']]\n",
      "ARA\n",
      "[[0.0, 'ARA']]\n",
      "ORTX\n",
      "[[-0.1765, 'ORTX']]\n",
      "GTHX\n",
      "[[0.0, 'GTHX']]\n",
      "TC\n",
      "[[0.0, 'TC']]\n",
      "ADT\n",
      "[[-0.0204, 'ADT']]\n",
      "SAIL\n",
      "[[-0.1275, 'SAIL']]\n",
      "ZS\n",
      "[[0.2, 'ZS']]\n",
      "GNTY\n",
      "[[0.0033, 'GNTY']]\n",
      "IPSC\n",
      "RWGEU\n",
      "MMDMU\n",
      "[[0.002, 'MMDMU']]\n",
      "SBPH\n",
      "[[-0.016, 'SBPH']]\n",
      "MCB\n",
      "[[-0.0447, 'MCB']]\n",
      "XTF\n",
      "OBNK\n",
      "[[0.0, 'OBNK']]\n",
      "YOGA\n",
      "[[-0.1182, 'YOGA']]\n",
      "GOSS\n",
      "[[-0.0558, 'GOSS']]\n",
      "ALLK\n",
      "[[0.1489, 'ALLK']]\n",
      "VKTX\n",
      "[[0.0541, 'VKTX']]\n",
      "FIXX\n",
      "[[-0.1887, 'FIXX']]\n",
      "GWRS\n",
      "[[-0.2312, 'GWRS']]\n",
      "OPRA\n",
      "[[-0.0858, 'OPRA']]\n",
      "SFTY\n",
      "HOTH\n",
      "[[0.2362, 'HOTH']]\n",
      "ADIN\n",
      "[[0.0, 'ADIN']]\n",
      "RYTM\n",
      "[[0.2925, 'RYTM']]\n",
      "OSS\n",
      "[[-0.1207, 'OSS']]\n",
      "VNCR\n",
      "GRSHU\n",
      "[[0.0, 'GRSHU']]\n",
      "LOACU\n",
      "[[0.001, 'LOACU']]\n",
      "CADE\n",
      "[[-0.0227, 'CADE']]\n",
      "RETO\n",
      "[[0.2735, 'RETO']]\n",
      "ARMO\n",
      "[[0.0621, 'ARMO']]\n",
      "HAIR\n",
      "[[-0.2179, 'HAIR']]\n",
      "CWK\n",
      "[[-0.0106, 'CWK']]\n",
      "QD\n",
      "[[-0.1505, 'QD']]\n",
      "DESP\n",
      "[[0.0959, 'DESP']]\n",
      "FTSV\n",
      "[[0.0027, 'FTSV']]\n",
      "KREF\n",
      "[[0.0538, 'KREF']]\n",
      "PROS\n",
      "HUD\n",
      "[[-0.0195, 'HUD']]\n",
      "OSPRU\n",
      "[[0.0, 'OSPRU']]\n",
      "SRTSU\n",
      "[[0.1179, 'SRTSU']]\n",
      "EDIT\n",
      "[[0.0111, 'EDIT']]\n",
      "LBC\n",
      "[[0.0033, 'LBC']]\n",
      "EYEN\n",
      "[[-0.008, 'EYEN']]\n",
      "CLGN\n",
      "[[0.0, 'CLGN']]\n",
      "NTLA\n",
      "[[0.0045, 'NTLA']]\n",
      "LTN'U\n",
      "ANDAU\n",
      "[[-0.007, 'ANDAU']]\n",
      "AMRH\n",
      "[[0.1565, 'AMRH']]\n",
      "CNACU\n",
      "[[0.001, 'CNACU']]\n",
      "CTACU\n",
      "[[0.0, 'CTACU']]\n",
      "CCB\n",
      "[[0.0092, 'CCB']]\n",
      "BRY\n",
      "[[-0.0089, 'BRY']]\n",
      "APPN\n",
      "[[0.0007, 'APPN']]\n",
      "ALEC\n",
      "[[-0.0374, 'ALEC']]\n",
      "LTHM\n",
      "[[0.0443, 'LTHM']]\n",
      "PLTEU\n",
      "HJLI\n",
      "[[0.048, 'HJLI']]\n",
      "TORC\n",
      "[[-0.0934, 'TORC']]\n",
      "MDB\n",
      "[[-0.0282, 'MDB']]\n",
      "OMADU\n",
      "NCSM\n",
      "[[0.0005, 'NCSM']]\n",
      "CLXT\n",
      "[[0.1842, 'CLXT']]\n",
      "BFRA\n",
      "[[-0.0631, 'BFRA']]\n",
      "ARCE\n",
      "[[-0.0408, 'ARCE']]\n",
      "QFIN\n",
      "[[-0.0184, 'QFIN']]\n",
      "DMAC\n",
      "[[0.0, 'DMAC']]\n",
      "XYF\n",
      "[[-0.202, 'XYF']]\n",
      "UTC\n",
      "[[0.0, 'UTC']]\n",
      "SLGG\n",
      "[[-0.2273, 'SLGG']]\n",
      "MGTX\n",
      "[[-0.0066, 'MGTX']]\n",
      "FTSI\n",
      "[[0.0847, 'FTSI']]\n",
      "VRNA\n",
      "[[0.0, 'VRNA']]\n",
      "EIDX\n",
      "[[0.156, 'EIDX']]\n",
      "DOCU\n",
      "[[0.0455, 'DOCU']]\n",
      "CDLX\n",
      "[[0.105, 'CDLX']]\n",
      "PRT\n",
      "[[-0.0312, 'PRT']]\n",
      "USOU\n",
      "[[-0.0511, 'USOU']]\n",
      "RCUS\n",
      "[[-0.15, 'RCUS']]\n",
      "PETQ\n",
      "[[0.1105, 'PETQ']]\n",
      "GMHIU\n",
      "[[-0.002, 'GMHIU']]\n",
      "BCML\n",
      "[[0.0, 'BCML']]\n",
      "TME\n",
      "[[-0.0071, 'TME']]\n",
      "ATIF\n",
      "[[-0.01, 'ATIF']]\n",
      "PETZ\n",
      "[[-0.0504, 'PETZ']]\n",
      "AUTL\n",
      "[[-0.1071, 'AUTL']]\n",
      "TLRY\n",
      "[[-0.0286, 'TLRY']]\n",
      "APM\n",
      "[[-0.0918, 'APM']]\n",
      "PVTL\n",
      "[[-0.0609, 'PVTL']]\n",
      "CMTA\n",
      "[[-0.1041, 'CMTA']]\n",
      "SGBX\n",
      "[[0.0, 'SGBX']]\n",
      "STNLU\n",
      "[[-0.0089, 'STNLU']]\n",
      "LEVB\n",
      "[[0.0756, 'LEVB']]\n",
      "RVLV\n",
      "IPOAU\n",
      "CDAY\n",
      "[[0.0799, 'CDAY']]\n",
      "SNAP\n",
      "[[0.02, 'SNAP']]\n",
      "LFACU\n",
      "[[-0.002, 'LFACU']]\n",
      "THOR\n",
      "[[0.0, 'THOR']]\n",
      "SSLJ\n",
      "[[-0.282, 'SSLJ']]\n",
      "SRRK\n",
      "[[0.0877, 'SRRK']]\n",
      "STRO\n",
      "[[0.0053, 'STRO']]\n",
      "QES\n",
      "[[0.0, 'QES']]\n",
      "PETC\n",
      "UBX\n",
      "[[-0.1237, 'UBX']]\n",
      "AQUA\n",
      "[[0.0932, 'AQUA']]\n",
      "XSPL\n",
      "[[0.0653, 'XSPL']]\n",
      "FPH\n",
      "[[-0.0105, 'FPH']]\n",
      "PLYM\n",
      "[[0.0016, 'PLYM']]\n",
      "CURV\n",
      "AVI\n",
      "MOR\n",
      "[[0.0057, 'MOR']]\n",
      "MESA\n",
      "[[-0.0208, 'MESA']]\n",
      "PHAS\n",
      "[[0.0, 'PHAS']]\n",
      "AMGP\n",
      "[[-0.0068, 'AMGP']]\n",
      "ASLN\n",
      "[[-0.0878, 'ASLN']]\n",
      "GSHD\n",
      "[[0.3125, 'GSHD']]\n",
      "TPGHU\n",
      "PUYI\n",
      "[[0.0543, 'PUYI']]\n",
      "BTAI\n",
      "[[0.0009, 'BTAI']]\n",
      "TDACU\n",
      "[[0.002, 'TDACU']]\n",
      "ICLK\n",
      "[[0.0, 'ICLK']]\n",
      "ANCN\n",
      "[[-0.0609, 'ANCN']]\n",
      "LEVL\n",
      "[[0.009, 'LEVL']]\n",
      "WAFU\n",
      "[[-0.2115, 'WAFU']]\n",
      "PLSE\n",
      "[[0.0097, 'PLSE']]\n",
      "MDRR\n",
      "[[-0.0105, 'MDRR']]\n",
      "PRVB\n",
      "[[-0.3988, 'PRVB']]\n",
      "DBX\n",
      "[[-0.0179, 'DBX']]\n",
      "NMRK\n",
      "[[-0.0071, 'NMRK']]\n",
      "PHCF\n",
      "[[-0.0083, 'PHCF']]\n",
      "DDOC\n",
      "TOTAU\n",
      "[[0.001, 'TOTAU']]\n",
      "TCDA\n",
      "[[0.04, 'TCDA']]\n",
      "SOLY\n",
      "[[0.0146, 'SOLY']]\n",
      "HTTP Error 505: HTTP Version Not Supported\n",
      "Problem fetching UMRX \n",
      "UMRX \n",
      "[[0.0146, 'UMRX ']]\n",
      "TRTX\n",
      "[[-0.001, 'TRTX']]\n",
      "SLGL\n",
      "[[-0.1102, 'SLGL']]\n",
      "BWAY\n",
      "[[-0.0045, 'BWAY']]\n",
      "IMAC\n",
      "[[0.0514, 'IMAC']]\n",
      "ENVN\n",
      "SAMAU\n",
      "[[-0.005, 'SAMAU']]\n",
      "GDI\n",
      "[[-0.0071, 'GDI']]\n",
      "FSCT\n",
      "[[-0.0286, 'FSCT']]\n",
      "RYB\n",
      "[[0.0738, 'RYB']]\n",
      "NINE\n",
      "[[0.0875, 'NINE']]\n",
      "MNCLU\n",
      "[[0.0005, 'MNCLU']]\n",
      "MUDSU\n",
      "[[0.0, 'MUDSU']]\n",
      "WOW\n",
      "[[0.0312, 'WOW']]\n",
      "COLD\n",
      "[[0.0154, 'COLD']]\n",
      "ARGX\n",
      "[[0.2849, 'ARGX']]\n",
      "GBSG\n",
      "ASV\n",
      "[[0.0537, 'ASV']]\n",
      "EQH\n",
      "[[0.0299, 'EQH']]\n",
      "KALA\n",
      "[[0.1072, 'KALA']]\n",
      "SPRO\n",
      "[[-0.1235, 'SPRO']]\n",
      "ARYAU\n",
      "[[0.001, 'ARYAU']]\n",
      "SNNA\n",
      "[[0.0105, 'SNNA']]\n",
      "WEI\n",
      "[[-0.0238, 'WEI']]\n",
      "EB\n",
      "[[0.0139, 'EB']]\n",
      "SE\n",
      "[[0.0131, 'SE']]\n",
      "PCB\n",
      "[[0.0, 'PCB']]\n",
      "GH\n",
      "[[0.1604, 'GH']]\n",
      "NFE\n",
      "[[-0.0136, 'NFE']]\n",
      "VCTR\n",
      "[[-0.0531, 'VCTR']]\n",
      "VRI\n",
      "NREF\n",
      "SI\n",
      "VNTR\n",
      "[[-0.0115, 'VNTR']]\n",
      "TBRGU\n",
      "[[0.0, 'TBRGU']]\n",
      "FND\n",
      "[[0.0595, 'FND']]\n",
      "MGTA\n",
      "[[-0.0632, 'MGTA']]\n",
      "HKIT\n",
      "HCCHU\n",
      "[[0.002, 'HCCHU']]\n",
      "SOGO\n",
      "[[0.0385, 'SOGO']]\n",
      "SFIX\n",
      "[[-0.1062, 'SFIX']]\n",
      "GLDM\n",
      "[[-0.001, 'GLDM']]\n",
      "BIOX\n",
      "[[-0.0103, 'BIOX']]\n",
      "TWLVU\n",
      "[[0.004, 'TWLVU']]\n",
      "CNTX\n",
      "[[0.0, 'CNTX']]\n",
      "HUYA\n",
      "[[0.0361, 'HUYA']]\n",
      "NEXA\n",
      "[[0.0545, 'NEXA']]\n",
      "CLPS\n",
      "[[-0.0434, 'CLPS']]\n",
      "NEW\n",
      "[[0.2353, 'NEW']]\n",
      "INDUU\n",
      "[[0.001, 'INDUU']]\n",
      "GSAHU\n",
      "MFACU\n",
      "BE\n",
      "[[0.3369, 'BE']]\n",
      "ALACU\n",
      "[[-0.003, 'ALACU']]\n",
      "BSTI\n",
      "[[-0.0836, 'BSTI']]\n",
      "MSC\n",
      "[[-0.0606, 'MSC']]\n",
      "SUPV\n",
      "[[0.0, 'SUPV']]\n",
      "CASA\n",
      "[[0.0667, 'CASA']]\n",
      "CSCA\n",
      "WRLSU\n",
      "[[-0.001, 'WRLSU']]\n",
      "EAF\n",
      "[[-0.0203, 'EAF']]\n",
      "MYFW\n",
      "[[-0.0506, 'MYFW']]\n",
      "CURO\n",
      "[[-0.0405, 'CURO']]\n",
      "SXTC\n",
      "[[0.2784, 'SXTC']]\n",
      "FPACU\n",
      "SSTI\n",
      "[[0.1787, 'SSTI']]\n",
      "REPL\n",
      "[[-0.0525, 'REPL']]\n",
      "ONE\n",
      "[[0.057, 'ONE']]\n",
      "CCHU\n",
      "STIM\n",
      "[[0.1112, 'STIM']]\n",
      "TPGEU\n",
      "SPAQU\n",
      "PS\n",
      "[[0.0, 'PS']]\n",
      "AZRE\n",
      "[[-0.0943, 'AZRE']]\n",
      "RBB\n",
      "[[0.0013, 'RBB']]\n",
      "YI\n",
      "[[-0.1761, 'YI']]\n",
      "REPX\n",
      "LOB\n",
      "[[0.0308, 'LOB']]\n",
      "DCPH\n",
      "[[-0.1197, 'DCPH']]\n",
      "HMI\n",
      "[[-0.0625, 'HMI']]\n",
      "FTACU\n",
      "[[-0.002, 'FTACU']]\n",
      "ATNX\n",
      "[[0.0467, 'ATNX']]\n",
      "MULE\n",
      "[[0.0206, 'MULE']]\n",
      "UROV\n",
      "[[0.1095, 'UROV']]\n",
      "XERS\n",
      "[[0.2838, 'XERS']]\n",
      "TRHC\n",
      "[[0.0122, 'TRHC']]\n",
      "RLTY\n",
      "VIOT\n",
      "[[-0.0442, 'VIOT']]\n",
      "ATUS\n",
      "[[0.0351, 'ATUS']]\n",
      "Vizio\n",
      "MOTS\n",
      "[[-0.124, 'MOTS']]\n",
      "FSACU\n",
      "[[0.0, 'FSACU']]\n",
      "ARDS\n",
      "[[-0.0015, 'ARDS']]\n",
      "MDJH\n",
      "[[-0.0105, 'MDJH']]\n",
      "LOMA\n",
      "[[-0.0139, 'LOMA']]\n",
      "BDFC\n",
      "ZKIN\n",
      "[[0.1104, 'ZKIN']]\n",
      "LOGC\n",
      "[[0.0, 'LOGC']]\n",
      "USOD\n",
      "[[0.0, 'USOD']]\n",
      "CHACU\n",
      "GTES\n",
      "[[-0.0289, 'GTES']]\n",
      "AKCA\n",
      "[[0.1661, 'AKCA']]\n",
      "CHRA\n",
      "[[0.0213, 'CHRA']]\n",
      "LACQU\n",
      "[[-0.004, 'LACQU']]\n",
      "BHVN\n",
      "[[-0.125, 'BHVN']]\n",
      "VZIO\n",
      "CPUL\n",
      "HTTP Error 505: HTTP Version Not Supported\n",
      "Problem fetching VTIQU \n",
      "VTIQU \n",
      "PLIN\n",
      "LBM\n",
      "[[-0.0196, 'LBM']]\n",
      "GSKY\n",
      "[[0.0546, 'GSKY']]\n",
      "MQAI\n",
      "CVNA\n",
      "[[-0.1778, 'CVNA']]\n",
      "DTSS\n",
      "[[0.0, 'DTSS']]\n",
      "URGN\n",
      "[[0.0519, 'URGN']]\n",
      "TAI\n",
      "[[0.0, 'TAI']]\n",
      "BY\n",
      "[[-0.0147, 'BY']]\n",
      "BRACU\n",
      "[[0.0059, 'BRACU']]\n",
      "SMAR\n",
      "[[0.0598, 'SMAR']]\n",
      "APLS\n",
      "[[-0.0324, 'APLS']]\n",
      "GNPX\n",
      "[[0.0021, 'GNPX']]\n",
      "CLDR\n",
      "[[0.0169, 'CLDR']]\n",
      "FMCIU\n",
      "[[0.002, 'FMCIU']]\n",
      "ITRM\n",
      "[[0.0118, 'ITRM']]\n",
      "RNGR\n",
      "[[0.0157, 'RNGR']]\n",
      "WTTR\n",
      "[[-0.0591, 'WTTR']]\n",
      "TUSK\n",
      "[[-0.0293, 'TUSK']]\n",
      "AMBO\n",
      "[[-0.2144, 'AMBO']]\n",
      "EAGLU\n",
      "[[0.0, 'EAGLU']]\n",
      "FAT\n",
      "[[-0.1156, 'FAT']]\n",
      "SCPH\n",
      "[[-0.0105, 'SCPH']]\n",
      "NITE\n",
      "[[0.3913, 'NITE']]\n",
      "OKTA\n",
      "[[-0.0101, 'OKTA']]\n",
      "YZCM\n",
      "AGS\n",
      "[[0.0882, 'AGS']]\n",
      "POLY\n",
      "[[0.0, 'POLY']]\n",
      "RMED\n",
      "[[-0.0698, 'RMED']]\n",
      "PUMP\n",
      "[[-0.0333, 'PUMP']]\n",
      "IFRX\n",
      "[[0.0, 'IFRX']]\n",
      "PSTX\n",
      "TRMT\n",
      "[[-0.0259, 'TRMT']]\n",
      "TZACU\n",
      "[[0.002, 'TZACU']]\n",
      "SAFE\n",
      "[[0.0, 'SAFE']]\n",
      "ZOOX\n",
      "TENB\n",
      "[[-0.0833, 'TENB']]\n",
      "ARDT\n",
      "LOVE\n",
      "[[-0.0404, 'LOVE']]\n",
      "AVLR\n",
      "[[0.284, 'AVLR']]\n",
      "BOMN\n",
      "[[0.0154, 'BOMN']]\n",
      "CCR\n",
      "[[-0.003, 'CCR']]\n",
      "GTWY\n",
      "ENTX\n",
      "[[-0.1397, 'ENTX']]\n",
      "CRVS\n",
      "[[-0.05, 'CRVS']]\n",
      "SOI\n",
      "[[-0.0541, 'SOI']]\n",
      "EVOP\n",
      "[[-0.0514, 'EVOP']]\n",
      "LASR\n",
      "[[0.1717, 'LASR']]\n",
      "AGLE\n",
      "[[-0.023, 'AGLE']]\n",
      "KOD\n",
      "[[0.016, 'KOD']]\n",
      "BOLD\n",
      "[[-0.2037, 'BOLD']]\n",
      "ADIL\n",
      "[[0.0125, 'ADIL']]\n",
      "HYRE\n",
      "[[-0.0811, 'HYRE']]\n",
      "FOCS\n",
      "SONO\n",
      "[[0.2444, 'SONO']]\n",
      "AVRO\n",
      "[[0.0667, 'AVRO']]\n",
      "MRMR\n",
      "RETA\n",
      "[[0.1817, 'RETA']]\n",
      "ALTR\n",
      "[[0.0557, 'ALTR']]\n",
      "SWI\n",
      "[[-0.0051, 'SWI']]\n",
      "HTTP Error 505: HTTP Version Not Supported\n",
      "Problem fetching PLSE \n",
      "PLSE \n",
      "[[-0.0051, 'PLSE ']]\n",
      "PAGS\n",
      "[[0.0355, 'PAGS']]\n",
      "ZUO\n",
      "[[0.0, 'ZUO']]\n",
      "KRYS\n",
      "[[0.0133, 'KRYS']]\n",
      "SURF\n",
      "[[-0.0963, 'SURF']]\n",
      "TBIO\n",
      "[[0.0017, 'TBIO']]\n",
      "OVID\n",
      "[[-0.125, 'OVID']]\n",
      "ARVN\n",
      "[[-0.2357, 'ARVN']]\n",
      "EYE\n",
      "[[0.0014, 'EYE']]\n",
      "VAPO\n",
      "[[0.0323, 'VAPO']]\n",
      "TBLT\n",
      "[[-0.2778, 'TBLT']]\n",
      "MSBI\n",
      "[[-0.0073, 'MSBI']]\n",
      "PLAN\n",
      "[[0.0021, 'PLAN']]\n",
      "ATXI\n",
      "[[0.0855, 'ATXI']]\n",
      "TKKSU\n",
      "[[-0.004, 'TKKSU']]\n",
      "DOTAU\n",
      "[[0.006, 'DOTAU']]\n",
      "CMSSU\n",
      "[[0.0, 'CMSSU']]\n",
      "ALLO\n",
      "[[0.1364, 'ALLO']]\n",
      "EQ\n",
      "[[-0.0244, 'EQ']]\n",
      "TWST\n",
      "[[0.0769, 'TWST']]\n",
      "TLSA\n",
      "[[-0.2048, 'TLSA']]\n",
      "JMPW\n",
      "ECOR\n",
      "[[0.1028, 'ECOR']]\n",
      "XRF\n",
      "[[-0.0376, 'XRF']]\n",
      "DOGZ\n",
      "[[0.0, 'DOGZ']]\n",
      "anaplan\n",
      "NIO\n",
      "[[0.1, 'NIO']]\n",
      "APTX\n",
      "[[0.1609, 'APTX']]\n",
      "PQG\n",
      "[[0.0147, 'PQG']]\n",
      "HTTP Error 505: HTTP Version Not Supported\n",
      "Problem fetching PACQU \n",
      "PACQU \n",
      "[[0.0147, 'PACQU ']]\n",
      "NEWA\n",
      "[[0.2199, 'NEWA']]\n",
      "RRI\n",
      "VRIO\n",
      "SMXT\n",
      "FNKO\n",
      "[[-0.1162, 'FNKO']]\n",
      "KLDO\n",
      "[[0.1117, 'KLDO']]\n",
      "ILPT\n",
      "[[-0.0148, 'ILPT']]\n",
      "PSAV\n",
      "MNLO\n",
      "[[0.4005, 'MNLO']]\n",
      "TLC\n",
      "[[-0.0429, 'TLC']]\n",
      "RDFN\n",
      "[[0.1094, 'RDFN']]\n",
      "TCMD\n",
      "[[0.0705, 'TCMD']]\n",
      "IMRN\n",
      "[[-0.0968, 'IMRN']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETTX\n",
      "[[-0.2074, 'ETTX']]\n",
      "ASNS\n",
      "[[0.0792, 'ASNS']]\n",
      "LBRT\n",
      "[[0.0, 'LBRT']]\n",
      "QTRX\n",
      "[[0.0751, 'QTRX']]\n",
      "WHD\n",
      "[[-0.0403, 'WHD']]\n",
      "GMR\n",
      "CTK\n",
      "[[-0.1791, 'CTK']]\n",
      "HHHHU\n",
      "[[0.0, 'HHHHU']]\n",
      "CBUS\n",
      "MTECU\n",
      "[[0.004, 'MTECU']]\n",
      "ESTA\n",
      "[[-0.0481, 'ESTA']]\n",
      "FTCH\n",
      "[[0.0537, 'FTCH']]\n",
      "QTT\n",
      "[[0.7549, 'QTT']]\n",
      "ELAN\n",
      "[[0.1163, 'ELAN']]\n",
      "AQ\n",
      "[[0.0215, 'AQ']]\n",
      "DDMXU\n",
      "[[-0.001, 'DDMXU']]\n",
      "LQDA\n",
      "[[-0.1084, 'LQDA']]\n",
      "BJ\n",
      "[[-0.016, 'BJ']]\n",
      "ALRN\n",
      "[[0.0, 'ALRN']]\n",
      "ROAD\n",
      "[[-0.0692, 'ROAD']]\n",
      "SWCH\n",
      "[[-0.0396, 'SWCH']]\n",
      "TNTR\n",
      "[[0.0211, 'TNTR']]\n",
      "ZYME\n",
      "[[-0.037, 'ZYME']]\n",
      "SIBN\n",
      "[[-0.0448, 'SIBN']]\n",
      "SPOT\n",
      "[[-0.1018, 'SPOT']]\n",
      "CIFS\n",
      "[[-0.0538, 'CIFS']]\n",
      "GPAQU\n",
      "[[0.0, 'GPAQU']]\n",
      "EVER\n",
      "[[0.0192, 'EVER']]\n",
      "ABLX\n",
      "[[0.0, 'ABLX']]\n",
      "KIDS\n",
      "[[-0.1074, 'KIDS']]\n",
      "SECO\n",
      "[[-0.1736, 'SECO']]\n",
      "PSIT\n",
      "STNE\n",
      "[[-0.0203, 'STNE']]\n",
      "KZR\n",
      "[[-0.1125, 'KZR']]\n",
      "NCNA\n",
      "[[0.0419, 'NCNA']]\n",
      "IPIC\n",
      "[[-0.1094, 'IPIC']]\n",
      "PDD\n",
      "[[0.0075, 'PDD']]\n",
      "CSTX\n",
      "GRAFU\n",
      "AFGL\n",
      "CODX\n",
      "[[-0.0317, 'CODX']]\n",
      "BJS\n",
      "[[-0.0061, 'BJS']]\n",
      "CELC\n",
      "[[0.0, 'CELC']]\n",
      "GRTS\n",
      "[[-0.0632, 'GRTS']]\n",
      "SENS\n",
      "[[0.036, 'SENS']]\n",
      "ALNA\n",
      "[[-0.1683, 'ALNA']]\n",
      "HVBH\n",
      "ZLAB\n",
      "[[0.1518, 'ZLAB']]\n",
      "BYND\n",
      "[[0.4293, 'BYND']]\n",
      "SBT\n",
      "[[0.0, 'SBT']]\n",
      "BRPAU\n",
      "[[-0.001, 'BRPAU']]\n",
      "LAUR\n",
      "[[0.06, 'LAUR']]\n",
      "YMAB\n",
      "[[-0.0943, 'YMAB']]\n",
      "ADOM\n",
      "[[-0.1049, 'ADOM']]\n",
      "PPDF\n",
      "[[-0.0165, 'PPDF']]\n",
      "GPMT\n",
      "[[0.0411, 'GPMT']]\n",
      "BCACU\n",
      "[[-0.003, 'BCACU']]\n",
      "CBTX\n",
      "[[0.0254, 'CBTX']]\n",
      "SCWX\n",
      "[[0.0079, 'SCWX']]\n",
      "CTRA\n",
      "[[0.0, 'CTRA']]\n",
      "WALL\n",
      "CNST\n",
      "[[0.0017, 'CNST']]\n",
      "TOCA\n",
      "[[0.2123, 'TOCA']]\n",
      "PZRX\n",
      "[[-0.0873, 'PZRX']]\n",
      "EOLS\n",
      "[[0.0222, 'EOLS']]\n",
      "NTGN\n",
      "[[-0.0625, 'NTGN']]\n",
      "REDU\n",
      "[[0.0349, 'REDU']]\n",
      "RUBY\n",
      "[[-0.2052, 'RUBY']]\n",
      "CAAP\n",
      "[[-0.0353, 'CAAP']]\n",
      "HMP\n",
      "LUNG\n",
      "NESRU\n",
      "[[0.0, 'NESRU']]\n",
      "IIIV\n",
      "[[0.0194, 'IIIV']]\n",
      "PT\n",
      "[[-0.1296, 'PT']]\n",
      "KNSA\n",
      "[[-0.2037, 'KNSA']]\n",
      "VTUS\n",
      "ACMR\n",
      "[[0.0, 'ACMR']]\n",
      "DNLI\n",
      "[[-0.0046, 'DNLI']]\n",
      "TWAV\n",
      "VCNX\n",
      "[[-0.0517, 'VCNX']]\n",
      "WGRD\n",
      "OMP\n",
      "[[0.0, 'OMP']]\n",
      "RTLR\n",
      "[[0.0653, 'RTLR']]\n",
      "CARG\n",
      "[[-0.049, 'CARG']]\n",
      "USX\n",
      "[[0.0233, 'USX']]\n",
      "MGP\n",
      "[[-0.0325, 'MGP']]\n",
      "OPTN\n",
      "[[-0.05, 'OPTN']]\n",
      "AXNX\n",
      "[[-0.0667, 'AXNX']]\n",
      "WAAS\n",
      "[[0.0452, 'WAAS']]\n",
      "CUE\n",
      "[[-0.03, 'CUE']]\n",
      "RMG'U\n",
      "MRSN\n",
      "[[-0.0175, 'MRSN']]\n",
      "ERYP\n",
      "[[0.0, 'ERYP']]\n",
      "RRR\n",
      "[[0.0108, 'RRR']]\n",
      "BPMP\n",
      "[[0.0237, 'BPMP']]\n",
      "EDTXU\n",
      "[[0.0, 'EDTXU']]\n",
      "ROKU\n",
      "[[0.4892, 'ROKU']]\n",
      "BWMCU\n",
      "[[0.0, 'BWMCU']]\n",
      "DAVA\n",
      "[[0.008, 'DAVA']]\n",
      "PCIM\n",
      "CRNX\n",
      "[[0.2732, 'CRNX']]\n",
      "HTTP Error 505: HTTP Version Not Supported\n",
      "Problem fetching RYTM \n",
      "RYTM \n",
      "[[0.2732, 'RYTM ']]\n",
      "BV\n",
      "[[0.0319, 'BV']]\n",
      "BEDU\n",
      "[[0.2182, 'BEDU']]\n",
      "NIU\n",
      "[[0.0176, 'NIU']]\n",
      "FAMI\n",
      "[[0.4045, 'FAMI']]\n",
      "PBTS\n",
      "[[-0.0655, 'PBTS']]\n",
      "XMLP\n",
      "SVMK\n",
      "[[-0.0805, 'SVMK']]\n",
      "TCL\n",
      "BSVN\n",
      "[[-0.0455, 'BSVN']]\n",
      "EPRT\n",
      "[[0.0111, 'EPRT']]\n",
      "ZLND\n",
      "GCAN\n",
      "[[-0.4554, 'GCAN']]\n",
      "SQFT\n",
      "DOVA\n",
      "[[0.0291, 'DOVA']]\n",
      "EVLO\n",
      "[[0.0, 'EVLO']]\n",
      "MIIIU\n",
      "[[-0.037, 'MIIIU']]\n",
      "STXB\n",
      "[[-0.0068, 'STXB']]\n",
      "MBIN\n",
      "[[-0.0118, 'MBIN']]\n",
      "LX\n",
      "[[-0.0932, 'LX']]\n",
      "LEGH\n",
      "[[-0.0066, 'LEGH']]\n",
      "INSP\n",
      "[[0.02, 'INSP']]\n",
      "VERI\n",
      "[[-0.1643, 'VERI']]\n",
      "BNGOU\n",
      "[[0.025, 'BNGOU']]\n",
      "NEBUU\n",
      "[[0.0, 'NEBUU']]\n",
      "AMCIU\n",
      "[[0.0, 'AMCIU']]\n",
      "AQST\n",
      "[[0.0664, 'AQST']]\n",
      "WISA\n",
      "[[-0.0601, 'WISA']]\n",
      "GEMP\n",
      "[[-0.0054, 'GEMP']]\n",
      "TMCXU\n",
      "[[0.001, 'TMCXU']]\n",
      "OSMT\n",
      "[[-0.0944, 'OSMT']]\n",
      "EEX\n",
      "[[0.0428, 'EEX']]\n",
      "ADV\n",
      "CBLK\n",
      "[[-0.0308, 'CBLK']]\n",
      "CEPU\n",
      "[[0.0727, 'CEPU']]\n",
      "SEND\n",
      "[[-0.0296, 'SEND']]\n",
      "NDRAU\n",
      "[[-0.0459, 'NDRAU']]\n",
      "APRN\n",
      "[[0.0, 'APRN']]\n",
      "PMAR\n",
      "ETON\n",
      "[[-0.0219, 'ETON']]\n",
      "writignerrors: []\n",
      "readingerrors: ['CLBR', 'VTEC', 'PARK', 'VPRL', 'IPSC', 'RWGEU', 'XTF', 'SFTY', 'VNCR', 'PROS', \"LTN'U\", 'PLTEU', 'OMADU', 'RVLV', 'IPOAU', 'PETC', 'CURV', 'AVI', 'TPGHU', 'DDOC', 'ENVN', 'GBSG', 'VRI', 'NREF', 'SI', 'HKIT', 'GSAHU', 'MFACU', 'CSCA', 'FPACU', 'CCHU', 'TPGEU', 'SPAQU', 'REPX', 'RLTY', 'Vizio', 'BDFC', 'CHACU', 'VZIO', 'CPUL', 'VTIQU ', 'PLIN', 'MQAI', 'YZCM', 'PSTX', 'ZOOX', 'ARDT', 'GTWY', 'FOCS', 'MRMR', 'JMPW', 'anaplan', 'RRI', 'VRIO', 'SMXT', 'PSAV', 'GMR', 'CBUS', 'PSIT', 'CSTX', 'GRAFU', 'AFGL', 'HVBH', 'WALL', 'HMP', 'LUNG', 'VTUS', 'TWAV', 'WGRD', \"RMG'U\", 'PCIM', 'XMLP', 'TCL', 'ZLND', 'SQFT', 'ADV', 'PMAR']\n"
     ]
    }
   ],
   "source": [
    "# Mine stock prices adjustedspark.sql(\"SELECT entities, ex\n",
    "import time\n",
    "problem_stocks = []\n",
    "i = 1\n",
    "writingerror=[]\n",
    "readingerror=[]\n",
    "#We'll iterate over all cashtags above the treshold, referd to as tickers.\n",
    "for ticker in ticker_list:\n",
    "    #We can only use the api 5 times per minute, that's why we sleep for a minute (+ some cushioning)\n",
    "    #every 5 times\n",
    "    if (i%5 == 0):\n",
    "        time.sleep(65)\n",
    "    try:\n",
    "        #increase our counter, so all the csv files get the correct input\n",
    "        i = i+1\n",
    "        #fetching the stock prices, all info can be found here: https://www.alphavantage.co/documentation/\n",
    "        text = urllib.request.urlopen(\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={}&outputsize=full&interval=60min&apikey={}&datatype=csv\".format(ticker,AV_API_key)).read()\n",
    "    except Exception as error:\n",
    "        #If there is an error, the tickers get added to a list.\n",
    "        print(error)\n",
    "        print(\"Problem fetching {}\".format(ticker))\n",
    "        problem_stocks.append(ticker)\n",
    "    print(ticker)\n",
    "    #We split the text into an array, first we need to extract every line, these end in \\\\r\\\\n\n",
    "    #then we split them on every comma, this is so we can convert them to a csv file later.\n",
    "    textrows = str(text).split(\"\\\\r\\\\n\")\n",
    "    #textcsv = map(lambda x:(x.split(',')), textrows)\n",
    "    try:\n",
    "        values = str(textrows[-2]).split(\",\")        \n",
    "        profit = round((float(values[-2]) - float(values[1]))/float(values[1]),4)\n",
    "        x=[[profit,ticker]]\n",
    "        print(x)\n",
    "        try:\n",
    "            with open(\"/home/mma/Downloads/IPO_project/stock_mining/profit.csv\", \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(x)\n",
    "        except:\n",
    "            writingerror.append(ticker)\n",
    "            #print(\"Error with writing {}\".format(ticker))\n",
    "    except:\n",
    "        readingerror.append(ticker)\n",
    "        #print(\"Error with fetching {}\".format(ticker))\n",
    "# Print out cashtags that have no corresponding stock data\n",
    "print(\"writignerrors: {}\".format(writingerror))\n",
    "print(\"readingerrors: {}\".format(readingerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#An example of the stock prices, this might be useful to look at.\n",
    "pd.read_csv(\"/home/mma/Downloads/IPO_project/stock_mining/SURF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profit = pd.read_csv(\"/home/mma/Downloads/IPO_project/stock_mining/profit.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = 'color : darkred'>  One observation per tweet </span style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = 'color:darkgreen'> Keep one observation per tweet </span style>\n",
    "\n",
    "The mechanism of tweets and retweets is the following:\n",
    "\n",
    "- An original tweet is sent out\n",
    "- Somebody retweets:\n",
    "    - The retweet is preceded by **RT @username_original_tweet: [text of original tweet]**\n",
    "    - The retweet count is incremented in the original tweet BUT in the retweet as well\n",
    " \n",
    "That means, even if we don't have the original tweet, we can obtain in by removing the pattern **RT @username_original_tweet:** from the retweet. This is important because, as shown below, we won't always have access to the original tweet.\n",
    "\n",
    "Another important step to take, is to delete all duplicates. Ultimately, we're going to be training and testing models on these tweets. Predicting a number of retweets using multiple training examples (i.e. retweets) per tweet would be erroneous. Hence we're going to take a few steps to try and avoid having  duplicate tweets in our dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = 'color:darkgreen'> Rename & unnest important columns </span style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_ipo = imp.reload(f_ipo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"*\").filter('lang == \"en\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tweets can contain a lot of useless data or data that's hard to analyze. This includes the retweet patern,\n",
    "#The hashtags, truncations and more.\n",
    "def clean_text(text):\n",
    "    # Remove pattern that signifies retweet\n",
    "    clean_text = re.sub(pattern = \"RT\\s@[a-zA-Z0-9]+:\\s\", repl = \"\", string = text)\n",
    "    clean_text = re.sub(r\"\\'s\", \" \", clean_text)\n",
    "    clean_text = re.sub(r\"\\'ve\", \" have \", clean_text)\n",
    "    clean_text = re.sub(r\"can't\", \"cannot \", clean_text)\n",
    "    clean_text = re.sub(r\"n't\", \" not \", clean_text)\n",
    "    clean_text = re.sub(r\"i'm\", \"i am \", clean_text)\n",
    "    clean_text = re.sub(r\"\\'re\", \" are \", clean_text)\n",
    "    clean_text = re.sub(r\"\\'d\", \" would \", clean_text)\n",
    "    clean_text = re.sub(r\"\\'ll\", \" will \", clean_text)\n",
    "    clean_text = re.sub(pattern = \"\\$[A-Za-z]{1,5}\", repl = \" ticker \", string = clean_text)\n",
    "    clean_text = re.sub(pattern =  \"w/\", repl = \"with\", string = clean_text)\n",
    "    clean_text = re.sub(pattern = \"http(s)?:\\/\\/[^\\s]+\", repl = \"website\", string = clean_text)\n",
    "    clean_text = re.sub(pattern = \"[\\|!,\\|?\\.\\[\\]\\\"'(\\n):\\$\\&\\#\\-\\%\\+]+\", repl = \" \", string = clean_text)\n",
    "    clean_text = re.sub(pattern = ' +', repl = ' ', string = clean_text)\n",
    "    clean_text = re.sub(pattern = \"@[A-Za-z0-9]+\", repl = \"user\", string= clean_text)\n",
    "    clean_text = clean_text.lower()\n",
    "    return clean_text\n",
    "\n",
    "clean_text(\"RT @VIZIO: @blabla Ever wonder what it's like  new vizio 56465% +22 d24d1 24 class led smart tv 1080p new 2016 model 13899end date wednesday jun... website to \" +            \"work w/ at\\n\\n VIZIO? Check out our company profile: https://t.co/IkJXNZ9pL4 https://t.co/TWSixQQnU\")\n",
    "#We define a user defined function from the clean text function.   \n",
    "udf_clean_text = udf(clean_text, StringType())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check wheter a tweet contains a url.\n",
    "def contains_url(isNotNull):\n",
    "    if isNotNull == True:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a temorary view as DF_SQL\n",
    "df.createOrReplaceTempView(\"DF_SQL\")\n",
    "#Filter all tweets to only contain the distinct tweets now\n",
    "df_distinct_tweets = spark.sql(\"SELECT entities, extended_entities, favorite_count, id_str, in_reply_to_status_id_str,                       is_quote_status, lang, place, a.text, user, a.timestamp, a.retweet_count,                       a.cashtags, a.contains_ipo, a.contains_cashtag, a.contains_cashtag_with_spaces                       FROM DF_SQL a Inner JOIN                                    (SELECT text, retweet_count, min(timestamp) as timestamp                                    FROM DF_SQL                                    GROUP BY text, retweet_count                                    ) b                      ON  a.text = b.text                      AND a.retweet_count=b.retweet_count AND a.timestamp = b.timestamp\").distinct()\n",
    "\n",
    "#There's some columns with sub colums, here we take all the necesary data out of these columns,\n",
    "#and when we have all the data drop the old column.\n",
    "#the output gives us a scheme so we can understand the tweets better.\n",
    "df_distinct_tweets = (df_distinct_tweets\n",
    "        .withColumn(\"cashtag_count\", udf(lambda x: len(x), IntegerType())(col(\"cashtags\")))\n",
    "    # ENTITIES  \n",
    "        .withColumn(\"hashtags\", col(\"entities.hashtags\"))\n",
    "        .withColumn(\"hashtag_count\", udf(lambda x: len(x), IntegerType())(col(\"hashtags\")))\n",
    "        .withColumn(\"contains_hashtags\", udf(f_ipo.contains_hashtags, IntegerType())(col(\"hashtags\")))\n",
    "        .withColumn(\"contains_url\", udf(contains_url, IntegerType())(col(\"entities.media.display_url\").isNotNull()))\n",
    "        .withColumn(\"user_mentions_count\", udf(lambda x: len(x), IntegerType())(col(\"entities.user_mentions\")))\n",
    "        .drop(\"entities\")\n",
    "    # USER\n",
    "        .withColumn(\"user_seniority\", udf(f_ipo.user_seniority, DoubleType())(\"user.created_at\"))\n",
    "        .withColumn(\"user_description\", col(\"user.description\"))\n",
    "        .withColumn(\"user_favourites_count\", col(\"user.favourites_count\"))\n",
    "        .withColumn(\"user_friends_count\", col(\"user.friends_count\"))\n",
    "        .withColumn(\"user_followers_count\", col(\"user.followers_count\"))\n",
    "        #.withColumn(\"user_description_clean\", udf_clean_text(col(\"user_description\")))\n",
    "        .drop(\"user\")\n",
    "    # PLACE\n",
    "        .drop(\"place\")\n",
    "    # EXTENDED_ENTITIES\n",
    "        .drop(\"extended_entities\")\n",
    "        .withColumn(\"retweet_count_double\", col(\"retweet_count\").cast(\"double\"))\n",
    "        .withColumn(\"day_of_week\", udf_day_of_week(col(\"timestamp\")))\n",
    "        .withColumn(\"text_clean\", udf_clean_text(col(\"text\")))\n",
    "        .withColumn(\"clean_text_len\", udf(lambda x: len(x), IntegerType())(\"text_clean\"))\n",
    "        .withColumn(\"nbr_words\",  udf(lambda x: len(x.split(\" \")), IntegerType())(col(\"text_clean\")))\n",
    "                     )\n",
    "\n",
    "df_distinct_tweets.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saving distinct tweets as parquet file for further use in Machine Learning\n",
    "df_distinct_tweets.write.mode('overwrite').parquet(\"/home/mma/Downloads/IPO_Mining/disctinct_tweets_test/ML_features22.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reading in the above parquet file\n",
    "df_distinct_tweets = spark.read.parquet(\"/home/mma/Downloads/IPO_Mining/disctinct_tweets_test/ML_features22.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We clean our tweets with the function we created before. Now they'll contain less useless info\n",
    "text_df = df_distinct_tweets.select(\"text\").toPandas() # converting pyspark DF to pandas DF\n",
    "text_df[\"clean\"] = text_df[\"text\"].apply(lambda x: clean_text(x))  # applying clean_text function\n",
    "text_df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def RF_classification(model_description, cat_columns, num_columns, output_column = \"retweet_binary\"):\n",
    "    stringIndexers = []\n",
    "    indexed_cols = []\n",
    "\n",
    "    encoders = []\n",
    "    encoded_cols = []\n",
    "    #add a numbered index to all categories from the columns\n",
    "    #this means all rows with category \"a\" will be \"1\", \"b\" will be \"2\", and so forth (as an example)\n",
    "    for column in cat_columns:\n",
    "        inCol= column\n",
    "        outCol = column + \"indexed\"\n",
    "        stringIndexer = StringIndexer(inputCol = inCol, outputCol=outCol)\n",
    "\n",
    "        stringIndexers.append(stringIndexer)\n",
    "        indexed_cols.append(outCol)\n",
    "\n",
    "    for column in indexed_cols:\n",
    "        inCol= column\n",
    "        outCol = column + \"encoded\"\n",
    "        encoder = OneHotEncoder(inputCol=inCol, outputCol=outCol)\n",
    "\n",
    "        encoders.append(encoder)\n",
    "        encoded_cols.append(outCol)\n",
    "\n",
    "    binarizer = Binarizer(inputCol = \"retweet_count_double\", outputCol = output_column, threshold=0.9)\n",
    "\n",
    "    assembler = VectorAssembler(inputCols= num_columns + encoded_cols, outputCol=\"features\")\n",
    "    \n",
    "    pipeline = Pipeline(stages= stringIndexers + encoders + [assembler] + [binarizer])\n",
    "    pipelineModel = pipeline.fit(df_distinct_tweets)\n",
    "\n",
    "    transformed_df = (pipelineModel.transform(df_distinct_tweets)\n",
    "                                   .select(\"features\", \n",
    "                                            col(\"retweet_binary\").cast(\"integer\"), \n",
    "                                            \"retweet_count\")\n",
    "                     )\n",
    "    \n",
    "    transformed_df = transformed_df\n",
    "    \n",
    "    train, validation, test = transformed_df.randomSplit([0.8, 0.1, 0.1], seed = 42)\n",
    "    \n",
    "    rf = RandomForestClassifier(numTrees=500, labelCol=\"retweet_binary\")\n",
    "    rf_fitted = rf.fit(train)\n",
    "\n",
    "    train_predictions = rf_fitted.transform(train)\n",
    "    validation_predictions = rf_fitted.transform(validation)\n",
    "    \n",
    "    AUC_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"retweet_binary\", metricName=\"areaUnderROC\")\n",
    "\n",
    "    AUC_train = AUC_evaluator.evaluate(train_predictions)\n",
    "    AUC_validation = AUC_evaluator.evaluate(validation_predictions)\n",
    "    print(\"###################\" + model_description.upper() + \"##################\")\n",
    "    print(\"TRAIN AUC:       {}\".format(AUC_train))\n",
    "    print(\"TRAIN VALIDATION:{}\".format(AUC_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** time_of_day? **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_columns = df_distinct_tweets.columns\n",
    "print(all_columns)\n",
    "\n",
    "non_text_num_cols = [\"user_seniority\", \"user_favourites_count\", \"user_friends_count\", \"user_followers_count\"]\n",
    "non_text_cat_cols = [\"day_of_week\"]\n",
    "text_num_cols = [\"cashtag_count\", \"hashtag_count\"]\n",
    "text_cat_cols = [\"lang\", \"contains_cashtag\", \"contains_cashtag_with_spaces\", \"contains_ipo\", \"contains_hashtags\", \"contains_url\"]\n",
    "dependent_variable = [\"retweet_count\"]\n",
    "text = [\"clean_text\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In all the notebooks there are problems with the random forest code, i am not sure if it is the code or the Data set used that triggers these problems.\n",
    "get_ipython().run_cell_magic('time', '', 'RF_classification(model_description = \"All inclusive model\", \\n                        cat_columns = text_cat_cols + non_text_cat_cols, \\n                      num_columns = text_num_cols + non_text_num_cols)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# also random forest error\n",
    "get_ipython().run_cell_magic('time', '', 'RF_classification(model_description = \"Text only model\", \\n                  cat_columns = text_cat_cols, \\n                  num_columns = text_num_cols)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = 'color:darkred'> Deep learning model using keras </span style>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This is the GoogleNews-Vectors, this is a .bin file that also has not been provided by the creator so we downloaded one from the internet.\n",
    "#This still gives us an error because this propably is not the same version useds by the creator. origin error => unkown\n",
    "\n",
    "\n",
    "\n",
    "#get_ipython().run_cell_magic('time', '', 'from gensim.models import KeyedVectors\\n\\nword2vec = KeyedVectors.load_word2vec_format(\"/home/sachadubrulle/Data/W2V/GoogleNews-vectors-negative300.bin\", binary=True)\\n\\nprint(\\'Found %s word vectors of word2vec\\' % len(word2vec.vocab))\\n\\nEMBEDDING_DIM = 300')\n",
    "get_ipython().run_cell_magic('time', '', 'from gensim.models import KeyedVectors\\n\\nword2vec = KeyedVectors.load_word2vec_format(\"googlevector/GoogleNews-vectors-negative300.bin\", binary=True)\\n\\nprint(\\'Found %s word vectors of word2vec\\' % len(word2vec.vocab))\\n\\nEMBEDDING_DIM = 300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages for text mining\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "KeyedVectors.load_word2vec_format(\"googlevector/GoogleNews-vectors-negative300.bin\", binary=True, limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# converting into Panda DF to export\n",
    "df_pandas = df_distinct_tweets.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exporting the file \n",
    "df_pandas.to_csv(\"/home/mma/Downloads/IPO_Mining/df_tweets/tweets.csv\", header=True, index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''# This never worked in the first place\n",
    "#train_labels = df_train(pd.read_csv(\"df_tweets/tweets.csv\"))\n",
    "#df_train['cashtags'].describe()\n",
    "df_new = pd.read_csv(open('/home/mma/Downloads/IPO_Mining/df_tweets/tweets.csv','rU'), encoding='utf-8', engine='c')\n",
    "#df_new = pd.read_csv('/home/mma/Downloads/IPO_Mining/df_tweets/tweets.csv', encoding='utf8')\n",
    "#df_train['cashtags'].describe()\n",
    "#train_labels = df_train[\"row name\"]\n",
    "\n",
    "def data_prep_LSTM(df, dataset =  \"\"):\n",
    "    # shorten sequence to maximum length\n",
    "   \n",
    "    df[\"Q1_max_len\"] = df[\"Q1_clean\"].apply(lambda x: \" \".join(x.split(\" \")[0:MAX_SEQ_LENGTH]))\n",
    "    df[\"Q2_max_len\"] = df[\"Q2_clean\"].apply(lambda x: \" \".join(x.split(\" \")[0:MAX_SEQ_LENGTH]))\n",
    "    #df[\"num_words\"] = df[\"Q1_max_len\"].apply(lambda x: len(x.split(\" \")))\n",
    "    \n",
    "    #df.drop([\"Q1_clean\", \"Q2_clean\"], axis = 1, inplace = True)\n",
    "    #df.drop([\"clean_text\"], axis = 1, inplace = True)\n",
    "    \n",
    "    if dataset == \"train\":\n",
    "            df.drop([\"is_duplicate\"], axis = 1, inplace = True) \n",
    "    return df\n",
    "\n",
    "\n",
    "tweetcount = df_new['id_str'].count() //4 * 3\n",
    "df_train = df_new[:tweetcount]\n",
    "df_test = df_new[tweetcount:]\n",
    "df_train_done = data_prep_LSTM(df_train, \"train\")\n",
    "df_test_done = data_prep_LSTM(df_test)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#summary of the text\n",
    "df_distinct_tweets.select(\"nbr_words\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <span style = 'color:darkred'> Basic retweet model Spark </span style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When a tweet is initially fetched, the retweet_count is almost inevitably equal to zero as the instance is fetched immediately after it appears on the social network. Hence to obtain the number of retweets to train a retweet prediction model, we must fetch the tweets again at a later point in time.\n",
    "\n",
    "Few problems arising with this:\n",
    "\n",
    "- A considerable number of tweets have been deleted (either only the tweet, or even the account of the sender)\n",
    "- With regards to storing the tweet object:\n",
    "    - If it is done at the initial fetch: \n",
    "        - the storage space needed is considerable\n",
    "        - we can filter out tweets that we aren't interested in based to avoid fetching irrelevant tweets a second time\n",
    "    - If we only save the tweet id in the initial fetch:\n",
    "        - Less storage space is needed initially\n",
    "        - We cannot filter based on the text in the tweets and hence we will have to refetch all tweets, which might be a time costly, as twitter has some pretty low rate limits\n",
    "    - Solution could be to:\n",
    "        - save only the tweet id and the text at initial fetch, which can then be used to filter without requiring too much storage\n",
    "        - Save the full object when tweet is fetched a second time\n",
    "    \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = 'color:darkgreen'> Cashtag over time </span style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df_distinct_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Function to plot the Tickers\n",
    "def plot_ticker(df, plot_ticker, image_save_dir):\n",
    "    \n",
    "    #print \"Enter your stock as a string, i.e. with quotation marks\"\n",
    "    filtered_CT_per_TS = df.select(\"timestamp\", explode(\"cashtags\").alias(\"cashtag\")).filter(\"cashtag == '{}'\".format(plot_ticker))\n",
    "    #print(filtered_CT_per_TS)\n",
    "    # cashtag per timestamp\n",
    "    CT_TS_pd = filtered_CT_per_TS.toPandas()\n",
    "    CT_TS_pd[\"date\"] = CT_TS_pd[\"timestamp\"].apply(lambda x: x.date())\n",
    "    CT_TS_pd[\"date_str\"] = CT_TS_pd[\"date\"].apply(lambda x: str(x))\n",
    "    CT_TS_pd.drop(\"timestamp\", axis = 1, inplace = True)\n",
    "    #print(CT_TS_pd)\n",
    "    \n",
    "    #cashtag count per day\n",
    "    CT_TWTCOUNT_DATE = pd.DataFrame(CT_TS_pd[\"date_str\"].value_counts()).sort_index(ascending = True).reset_index(drop = False)\n",
    "    CT_TWTCOUNT_DATE.columns = [\"date\", \"count\"]\n",
    "    #print(CT_TWTCOUNT_DATE)\n",
    "    # stock prices\n",
    "    stock_prices_ticker = pd.read_csv(\"/home/mma/Downloads/IPO_Mining/stock_mining/{}.csv\".format(plot_ticker))\n",
    "    #[[\"Date\", \"Close\"]].sort_values(\"Date\", ascending = True)\n",
    "    stock_prices_ticker = stock_prices_ticker.drop(['open','high','low','volume'],axis=1)\n",
    "    stock_prices_ticker = stock_prices_ticker.rename(columns= {'b\\'timestamp':'date'})\n",
    "    stock_prices_ticker = stock_prices_ticker.sort_values(\"date\", ascending = True)\n",
    "    #print(stock_prices_ticker)\n",
    "    full_stock_df = pd.merge(left = stock_prices_ticker, right = CT_TWTCOUNT_DATE, on = \"date\", how = \"right\").sort_values(\"date\", ascending = True)\n",
    "    print (full_stock_df)\n",
    "    full_stock_df[\"close\"].fillna(value = 0, inplace = True)\n",
    "    full_stock_df.reset_index(drop = True, inplace = True)\n",
    "    #print (full_stock_df)\n",
    "    \n",
    "    # PLOTTING\n",
    "    fig = plt.figure(figsize = (20,7))\n",
    "\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.set_title(plot_ticker)\n",
    "    ax1.plot(full_stock_df[\"count\"], color= colors[\"orange\"], linewidth = 2.5)\n",
    "    ax1.set_ylabel('count')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(full_stock_df[\"close\"].loc[(full_stock_df.close != 0)], color = colors[\"blue\"], linewidth = 2.5)\n",
    "    ax2.set_ylabel('close')\n",
    "    \n",
    "    ax1.xaxis.set_major_locator(YearLocator())\n",
    "    ax1.xaxis.set_major_formatter(DateFormatter('%Y'))\n",
    "    ax1.xaxis.set_minor_locator(MonthLocator())\n",
    "    # format the coords message box\n",
    "    ax2.format_xdata = DateFormatter('%Y-%m-%d')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    print(\"Count is orange, stock price is blue\")\n",
    "    #fig.savefig('{}{}.png'.format(image_save_dir, plot_ticker))\n",
    "    #fig.show()\n",
    "    #return full_stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"cashtags\").show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering required cashtags\n",
    "#filtered_CT_per_TS = \n",
    "df.where(col(\"cashtags\").isin({\"SPOT\"}))\n",
    "\n",
    "#df.filter(col(\"cashtags\").isin('SPOT')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting into pands and saving in the object\n",
    "pandas_df = filtered_CT_per_TS.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting the Tickers count of tweets vs  price\n",
    "plot_ticker(df, \"AMZN\", \"ipoFiles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most plot_tickers have problems with the : Cannot convert tz-naive Timestamp, use tz_localize to localize = > problem with the timestamp convertion to panda's.\n",
    "# look at the code of Twitter filter or twitter analyse for a solution, timestamps have also been used. \n",
    "# yet the plots do not give this error. the conversion has been donde differently\n",
    "# Here however it is propably because my limited data set did not haave a ticker called \"SNAP\" so normaly this would also give the Timestamp error. this df is just  missing the ticker\n",
    "plot_ticker(df, \"SPOT\", \"ipoFiles/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_ticker_data_prep(df, plot_ticker):\n",
    "    \n",
    "    #print \"Enter your stock as a string, i.e. with quotation marks\"\n",
    "    filtered_CT_per_TS = df.select(\"timestamp\", explode(\"cashtags\").alias(\"cashtag\")).filter(\"cashtag == '{}'\".format(plot_ticker))\n",
    "    \n",
    "    # cashtag per timestamp\n",
    "    CT_TS_pd = filtered_CT_per_TS.toPandas()\n",
    "    CT_TS_pd[\"date\"] = CT_TS_pd[\"timestamp\"].apply(lambda x: x.date())\n",
    "    CT_TS_pd[\"date_str\"] = CT_TS_pd[\"date\"].apply(lambda x: str(x))\n",
    "    CT_TS_pd.drop(\"timestamp\", axis = 1, inplace = True)\n",
    "    \n",
    "    #cashtag count per day\n",
    "    CT_TWTCOUNT_DATE = pd.DataFrame(CT_TS_pd[\"date_str\"].value_counts()).sort_index(ascending = True).reset_index(drop = False)\n",
    "    CT_TWTCOUNT_DATE.columns = [\"date\", \"count\"]\n",
    "    \n",
    "    # stock prices\n",
    "    stock_prices_ticker = pd.read_csv(\"stock_mining/{}.csv\".format(plot_ticker))\n",
    "    #[[\"date\", \"close\"]].sort_values(\"date\", ascending = True)\n",
    "    stock_prices_ticker = stock_prices_ticker.drop(['open','high','low','volume'],axis=1)\n",
    "    print(stock_prices_ticker)\n",
    "    stock_prices_ticker = stock_prices_ticker.rename(columns= {'timestamp':'date','close':'closing_price'})\n",
    "    #print(stock_prices_ticker)\n",
    "    stock_prices_ticker = stock_prices_ticker.sort_values('date', ascending = True)\n",
    "    #stock_prices_ticker.columns = [\"date\", \"closing_price\"]\n",
    "    \n",
    "    full_stock_df = pd.merge(left = stock_prices_ticker, right = CT_TWTCOUNT_DATE, on = \"date\", how = \"right\").sort_values(\"date\", ascending = True)\n",
    "    full_stock_df[\"closing_price\"].fillna(value = 0, inplace = True)\n",
    "    full_stock_df.reset_index(drop = True, inplace = True)\n",
    "    return full_stock_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-17508baf364c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SPOT\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstock_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_ticker_data_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# again \"SNAP\"ticker is propably just missing.\n",
    "stock = \"SPOT\"\n",
    "\n",
    "stock_df = plot_ticker_data_prep(df, stock)\n",
    "print(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# because previous cell gives an error the stock_df is never created so this ceel can not run\n",
    "stock_df[\"timestamp\"] = stock_df[\"date\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "print(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# because previous cell gives an error the stock_df is never created so this ceel can not run\n",
    "\n",
    "# FOR SNAPCHAT\n",
    "stock_df[\"timestamp\"] = stock_df[\"timestamp\"].apply(lambda x: x.year)\n",
    "#stock_df = stock_df.ix[stock_df[\"timestamp\"].apply(lambda x: x.year) == 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''stock_df = stock_df.ix[stock_df[\"timestamp\"] == 2018]\n",
    "print(stock_df)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# because previous cell gives an error the stock_df is never created so this ceel can not run\n",
    "\n",
    "'''plt.rc('font', size=15)\n",
    "plt.rc('axes', titlesize=20)\n",
    "\n",
    "fig = plt.figure(figsize = (20,10))\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_title('stock')\n",
    "\n",
    "ax1.plot(stock_df[\"date\"], stock_df[\"count\"], \n",
    "         color= colors[\"orange\"], \n",
    "         linewidth = 2.5, \n",
    "         label = \"Tweet count\")\n",
    "\n",
    "ax1.grid(True, axis = 'x')\n",
    "ax1.format_xdata = DateFormatter('%Y-%m-%d')\n",
    "ax1.xaxis.set_minor_locator(MonthLocator())\n",
    "ax1.set_ylabel('Tweet count')\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(stock_df[\"date\"].ix[(stock_df.closing_price != 0)], \n",
    "         stock_df[\"closing_price\"].ix[(stock_df.closing_price != 0)], \n",
    "         color = colors[\"blue\"], \n",
    "         linewidth = 2.5,\n",
    "         label = \"Closing price\")\n",
    "ax2.set_ylabel('Closing price')\n",
    "\n",
    "legend = ax1.legend(loc=\"upper left\", shadow=True)\n",
    "legend = ax2.legend(loc=\"upper right\", shadow=True)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('{}.png'.format(stock))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This is just to see how long this notebook ran\n",
    "datetime.now() - date_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "174px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
